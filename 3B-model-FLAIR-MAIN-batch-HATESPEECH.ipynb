{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train number of different models from Flair framework.\n",
    "# With different sized trainin data\n",
    "# save predictions of each model to file\n",
    "\n",
    "# Notice - 1st run may take long as model weights are downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "# https://github.com/t-davidson/hate-speech-and-offensive-language\n",
    "\n",
    "# Paper\n",
    "# https://aaai.org/ocs/index.php/ICWSM/ICWSM17/paper/view/15665\n",
    "\n",
    "# Their code\n",
    "# https://github.com/t-davidson/hate-speech-and-offensive-language/blob/master/src/Automated%20Hate%20Speech%20Detection%20and%20the%20Problem%20of%20Offensive%20Language%20Python%203.6.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code based on https://github.com/zalandoresearch/flair/blob/master/resources/docs/TUTORIAL_4_ELMO_BERT_FLAIR_EMBEDDING.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "from nltk.stem.porter import *\n",
    "import time\n",
    "\n",
    "# Display whole text of dataframe field and don't cut it\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 1.3.1\n"
     ]
    }
   ],
   "source": [
    "print(f'torch version: {torch.__version__}')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(f'torch version: {torch.__version__}')\n",
    "torch version: 1.0.1.post2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/max/git/modelcompare/dataset_hatespeech/\n"
     ]
    }
   ],
   "source": [
    "dataset = 'hatespeech'\n",
    "\n",
    "current = os.getcwd()\n",
    "basefolder = current + '/dataset_'+ dataset+'/'\n",
    "datafolder = basefolder + 'data/'  # for example /dataset_businessnews/data/\n",
    "print(basefolder)\n",
    "\n",
    "infolder =  basefolder + 'input/'\n",
    "outfolder = basefolder + 'output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.data_fetcher import NLPTaskDataFetcher\n",
    "\n",
    "from flair.embeddings import WordEmbeddings, StackedEmbeddings, DocumentRNNEmbeddings\n",
    "from flair.embeddings import DocumentPoolEmbeddings\n",
    "from flair.embeddings import FlairEmbeddings, BertEmbeddings, ELMoEmbeddings\n",
    "from flair.embeddings import BytePairEmbeddings\n",
    "\n",
    "from flair.embeddings import OpenAIGPTEmbeddings\n",
    "#from flair.embeddings import OpenAIGPT2Embeddings\n",
    "\n",
    "# New DocumentRNNEmbeddings, deprecates DocumentLSTMembeddings\n",
    "# from flair.embeddings import #DocumentLSTMEmbeddings\n",
    "\n",
    "from flair.models import TextClassifier\n",
    "from flair.trainers import ModelTrainer\n",
    "from pathlib import Path\n",
    "\n",
    "# new ones: GPT-1 and GPT-2\n",
    "# https://github.com/flairNLP/flair/tree/master/resources/docs/embeddings\n",
    "# https://github.com/flairNLP/flair/blob/master/resources/docs/embeddings/TRANSFORMER_EMBEDDINGS.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1\n",
    "# REPEATABILITY\n",
    "def seed_everything(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything() # also called here\n",
    "\n",
    "# TEXT PREPROCESS\n",
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "\n",
    "def clean_text(x):\n",
    "    x = str(x)\n",
    "    for punct in puncts:\n",
    "        if punct in x: # comparison makes faster\n",
    "            x = x.replace(punct, f' {punct} ')\n",
    "    return x\n",
    "\n",
    "quotes = ['″', '′', '\"'] # apostrophe \"'\"\n",
    "def mark_quotes(x):\n",
    "    x = str(x)\n",
    "    for quote in quotes:\n",
    "        if quote in x: # comparison makes faster\n",
    "            x = x.replace(quote, f'quote')\n",
    "    return x\n",
    "\n",
    "def preprocess(text_string):\n",
    "    \"\"\"\n",
    "    Accepts a text string and replaces:\n",
    "    1) urls with URLHERE\n",
    "    2) lots of whitespace with one instance\n",
    "    3) mentions with MENTIONHERE\n",
    "\n",
    "    This allows us to get standardized counts of urls and mentions\n",
    "    Without caring about specific people mentioned\n",
    "    \"\"\"\n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    \n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    \n",
    "    #add #, mention, e.g. &#8120     \n",
    "    mention_regex2  =   '&#[0-9]*' \n",
    "    \n",
    "    \n",
    "    parsed_text = re.sub(space_pattern, ' ', text_string)\n",
    "    parsed_text = re.sub(giant_url_regex, ' URL ', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex, ' MENTION', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex2, ' MENTION', parsed_text)    \n",
    "    return parsed_text\n",
    "\n",
    "def tokenize(tweet):\n",
    "    \"\"\"Removes punctuation & excess whitespace, sets to lowercase,\n",
    "    and stems tweets. Returns a list of stemmed tokens.\"\"\"\n",
    "    tweet = \" \".join(re.split(\"[^a-zA-Z]*\", tweet.lower())).strip()\n",
    "    tokens = [stemmer.stem(t) for t in tweet.split()]\n",
    "    return tokens\n",
    "\n",
    "def basic_tokenize(tweet):\n",
    "    \"\"\"Same as tokenize but without the stemming\"\"\"\n",
    "    # *needed to be removed or outputs a list of letters\n",
    "    #tweet = \" \".join(re.split(\"[^a-zA-Z.,!?]*\", tweet.lower())).strip()\n",
    "    tweet = \" \".join(re.split(\"[^a-zA-Z.,!?]\", tweet.lower())).strip()  \n",
    "    #tweet = \" \".join(re.split(r'\\s+', tweet.lower())).strip()\n",
    "    return tweet.split()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Test-cases\n",
    "print(basic_tokenize('aaa'))\n",
    "print(re.split(\"[^a-zA-Z.,!?]\", 'aaa aa'))\n",
    "\n",
    "# test, @-reference and url\n",
    "# still misses this ''#8221;food' (;text continuing on end without space)\n",
    "preprocess('&#8220;@_CiaraaaS: What things do you love? &#8212; Myself http://t.co/D23mKPgMxu&#8221;food weed pussy life &#128129')\n",
    "\n",
    "preprocess('http://t.co/D23mKPgMxu&#8221;food')\n",
    "\n",
    "giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "\n",
    "re.sub(giant_url_regex, ' URL ', 'http://t.co/D23mKPgMxu&#8221;food')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Dataset - hatespeech\n",
    "# count: nr of users who coded item\n",
    "# middle: nr who coded as hate speech / offecinve / neither\n",
    "# class: majority label of users \n",
    "#  0 - hate speech\n",
    "#  1 - ONLY offensive  language\n",
    "#  2 - neither\n",
    "\n",
    "mydata = pd.read_csv('data/labeled_data.csv', index_col=0)\n",
    "mydata.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData():\n",
    "    train = pd.read_csv(basefolder+'input/train.csv',sep='\\t', header = None)\n",
    "    dev = pd.read_csv(basefolder+'input/dev.csv'  ,sep='\\t', header = None)\n",
    "    test = pd.read_csv(basefolder+'input/test.csv'  ,sep='\\t', header = None)    \n",
    "    train.columns = ['id','label','text']\n",
    "    dev.columns  = ['id','label', 'text']\n",
    "    test.columns  = ['id','label', 'text']   \n",
    "    return train, dev, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprosess_hatespeech(df):\n",
    "    df.text = df.text.apply(lambda x: preprocess(x)) #URL, @mention etc\n",
    "    df[\"text\"] = df[\"text\"].apply(lambda x: clean_text(x))\n",
    "    df[\"text\"] = df[\"text\"].apply(lambda x: mark_quotes(x))\n",
    "    #df.text = df.text.apply(lambda x: basic_tokenize(x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn label from digit into Fasttext format __label__.  \"1\" into \"__label__1\"\n",
    "def toFasttext(df):\n",
    "    df['label'] = '__label__' + df['label'].astype(str)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = infolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "3000\n",
      "3000\n"
     ]
    }
   ],
   "source": [
    "# randomize\n",
    "\n",
    "# Load data\n",
    "train, dev, test = loadData()\n",
    "\n",
    "train = train.iloc[np.random.permutation(len(train))]\n",
    "\n",
    "\n",
    "#Test smaller\n",
    "trainsize = len(train)\n",
    "\n",
    "'''SET TRAINSIZE HERE'''\n",
    "# 100, 200, 500, 1k, 3k, 7k, 18k\n",
    "trainsize = 100\n",
    "train = train[0:trainsize]\n",
    "\n",
    "print(len(train))\n",
    "print(len(dev))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Non randomize\n",
    "\n",
    "# Load data\n",
    "train, dev, test = loadData()\n",
    "\n",
    "#Test smaller\n",
    "trainsize = len(train)\n",
    "\n",
    "'''SET TRAINSIZE HERE'''\n",
    "# 100, 200, 500, 1k, 3k, 7k, 18k\n",
    "trainsize = 501\n",
    "train = train[0:trainsize]\n",
    "\n",
    "print(len(train))\n",
    "print(len(dev))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = preprosess_hatespeech(train)\n",
    "dev = preprosess_hatespeech(dev)\n",
    "test = preprosess_hatespeech(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = toFasttext(train)\n",
    "dev = toFasttext(dev)\n",
    "test = toFasttext(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15131</th>\n",
       "      <td>18202</td>\n",
       "      <td>__label__1</td>\n",
       "      <td>RT MENTION  :  MENTION That has redneck written all over it lol  .  Drunks can use gravity to get down MENTION  ;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4106</th>\n",
       "      <td>3986</td>\n",
       "      <td>__label__2</td>\n",
       "      <td>MENTION her face ugly to me  ,   &amp;  amp  ;  her nudes were trash  .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3365</th>\n",
       "      <td>4483</td>\n",
       "      <td>__label__1</td>\n",
       "      <td>MENTION come draw and paint with me niggah  !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6004</th>\n",
       "      <td>12574</td>\n",
       "      <td>__label__1</td>\n",
       "      <td>Krakin this bitch ipem at 12  .   .   .  WHO SIPPIN WIT ME  .   .   .  ELEMENTS URL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11261</th>\n",
       "      <td>10768</td>\n",
       "      <td>__label__1</td>\n",
       "      <td>I hate when bitches quote been thinking quote smh</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id       label  \\\n",
       "15131  18202  __label__1   \n",
       "4106   3986   __label__2   \n",
       "3365   4483   __label__1   \n",
       "6004   12574  __label__1   \n",
       "11261  10768  __label__1   \n",
       "\n",
       "                                                                                                                      text  \n",
       "15131  RT MENTION  :  MENTION That has redneck written all over it lol  .  Drunks can use gravity to get down MENTION  ;    \n",
       "4106    MENTION her face ugly to me  ,   &  amp  ;  her nudes were trash  .                                                 \n",
       "3365    MENTION come draw and paint with me niggah  !                                                                       \n",
       "6004   Krakin this bitch ipem at 12  .   .   .  WHO SIPPIN WIT ME  .   .   .  ELEMENTS URL                                  \n",
       "11261  I hate when bitches quote been thinking quote smh                                                                    "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Id in start of line is not Fasttext format: remove id\n",
    "train.drop(['id'], axis=1, inplace=True)\n",
    "dev.drop(['id'], axis=1, inplace=True)\n",
    "test.drop(['id'], axis=1, inplace=True)\n",
    "\n",
    "# Write to Flairs input csv files\n",
    "train.to_csv(basefolder+'input/flair_train.csv',sep='\\t', index = False, header = False)\n",
    "dev.to_csv(basefolder+'input/flair_dev.csv'  ,sep='\\t', index = False, header = False)\n",
    "test.to_csv(basefolder+'input/flair_test.csv',sep='\\t', index = False, header = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flair"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# init Flair embeddings\n",
    "flair_forward_embedding = FlairEmbeddings('multi-forward')\n",
    "flair_backward_embedding = FlairEmbeddings('multi-backward')\n",
    "\n",
    "# init multilingual BERT\n",
    "#bert_embedding = BertEmbeddings('bert-base-multilingual-cased')\n",
    "\n",
    "embedding_flair = FlairEmbeddings('news-forward')\n",
    "sentence = Sentence('I love Austin')\n",
    "flair_forward_embedding.embed(sentence)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacked embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings\n",
    "# https://github.com/zalandoresearch/flair/blob/master/resources/docs/TUTORIAL_4_ELMO_BERT_FLAIR_EMBEDDING.md\n",
    "\n",
    "# 'multi-forward', multi-lang English, German, French, Italian, Dutch, Polish, \n",
    "#        Mix of corpora (Web, Wikipedia, Subtitles, News)\n",
    "\n",
    "# 'mix-forward'English,   Forward LM embeddings over mixed corpus (Web, Wikipedia, Subtitles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`StackedEmbeddings` are currently a `WordEmbeddings` class, so they cannot directly be used to classify \n",
    "documents. They can only be used for sequence labeling.\n",
    "\n",
    "However, you can put a stack of word embeddings into one of the `DocumentEmbeddings` classes such as `DocumentPoolEmbeddings` or `DocumentLSTMEmbeddings`. This way, you are specifying how to aggregate word embeddings for text classification\n",
    "\n",
    "So `DocumentPoolEmbeddings` will simply average them, while `DocumentLSTMEmbeddings` will train an LSTM over them.\n",
    "\n",
    " https://github.com/zalandoresearch/flair/issues/414\n",
    " \n",
    " *update depracated: DocumentLSTMEmbeddings. (The functionality of this class is moved to 'DocumentRNNEmbeddings')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# create a StackedEmbedding object that combines glove and forward/backward flair embeddings\n",
    "stacked_embeddings = StackedEmbeddings([\n",
    "                                        WordEmbeddings('glove'), \n",
    "                                        #FlairEmbeddings('news-forward'), \n",
    "                                        #FlairEmbeddings('news-backward'),\n",
    "                                        FlairEmbeddings('multi-forward'), \n",
    "                                        #FlairEmbeddings('multi-backward'),    \n",
    "\n",
    "                                        # init multilingual BERT\n",
    "                                        BertEmbeddings('bert-base-cased'),   \n",
    "                                        # 'bert-base-cased'\n",
    "                                        # BertEmbeddings('bert-base-multilingual-cased'),   \n",
    "                                       ])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Word embeddings in Flair\n",
    "# https://github.com/zalandoresearch/flair/blob/master/resources/docs/TUTORIAL_3_WORD_EMBEDDING.md\n",
    "\n",
    "WordEmbeddings('en')   English Fasttext. Only country-code is Fasttext\n",
    "\n",
    "'en-glove' (or 'glove') \tEnglish \tGloVe embeddings\n",
    "'en-extvec' (or 'extvec') \tEnglish \tKomninos embeddings\n",
    "'en-crawl' (or 'crawl') \tEnglish \tFastText embeddings over Web crawls\n",
    "'en-twitter' (or 'twitter') English \tTwitter embeddings\n",
    "\n",
    "'fi' \tFinnish \tFinnish FastText embeddings"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "word_embeddings = [WordEmbeddings('glove'),                    \n",
    "                  ]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 0.9242\n",
    "word_embeddings = [WordEmbeddings('glove'), \n",
    " \n",
    "                   WordEmbeddings('en-twitter'),\n",
    "                     #WordEmbeddings('en'),    FastText embeddings over news and wikipedia data                \n",
    "                   WordEmbeddings('en-crawl'), # English FastText embeddings over Web crawls\n",
    "                   # WordEmbeddings('en-extvec') #Komnios embeddings\n",
    "                   #FlairEmbeddings('news-forward-fast'), \n",
    "                   #FlairEmbeddings('news-backward-fast'),\n",
    "                   \n",
    "                #FlairEmbeddings('multi-forward'), \n",
    "                   # FlairEmbeddings('multi-backward'), 17 GB each!\n",
    "                   #BertEmbeddings('bert-base-cased'),  \n",
    "                   #ELMoEmbeddings('original')\n",
    "                  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OR average \n",
    "# document_embeddings = DocumentPoolEmbeddings(word_embeddings)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# initialize the document embeddings, mode = mean\n",
    "document_embeddings = DocumentPoolEmbeddings([WordEmbeddings('glove'),\n",
    "                                               WordEmbeddings('en-crawl')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/text-classification-with-state-of-the-art-nlp-library-flair-b541d7add21f\n",
    "\n",
    "# https://github.com/zalandoresearch/flair/blob/master/resources/docs/TUTORIAL_7_TRAINING_A_MODEL.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/max/git/modelcompare/dataset_hatespeech/input/'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data_folder = infolder\n",
    "data_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-23 11:58:35,988 Reading data from /home/max/git/modelcompare/dataset_hatespeech/input\n",
      "2020-06-23 11:58:35,989 Train: /home/max/git/modelcompare/dataset_hatespeech/input/flair_train.csv\n",
      "2020-06-23 11:58:35,991 Dev: /home/max/git/modelcompare/dataset_hatespeech/input/flair_dev.csv\n",
      "2020-06-23 11:58:35,992 Test: /home/max/git/modelcompare/dataset_hatespeech/input/flair_test.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated function (or staticmethod) load_classification_corpus. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/flair/data_fetcher.py:452: DeprecationWarning: Call to deprecated function (or staticmethod) read_text_classification_file. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  train_file, tokenizer=tokenizer, max_tokens_per_doc=max_tokens_per_doc\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/flair/data_fetcher.py:457: DeprecationWarning: Call to deprecated function (or staticmethod) read_text_classification_file. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  test_file, tokenizer=tokenizer, max_tokens_per_doc=max_tokens_per_doc\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/flair/data_fetcher.py:464: DeprecationWarning: Call to deprecated function (or staticmethod) read_text_classification_file. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  dev_file, tokenizer=tokenizer, max_tokens_per_doc=max_tokens_per_doc\n"
     ]
    }
   ],
   "source": [
    "corpus = NLPTaskDataFetcher.load_classification_corpus(data_folder, test_file='flair_test.csv', dev_file='flair_dev.csv', train_file='flair_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus: 100 train + 3000 dev + 3000 test sentences\n",
      "2020-06-23 11:58:51,988 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 79724.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-23 11:58:52,005 [b'1', b'2', b'0']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(corpus)\n",
    "#print(len(corpus.train))\n",
    "\n",
    "# 2. create the label dictionary\n",
    "label_dict = corpus.make_label_dictionary()\n",
    "#print(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Individual model only, skip when in batch"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "classifier = TextClassifier(document_embeddings, label_dictionary=corpus.make_label_dictionary(),\n",
    "                            multi_label=False)\n",
    "trainer = ModelTrainer(classifier, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier = TextClassifier(stacked_embeddings, label_dictionary=corpus.make_label_dictionary(),\n",
    "#                            multi_label=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "start = time.time()\n",
    "\n",
    "# max epochs 5 for testing\n",
    "trainer.train('./', \n",
    "              learning_rate=0.1,\n",
    "              mini_batch_size=32, # 32  # BERT OOM even with 16 batch -> need 8. others run on 32 or even more\n",
    "              anneal_factor=0.5,\n",
    "              patience=5,     \n",
    "              max_epochs=5,  #15\n",
    "              ) #max_epochs=150\n",
    "\n",
    "duration = time.time()-start\n",
    "print(f'Duration {duration:.2f} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glove alone 0.902"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Winners: Fasttext: en-crawl, 0.9129 solo\n",
    "\n",
    "# word embeddings pooled:glove+fasttext 'test_score': 0.8892,\n",
    "\n",
    "# glove, fasttext:en-crawl, flair-multi-forward, elmo('original') f1-score 0.9155\n",
    "# ELMO(original) test_score': 0.9002 (f-score 0.9145 )\n",
    "# flair-news-forward WITHOUT TOKENIZATION, 'test_score': 0.8526\n",
    "# flair-news-forward: 'test_score': 0.8741\n",
    "# BERT only + RNN 256, bidir=True, 'test_score': 0.904,\n",
    "# Tweet + Bert same, not very great, used 128 rnn?\n",
    "#.\n",
    "# 0.8911 flair + BPE\n",
    "\n",
    "\n",
    "##############################\n",
    "\n",
    "#NEW BEST adding preprocess puncts+quotes, glove, en-twitter, en-crawl:   test_score': 0.9242,\n",
    "# no stemming\n",
    "\n",
    "\n",
    "# BEST glove, en-twitter, en-crawl, flair-multi-forward:  f1-score 0.9129\n",
    "# add stemming f-score 0.7808   Breaking down, very bad score\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------\n",
    "\n",
    "# Added Preprosessing, no stemming\n",
    "# 'test_score': 0.9072 glove, en-twitter , flair-multi-forward\n",
    "\n",
    "# Add to this, en-crawl Fastext  # f1-score 0.9129 BEST\n",
    "\n",
    "# Adding Kominov embedding: 'test_score': 0.8868 -> weaker considerably, by 0.03\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Compare single embeddings\n",
    "# en-twitter          'test_score': 0.8943\n",
    "# en-crawl, Fasttext 'test_score': 0.9064,\n",
    "# flai-multi-forward: 'test_score': 0.7738  WEAK! WHY? Also slow\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "\n",
    "# Glove, en-twitter, en-crawl: 'test_score': 0.8865,  (20 epoch)\n",
    "# Glove, en-twitter, en-crawl: same + 2-way FLAIR  'test_score': 0.8854 - weaker, but 15 epoch\n",
    "\n",
    "# bert only, no glove: slow, 'test_score': 0.7329\n",
    "# Flair only, no glove: 'test_score': 0.8158  (0.06 lower than with Glove)\n",
    "\n",
    "# Glove, Flair-multi 2ways, Bert-base-cased : OOM\n",
    "\n",
    "# test_score': 0.8763, Glove + doc-embedding multi \n",
    "\n",
    "# test_score': 0.8731, Glove + doc-embedding news\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.train('./', max_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveResults(savelist, name='all_default'):\n",
    "    import shelve\n",
    "    # file to be used\n",
    "    filename= name+'.shlf'\n",
    "    shelf = shelve.open(outfolder+filename)\n",
    "    #shelf = shelve.open(\"all_flair.shlf\")\n",
    "    # serializing\n",
    "    #shelf[\"all_flair\"] = all_flair\n",
    "    shelf[name] = savelist\n",
    "    shelf.close() # you must close the shelve file!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadResults(name='all_default'):\n",
    "    import shelve\n",
    "    filename= name+'.shlf'\n",
    "    shelf = shelve.open(outfolder+filename) \n",
    "    new = shelf[name]\n",
    "    shelf.close()\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Train with each embedding in the list, predict, add results to the list\n",
    "\n",
    "Parameters: word_embeddings,    eg   WordEmbeddings('glove')\n",
    "            modelname and modeldesc - text to be added in results\n",
    "            savelist : list where results are appended. Can be empty or already including results\n",
    "            epohcs: epochs to run\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "def train_and_predict(embeddings, modelname, modeldesc, savelist, epochs=15, batch_size=32):\n",
    "    \n",
    "    print(modelname)\n",
    "    start = time.time()\n",
    "    \n",
    "# PREPARE\n",
    "    document_embeddings = DocumentRNNEmbeddings(word_embeddings, hidden_size=512,\n",
    "                                           bidirectional = False,\n",
    "                                           rnn_type='LSTM', \n",
    "                                           reproject_words=True, reproject_words_dimension=256                                                    \n",
    "                                           )\n",
    "\n",
    "    seed_everything(SEED)\n",
    "    corpus = NLPTaskDataFetcher.load_classification_corpus(data_folder, test_file='flair_test.csv', dev_file='flair_dev.csv', train_file='flair_train.csv')\n",
    "\n",
    "    seed_everything(SEED)\n",
    "    label_dict = corpus.make_label_dictionary()\n",
    "\n",
    "    seed_everything(SEED)\n",
    "    classifier = TextClassifier(document_embeddings, label_dictionary=corpus.make_label_dictionary(),\n",
    "                            multi_label=False)\n",
    "    trainer = ModelTrainer(classifier, corpus)\n",
    "    \n",
    "# TRAINING\n",
    "    seed_everything(SEED)\n",
    "    trainer.train('./', \n",
    "              learning_rate=0.1,\n",
    "              mini_batch_size=batch_size, # 32  # BERT OOM even with 16 batch -> need 8. others run on 32 or even more\n",
    "              anneal_factor=0.5,\n",
    "              patience=5,     \n",
    "              max_epochs=epochs,  #15\n",
    "              ) #max_epochs=150\n",
    "\n",
    "    duration_train = time.time()-start\n",
    "    \n",
    "# PREDICT\n",
    "    start_pred = time.time()\n",
    "\n",
    "    # turn text into Flairs \"Sentence object\"\n",
    "    test['flair_sentence'] = test['text'].apply(lambda x: Sentence(x))\n",
    "\n",
    "    # discard output, result is put into object itself\n",
    "    _ = test['flair_sentence'].apply(lambda x: classifier.predict(x))\n",
    "\n",
    "    # sentence.labels returns a list containing flairs Label object that includes a dict. \n",
    "    # dig the values for predicted label + confidence from within the dict\n",
    "    # the 'value' returns a str, cast it to int\n",
    "    test['yhat'] = test['flair_sentence'].apply(lambda x: int(x.labels[0].to_dict()['value']))\n",
    "\n",
    "    test['confidence'] = test['flair_sentence'].apply(lambda x: x.labels[0].to_dict()['confidence'])\n",
    "\n",
    "    results = pd.DataFrame(test[['yhat', 'confidence']])\n",
    "    results.columns=['label','confidence']\n",
    "    results.head()\n",
    "    \n",
    "# ADD RESULTS TO LIST\n",
    "\n",
    "    duration_predict = time.time() - start_pred\n",
    "    #print(f'Duration {duration:.2f} s')\n",
    "\n",
    "    savelist.append({'model': modelname,\n",
    "                'labels': results['label'],\n",
    "                'confidence': results['confidence'],\n",
    "                'traintime': duration_train,\n",
    "                'predtime3k': duration_predict,\n",
    "                'modeldesc': modeldesc\n",
    "               }\n",
    "              )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Using flairs Label object\n",
    "\n",
    "sentence.labels returns a flairs Label object:  flair.data.Label, type of [1 (1.0)]\n",
    "to access its values, use .to_dict()\n",
    "\n",
    "sentence.labels[0].to_dict()\n",
    "sent.labels[0].to_dict()['value']\n",
    "sent.labels[0].to_dict()['confidence']\n",
    "\n",
    "Label object is returned inside a list, so need to use [0] first to access it. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# This adds the results into the Sentence-object itself!\n",
    "classifier.predict(sentence)\n",
    "\n",
    "sentence.labels\n",
    "[1 (1.0)]\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# if run, downloads also the embeddings\n",
    "embeddings_list = [WordEmbeddings('glove'), \n",
    "                   WordEmbeddings('en-twitter'),\n",
    "                   WordEmbeddings('en'),       # FastText embeddings over news and wikipedia data                \n",
    "                   WordEmbeddings('en-crawl'), # FastText embeddings over Web crawls\n",
    "                   #WordEmbeddings('en-extvec'), #Komnios embeddings\n",
    "                    \n",
    "                   # Flair can get to 20-40 GB on disk for each direction!\n",
    "                   FlairEmbeddings('multi-forward'), \n",
    "                   # FlairEmbeddings('multi-backward'), \n",
    "                   #FlairEmbeddings('news-forward-fast'), \n",
    "                   #FlairEmbeddings('news-backward-fast'),\n",
    "                   \n",
    "                   #BPE - takes memory - reduce batch size radically!\n",
    "                   BytePairEmbeddings('en'),\n",
    "\n",
    "                   BertEmbeddings('bert-base-cased'),    \n",
    "                   ELMoEmbeddings('original')\n",
    "                  ]\n",
    "\n",
    "# Good ones\n",
    "# glove, en-twitter, en-crawl  0.9242\n",
    "# Fasttext: en-crawl, 0.9129 solo\n",
    "embeddings_list"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Timings\n",
    "en-crawl 5 epoch, RNN 256, bi:false,  Duration 146.53 s\n",
    "raised to RNN 512,                    Duration 146.97 s  almost identical, 256 vs 512 \n",
    "MICRO_AVG: acc 0.8365 - f1-score 0.911\n",
    "\n",
    "change to LSTM: Duration 150.04 s\n",
    "MICRO_AVG: acc 0.8377 - f1-score 0.9117\n",
    "\n",
    "epochs to 15 from 5        - train time abotu 3x, f1 +0.003\n",
    "Duration 433.72 s\n",
    "MICRO_AVG: acc 0.8428 - f1-score 0.9147\n",
    "\n",
    "\n",
    "# BI DIRECTIONAL. not increasin, might require larger model and/or more epochs.\n",
    "bi-direcitonal\n",
    "Duration 525.53 s\n",
    "MICRO_AVG: acc 0.8365 - f1-score 0.911 \n",
    "# ANOTHER 15 EPOCHS RAISES F1 by 0.002. still not as good as uni-directional was!\n",
    "# if cont another 15 epochs: MICRO_AVG: acc 0.8411 - f1-score 0.9137\n",
    "\n",
    "bi-directional (RNN version)\n",
    "Duration 502.94\n",
    "MICRO_AVG: acc 0.8349 - f1-score 0.91\n",
    "\n",
    "\n",
    "# new try, seed 1\n",
    "RNN MICRO_AVG: acc 0.8051 - f1-score 0.892\n",
    "\n",
    "\n",
    "# BATCH SIZE 128\n",
    "MICRO_AVG: acc 0.7953 - f1-score 0.886\n",
    "Duration 388.62 s\n",
    "en-crawl\n",
    "Huge drop when raising batch from 16 to 128: 0.91 to 0.886, both 15 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "savelist = [] # list to save results\n",
    "modeldesc = '512LSTM_15epoch_non-bi'\n",
    "#BATCH_SIZE=32\n",
    "EPOCHS = 15"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "word_embeddings = [ WordEmbeddings('en-crawl'),                 ]\n",
    "modelname = 'fasttext web-crawl'\n",
    "train_and_predict(word_embeddings, modelname, modeldesc, savelist=savelist, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To use ELMoEmbeddings, please first install with \"pip install allennlp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove\n",
      "2020-06-23 11:59:33,094 Reading data from /home/max/git/modelcompare/dataset_hatespeech/input\n",
      "2020-06-23 11:59:33,095 Train: /home/max/git/modelcompare/dataset_hatespeech/input/flair_train.csv\n",
      "2020-06-23 11:59:33,096 Dev: /home/max/git/modelcompare/dataset_hatespeech/input/flair_dev.csv\n",
      "2020-06-23 11:59:33,097 Test: /home/max/git/modelcompare/dataset_hatespeech/input/flair_test.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/ipykernel_launcher.py:24: DeprecationWarning: Call to deprecated function (or staticmethod) load_classification_corpus. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/flair/data_fetcher.py:452: DeprecationWarning: Call to deprecated function (or staticmethod) read_text_classification_file. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  train_file, tokenizer=tokenizer, max_tokens_per_doc=max_tokens_per_doc\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/flair/data_fetcher.py:457: DeprecationWarning: Call to deprecated function (or staticmethod) read_text_classification_file. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  test_file, tokenizer=tokenizer, max_tokens_per_doc=max_tokens_per_doc\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/flair/data_fetcher.py:464: DeprecationWarning: Call to deprecated function (or staticmethod) read_text_classification_file. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  dev_file, tokenizer=tokenizer, max_tokens_per_doc=max_tokens_per_doc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-23 11:59:34,817 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 168988.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-23 11:59:34,821 [b'1', b'2', b'0']\n",
      "2020-06-23 11:59:34,821 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 100/100 [00:00<00:00, 96799.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-23 11:59:34,825 [b'1', b'2', b'0']\n",
      "2020-06-23 11:59:34,829 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 11:59:34,831 Model: \"TextClassifier(\n",
      "  (document_embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): WordEmbeddings('glove')\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=100, out_features=256, bias=True)\n",
      "    (rnn): LSTM(256, 512, batch_first=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=512, out_features=3, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\"\n",
      "2020-06-23 11:59:34,831 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 11:59:34,832 Corpus: \"Corpus: 100 train + 3000 dev + 3000 test sentences\"\n",
      "2020-06-23 11:59:34,833 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 11:59:34,834 Parameters:\n",
      "2020-06-23 11:59:34,835  - learning_rate: \"0.1\"\n",
      "2020-06-23 11:59:34,836  - mini_batch_size: \"32\"\n",
      "2020-06-23 11:59:34,837  - patience: \"5\"\n",
      "2020-06-23 11:59:34,837  - anneal_factor: \"0.5\"\n",
      "2020-06-23 11:59:34,838  - max_epochs: \"15\"\n",
      "2020-06-23 11:59:34,839  - shuffle: \"True\"\n",
      "2020-06-23 11:59:34,842  - train_with_dev: \"False\"\n",
      "2020-06-23 11:59:34,843  - batch_growth_annealing: \"False\"\n",
      "2020-06-23 11:59:34,845 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 11:59:34,846 Model training base path: \".\"\n",
      "2020-06-23 11:59:34,846 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 11:59:34,847 Device: cuda:0\n",
      "2020-06-23 11:59:34,848 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 11:59:34,849 Embeddings storage mode: cpu\n",
      "2020-06-23 11:59:34,850 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 11:59:35,019 epoch 1 - iter 0/4 - loss 1.11143219 - samples/sec: 192.28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-23 11:59:35,096 epoch 1 - iter 1/4 - loss 0.98181307 - samples/sec: 458.72\n",
      "2020-06-23 11:59:35,166 epoch 1 - iter 2/4 - loss 0.95769475 - samples/sec: 503.09\n",
      "2020-06-23 11:59:35,183 epoch 1 - iter 3/4 - loss 0.90562998 - samples/sec: 3110.85\n",
      "2020-06-23 11:59:35,194 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 11:59:35,195 EPOCH 1 done: loss 0.9056 - lr 0.1000\n",
      "2020-06-23 11:59:39,585 DEV : loss 0.7174628973007202 - score 0.7667\n",
      "2020-06-23 11:59:39,718 BAD EPOCHS (no improvement): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DocumentRNNEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type StackedEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type WordEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-23 11:59:42,639 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 11:59:42,683 epoch 2 - iter 0/4 - loss 0.70180404 - samples/sec: 769.43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-23 11:59:42,728 epoch 2 - iter 1/4 - loss 0.77091384 - samples/sec: 889.32\n",
      "2020-06-23 11:59:42,775 epoch 2 - iter 2/4 - loss 0.76606268 - samples/sec: 801.06\n",
      "2020-06-23 11:59:42,792 epoch 2 - iter 3/4 - loss 0.67171406 - samples/sec: 3102.44\n",
      "2020-06-23 11:59:42,798 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 11:59:42,799 EPOCH 2 done: loss 0.6717 - lr 0.1000\n",
      "2020-06-23 11:59:44,594 DEV : loss 0.6841868162155151 - score 0.7667\n",
      "2020-06-23 11:59:44,726 BAD EPOCHS (no improvement): 1\n",
      "2020-06-23 11:59:47,907 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 11:59:47,951 epoch 3 - iter 0/4 - loss 0.82007998 - samples/sec: 757.97\n",
      "2020-06-23 11:59:47,998 epoch 3 - iter 1/4 - loss 0.72963786 - samples/sec: 849.77\n",
      "2020-06-23 11:59:48,047 epoch 3 - iter 2/4 - loss 0.68627578 - samples/sec: 810.01\n",
      "2020-06-23 11:59:48,063 epoch 3 - iter 3/4 - loss 0.77841528 - samples/sec: 3166.63\n",
      "2020-06-23 11:59:48,074 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 11:59:48,074 EPOCH 3 done: loss 0.7784 - lr 0.1000\n",
      "2020-06-23 11:59:49,849 DEV : loss 0.7442651987075806 - score 0.767\n",
      "2020-06-23 11:59:49,985 BAD EPOCHS (no improvement): 0\n",
      "2020-06-23 11:59:53,263 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 11:59:53,311 epoch 4 - iter 0/4 - loss 0.73708606 - samples/sec: 703.65\n",
      "2020-06-23 11:59:53,362 epoch 4 - iter 1/4 - loss 0.78099963 - samples/sec: 887.45\n",
      "2020-06-23 11:59:53,409 epoch 4 - iter 2/4 - loss 0.70729492 - samples/sec: 785.73\n",
      "2020-06-23 11:59:53,429 epoch 4 - iter 3/4 - loss 0.66640107 - samples/sec: 3737.20\n",
      "2020-06-23 11:59:53,434 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 11:59:53,435 EPOCH 4 done: loss 0.6664 - lr 0.1000\n",
      "2020-06-23 11:59:55,355 DEV : loss 0.6583921909332275 - score 0.7667\n",
      "2020-06-23 11:59:55,495 BAD EPOCHS (no improvement): 1\n",
      "2020-06-23 11:59:55,496 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 11:59:55,534 epoch 5 - iter 0/4 - loss 0.81126213 - samples/sec: 854.56\n",
      "2020-06-23 11:59:55,575 epoch 5 - iter 1/4 - loss 0.77141699 - samples/sec: 919.92\n",
      "2020-06-23 11:59:55,624 epoch 5 - iter 2/4 - loss 0.69163905 - samples/sec: 755.14\n",
      "2020-06-23 11:59:55,645 epoch 5 - iter 3/4 - loss 0.59015536 - samples/sec: 2116.83\n",
      "2020-06-23 11:59:55,657 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 11:59:55,659 EPOCH 5 done: loss 0.5902 - lr 0.1000\n",
      "2020-06-23 11:59:57,594 DEV : loss 0.6952734589576721 - score 0.7667\n",
      "2020-06-23 11:59:57,731 BAD EPOCHS (no improvement): 2\n",
      "2020-06-23 11:59:57,732 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 11:59:57,769 epoch 6 - iter 0/4 - loss 0.51792979 - samples/sec: 910.80\n",
      "2020-06-23 11:59:57,816 epoch 6 - iter 1/4 - loss 0.53096768 - samples/sec: 799.85\n",
      "2020-06-23 11:59:57,858 epoch 6 - iter 2/4 - loss 0.66191081 - samples/sec: 912.84\n",
      "2020-06-23 11:59:57,878 epoch 6 - iter 3/4 - loss 0.62708585 - samples/sec: 2924.45\n",
      "2020-06-23 11:59:57,885 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 11:59:57,886 EPOCH 6 done: loss 0.6271 - lr 0.1000\n",
      "2020-06-23 11:59:59,756 DEV : loss 0.6570211052894592 - score 0.7667\n",
      "2020-06-23 11:59:59,887 BAD EPOCHS (no improvement): 3\n",
      "2020-06-23 11:59:59,888 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 11:59:59,928 epoch 7 - iter 0/4 - loss 0.63475960 - samples/sec: 834.39\n",
      "2020-06-23 11:59:59,966 epoch 7 - iter 1/4 - loss 0.73614424 - samples/sec: 1159.89\n",
      "2020-06-23 12:00:00,017 epoch 7 - iter 2/4 - loss 0.67564134 - samples/sec: 815.32\n",
      "2020-06-23 12:00:00,036 epoch 7 - iter 3/4 - loss 0.66324094 - samples/sec: 2790.80\n",
      "2020-06-23 12:00:00,047 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:00:00,048 EPOCH 7 done: loss 0.6632 - lr 0.1000\n",
      "2020-06-23 12:00:01,836 DEV : loss 0.6597952246665955 - score 0.7667\n",
      "2020-06-23 12:00:01,968 BAD EPOCHS (no improvement): 4\n",
      "2020-06-23 12:00:01,970 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:00:02,012 epoch 8 - iter 0/4 - loss 0.64637500 - samples/sec: 779.49\n",
      "2020-06-23 12:00:02,055 epoch 8 - iter 1/4 - loss 0.75073957 - samples/sec: 899.33\n",
      "2020-06-23 12:00:02,102 epoch 8 - iter 2/4 - loss 0.66666432 - samples/sec: 884.60\n",
      "2020-06-23 12:00:02,124 epoch 8 - iter 3/4 - loss 0.65378937 - samples/sec: 2967.65\n",
      "2020-06-23 12:00:02,130 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:00:02,130 EPOCH 8 done: loss 0.6538 - lr 0.1000\n",
      "2020-06-23 12:00:03,930 DEV : loss 0.6661903858184814 - score 0.767\n",
      "2020-06-23 12:00:04,061 BAD EPOCHS (no improvement): 5\n",
      "2020-06-23 12:00:07,350 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:00:07,394 epoch 9 - iter 0/4 - loss 0.47503069 - samples/sec: 775.32\n",
      "2020-06-23 12:00:07,439 epoch 9 - iter 1/4 - loss 0.56038608 - samples/sec: 952.52\n",
      "2020-06-23 12:00:07,485 epoch 9 - iter 2/4 - loss 0.59851137 - samples/sec: 786.06\n",
      "2020-06-23 12:00:07,503 epoch 9 - iter 3/4 - loss 0.75419392 - samples/sec: 2705.78\n",
      "2020-06-23 12:00:07,511 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:00:07,512 EPOCH 9 done: loss 0.7542 - lr 0.1000\n",
      "2020-06-23 12:00:09,337 DEV : loss 0.7121744751930237 - score 0.7453\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-02.\n",
      "2020-06-23 12:00:09,470 BAD EPOCHS (no improvement): 6\n",
      "2020-06-23 12:00:09,471 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:00:09,511 epoch 10 - iter 0/4 - loss 0.60178679 - samples/sec: 826.32\n",
      "2020-06-23 12:00:09,550 epoch 10 - iter 1/4 - loss 0.73913124 - samples/sec: 1004.59\n",
      "2020-06-23 12:00:09,597 epoch 10 - iter 2/4 - loss 0.66735015 - samples/sec: 825.11\n",
      "2020-06-23 12:00:09,618 epoch 10 - iter 3/4 - loss 0.62965752 - samples/sec: 3266.99\n",
      "2020-06-23 12:00:09,624 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:00:09,625 EPOCH 10 done: loss 0.6297 - lr 0.0500\n",
      "2020-06-23 12:00:11,477 DEV : loss 0.690990686416626 - score 0.7647\n",
      "2020-06-23 12:00:11,610 BAD EPOCHS (no improvement): 1\n",
      "2020-06-23 12:00:11,611 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:00:11,646 epoch 11 - iter 0/4 - loss 0.47148287 - samples/sec: 962.85\n",
      "2020-06-23 12:00:11,690 epoch 11 - iter 1/4 - loss 0.57875806 - samples/sec: 907.63\n",
      "2020-06-23 12:00:11,741 epoch 11 - iter 2/4 - loss 0.68115360 - samples/sec: 805.84\n",
      "2020-06-23 12:00:11,757 epoch 11 - iter 3/4 - loss 0.66729118 - samples/sec: 3064.33\n",
      "2020-06-23 12:00:11,766 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:00:11,767 EPOCH 11 done: loss 0.6673 - lr 0.0500\n",
      "2020-06-23 12:00:13,672 DEV : loss 0.6641004085540771 - score 0.7673\n",
      "2020-06-23 12:00:13,806 BAD EPOCHS (no improvement): 0\n",
      "2020-06-23 12:00:17,045 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:00:17,088 epoch 12 - iter 0/4 - loss 0.57947063 - samples/sec: 779.32\n",
      "2020-06-23 12:00:17,129 epoch 12 - iter 1/4 - loss 0.60031205 - samples/sec: 1005.48\n",
      "2020-06-23 12:00:17,165 epoch 12 - iter 2/4 - loss 0.63953676 - samples/sec: 1114.20\n",
      "2020-06-23 12:00:17,180 epoch 12 - iter 3/4 - loss 0.59695444 - samples/sec: 3476.78\n",
      "2020-06-23 12:00:17,186 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:00:17,187 EPOCH 12 done: loss 0.5970 - lr 0.0500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-23 12:00:18,943 DEV : loss 0.6587111353874207 - score 0.7673\n",
      "2020-06-23 12:00:19,079 BAD EPOCHS (no improvement): 1\n",
      "2020-06-23 12:00:22,376 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:00:22,417 epoch 13 - iter 0/4 - loss 0.62206751 - samples/sec: 834.50\n",
      "2020-06-23 12:00:22,470 epoch 13 - iter 1/4 - loss 0.61146039 - samples/sec: 782.17\n",
      "2020-06-23 12:00:22,522 epoch 13 - iter 2/4 - loss 0.62978804 - samples/sec: 801.99\n",
      "2020-06-23 12:00:22,539 epoch 13 - iter 3/4 - loss 0.70366386 - samples/sec: 3076.91\n",
      "2020-06-23 12:00:22,550 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:00:22,551 EPOCH 13 done: loss 0.7037 - lr 0.0500\n",
      "2020-06-23 12:00:24,551 DEV : loss 0.7141181230545044 - score 0.7213\n",
      "2020-06-23 12:00:24,683 BAD EPOCHS (no improvement): 2\n",
      "2020-06-23 12:00:24,685 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:00:24,723 epoch 14 - iter 0/4 - loss 0.60347623 - samples/sec: 858.80\n",
      "2020-06-23 12:00:24,765 epoch 14 - iter 1/4 - loss 0.63610950 - samples/sec: 903.83\n",
      "2020-06-23 12:00:24,808 epoch 14 - iter 2/4 - loss 0.63462786 - samples/sec: 945.47\n",
      "2020-06-23 12:00:24,825 epoch 14 - iter 3/4 - loss 0.60887598 - samples/sec: 3195.28\n",
      "2020-06-23 12:00:24,836 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:00:24,837 EPOCH 14 done: loss 0.6089 - lr 0.0500\n",
      "2020-06-23 12:00:26,770 DEV : loss 0.6705648899078369 - score 0.7667\n",
      "2020-06-23 12:00:26,899 BAD EPOCHS (no improvement): 3\n",
      "2020-06-23 12:00:26,900 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:00:26,937 epoch 15 - iter 0/4 - loss 0.57074881 - samples/sec: 915.37\n",
      "2020-06-23 12:00:26,981 epoch 15 - iter 1/4 - loss 0.58889750 - samples/sec: 907.07\n",
      "2020-06-23 12:00:27,030 epoch 15 - iter 2/4 - loss 0.59288319 - samples/sec: 848.27\n",
      "2020-06-23 12:00:27,048 epoch 15 - iter 3/4 - loss 0.65846521 - samples/sec: 2995.80\n",
      "2020-06-23 12:00:27,060 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:00:27,061 EPOCH 15 done: loss 0.6585 - lr 0.0500\n",
      "2020-06-23 12:00:28,838 DEV : loss 0.7048906087875366 - score 0.7247\n",
      "2020-06-23 12:00:28,970 BAD EPOCHS (no improvement): 4\n",
      "2020-06-23 12:00:31,908 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:00:31,909 Testing using best model ...\n",
      "2020-06-23 12:00:31,909 loading file best-model.pt\n",
      "2020-06-23 12:00:34,055 0.759\t0.759\t0.759\n",
      "2020-06-23 12:00:34,056 \n",
      "MICRO_AVG: acc 0.6116 - f1-score 0.759\n",
      "MACRO_AVG: acc 0.253 - f1-score 0.2876666666666667\n",
      "0          tp: 0 - fp: 0 - fn: 173 - tn: 2827 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000\n",
      "1          tp: 2277 - fp: 721 - fn: 2 - tn: 0 - precision: 0.7595 - recall: 0.9991 - accuracy: 0.7590 - f1-score: 0.8630\n",
      "2          tp: 0 - fp: 2 - fn: 548 - tn: 2450 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000\n",
      "2020-06-23 12:00:34,057 ----------------------------------------------------------------------------------------------------\n",
      "fasttext web-crawl\n",
      "2020-06-23 12:00:43,881 Reading data from /home/max/git/modelcompare/dataset_hatespeech/input\n",
      "2020-06-23 12:00:43,882 Train: /home/max/git/modelcompare/dataset_hatespeech/input/flair_train.csv\n",
      "2020-06-23 12:00:43,882 Dev: /home/max/git/modelcompare/dataset_hatespeech/input/flair_dev.csv\n",
      "2020-06-23 12:00:43,883 Test: /home/max/git/modelcompare/dataset_hatespeech/input/flair_test.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/ipykernel_launcher.py:24: DeprecationWarning: Call to deprecated function (or staticmethod) load_classification_corpus. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/flair/data_fetcher.py:452: DeprecationWarning: Call to deprecated function (or staticmethod) read_text_classification_file. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  train_file, tokenizer=tokenizer, max_tokens_per_doc=max_tokens_per_doc\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/flair/data_fetcher.py:457: DeprecationWarning: Call to deprecated function (or staticmethod) read_text_classification_file. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  test_file, tokenizer=tokenizer, max_tokens_per_doc=max_tokens_per_doc\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/flair/data_fetcher.py:464: DeprecationWarning: Call to deprecated function (or staticmethod) read_text_classification_file. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  dev_file, tokenizer=tokenizer, max_tokens_per_doc=max_tokens_per_doc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-23 12:00:45,396 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 162759.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-23 12:00:45,399 [b'1', b'2', b'0']\n",
      "2020-06-23 12:00:45,400 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 100/100 [00:00<00:00, 176527.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-23 12:00:45,404 [b'1', b'2', b'0']\n",
      "2020-06-23 12:00:45,406 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:00:45,408 Model: \"TextClassifier(\n",
      "  (document_embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): WordEmbeddings('en-crawl')\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=300, out_features=256, bias=True)\n",
      "    (rnn): LSTM(256, 512, batch_first=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=512, out_features=3, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\"\n",
      "2020-06-23 12:00:45,408 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:00:45,409 Corpus: \"Corpus: 100 train + 3000 dev + 3000 test sentences\"\n",
      "2020-06-23 12:00:45,410 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:00:45,410 Parameters:\n",
      "2020-06-23 12:00:45,411  - learning_rate: \"0.1\"\n",
      "2020-06-23 12:00:45,412  - mini_batch_size: \"32\"\n",
      "2020-06-23 12:00:45,412  - patience: \"5\"\n",
      "2020-06-23 12:00:45,413  - anneal_factor: \"0.5\"\n",
      "2020-06-23 12:00:45,414  - max_epochs: \"15\"\n",
      "2020-06-23 12:00:45,414  - shuffle: \"True\"\n",
      "2020-06-23 12:00:45,415  - train_with_dev: \"False\"\n",
      "2020-06-23 12:00:45,415  - batch_growth_annealing: \"False\"\n",
      "2020-06-23 12:00:45,416 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:00:45,416 Model training base path: \".\"\n",
      "2020-06-23 12:00:45,418 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:00:45,419 Device: cuda:0\n",
      "2020-06-23 12:00:45,420 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:00:45,420 Embeddings storage mode: cpu\n",
      "2020-06-23 12:00:45,423 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:00:45,486 epoch 1 - iter 0/4 - loss 1.16925943 - samples/sec: 515.08\n",
      "2020-06-23 12:00:45,559 epoch 1 - iter 1/4 - loss 1.09272456 - samples/sec: 485.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-23 12:00:45,625 epoch 1 - iter 2/4 - loss 1.05136242 - samples/sec: 532.06\n",
      "2020-06-23 12:00:45,640 epoch 1 - iter 3/4 - loss 1.00603278 - samples/sec: 3371.88\n",
      "2020-06-23 12:00:45,646 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:00:45,646 EPOCH 1 done: loss 1.0060 - lr 0.1000\n",
      "2020-06-23 12:00:49,757 DEV : loss 0.8139700889587402 - score 0.7667\n",
      "2020-06-23 12:00:49,895 BAD EPOCHS (no improvement): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DocumentRNNEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type StackedEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type WordEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-23 12:01:00,538 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:01:00,584 epoch 2 - iter 0/4 - loss 0.79311574 - samples/sec: 725.07\n",
      "2020-06-23 12:01:00,623 epoch 2 - iter 1/4 - loss 0.82000515 - samples/sec: 979.06\n",
      "2020-06-23 12:01:00,675 epoch 2 - iter 2/4 - loss 0.80578156 - samples/sec: 796.47\n",
      "2020-06-23 12:01:00,694 epoch 2 - iter 3/4 - loss 0.72391286 - samples/sec: 2958.68\n",
      "2020-06-23 12:01:00,700 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:01:00,702 EPOCH 2 done: loss 0.7239 - lr 0.1000\n",
      "2020-06-23 12:01:02,619 DEV : loss 0.6994582414627075 - score 0.7667\n",
      "2020-06-23 12:01:02,765 BAD EPOCHS (no improvement): 1\n",
      "2020-06-23 12:01:13,604 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:01:13,665 epoch 3 - iter 0/4 - loss 0.79923809 - samples/sec: 546.64\n",
      "2020-06-23 12:01:13,707 epoch 3 - iter 1/4 - loss 0.75012681 - samples/sec: 877.26\n",
      "2020-06-23 12:01:13,754 epoch 3 - iter 2/4 - loss 0.69771830 - samples/sec: 813.84\n",
      "2020-06-23 12:01:13,773 epoch 3 - iter 3/4 - loss 0.79337007 - samples/sec: 2989.73\n",
      "2020-06-23 12:01:13,780 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:01:13,781 EPOCH 3 done: loss 0.7934 - lr 0.1000\n",
      "2020-06-23 12:01:15,775 DEV : loss 0.6915169358253479 - score 0.7667\n",
      "2020-06-23 12:01:15,912 BAD EPOCHS (no improvement): 2\n",
      "2020-06-23 12:01:26,632 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:01:26,681 epoch 4 - iter 0/4 - loss 0.76379246 - samples/sec: 676.39\n",
      "2020-06-23 12:01:26,728 epoch 4 - iter 1/4 - loss 0.78457645 - samples/sec: 786.57\n",
      "2020-06-23 12:01:26,780 epoch 4 - iter 2/4 - loss 0.69996162 - samples/sec: 800.33\n",
      "2020-06-23 12:01:26,800 epoch 4 - iter 3/4 - loss 0.69301069 - samples/sec: 3418.08\n",
      "2020-06-23 12:01:26,806 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:01:26,807 EPOCH 4 done: loss 0.6930 - lr 0.1000\n",
      "2020-06-23 12:01:28,733 DEV : loss 0.6718920469284058 - score 0.7667\n",
      "2020-06-23 12:01:28,871 BAD EPOCHS (no improvement): 3\n",
      "2020-06-23 12:01:39,792 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:01:39,853 epoch 5 - iter 0/4 - loss 0.83814287 - samples/sec: 548.32\n",
      "2020-06-23 12:01:39,910 epoch 5 - iter 1/4 - loss 0.79305160 - samples/sec: 630.89\n",
      "2020-06-23 12:01:39,958 epoch 5 - iter 2/4 - loss 0.69222931 - samples/sec: 811.21\n",
      "2020-06-23 12:01:39,986 epoch 5 - iter 3/4 - loss 0.59107142 - samples/sec: 2273.45\n",
      "2020-06-23 12:01:39,993 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:01:39,993 EPOCH 5 done: loss 0.5911 - lr 0.1000\n",
      "2020-06-23 12:01:41,992 DEV : loss 0.6699284315109253 - score 0.7667\n",
      "2020-06-23 12:01:42,125 BAD EPOCHS (no improvement): 4\n",
      "2020-06-23 12:01:52,938 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:01:52,983 epoch 6 - iter 0/4 - loss 0.57646233 - samples/sec: 743.61\n",
      "2020-06-23 12:01:53,030 epoch 6 - iter 1/4 - loss 0.57521221 - samples/sec: 812.79\n",
      "2020-06-23 12:01:53,081 epoch 6 - iter 2/4 - loss 0.66354370 - samples/sec: 782.33\n",
      "2020-06-23 12:01:53,102 epoch 6 - iter 3/4 - loss 0.62902467 - samples/sec: 2760.43\n",
      "2020-06-23 12:01:53,114 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:01:53,115 EPOCH 6 done: loss 0.6290 - lr 0.1000\n",
      "2020-06-23 12:01:55,121 DEV : loss 0.66375732421875 - score 0.7667\n",
      "2020-06-23 12:01:55,262 BAD EPOCHS (no improvement): 5\n",
      "2020-06-23 12:02:06,039 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:02:06,088 epoch 7 - iter 0/4 - loss 0.65033436 - samples/sec: 678.64\n",
      "2020-06-23 12:02:06,128 epoch 7 - iter 1/4 - loss 0.73859048 - samples/sec: 976.33\n",
      "2020-06-23 12:02:06,174 epoch 7 - iter 2/4 - loss 0.66952296 - samples/sec: 798.04\n",
      "2020-06-23 12:02:06,195 epoch 7 - iter 3/4 - loss 0.67240840 - samples/sec: 2634.98\n",
      "2020-06-23 12:02:06,203 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:02:06,204 EPOCH 7 done: loss 0.6724 - lr 0.1000\n",
      "2020-06-23 12:02:08,107 DEV : loss 0.6636840105056763 - score 0.7667\n",
      "Epoch     6: reducing learning rate of group 0 to 5.0000e-02.\n",
      "2020-06-23 12:02:08,245 BAD EPOCHS (no improvement): 6\n",
      "2020-06-23 12:02:19,136 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:02:19,185 epoch 8 - iter 0/4 - loss 0.66289496 - samples/sec: 685.26\n",
      "2020-06-23 12:02:19,236 epoch 8 - iter 1/4 - loss 0.75163540 - samples/sec: 735.22\n",
      "2020-06-23 12:02:19,286 epoch 8 - iter 2/4 - loss 0.67052835 - samples/sec: 847.76\n",
      "2020-06-23 12:02:19,305 epoch 8 - iter 3/4 - loss 0.66655192 - samples/sec: 2680.23\n",
      "2020-06-23 12:02:19,317 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:02:19,318 EPOCH 8 done: loss 0.6666 - lr 0.0500\n",
      "2020-06-23 12:02:21,259 DEV : loss 0.6623375415802002 - score 0.7667\n",
      "2020-06-23 12:02:21,397 BAD EPOCHS (no improvement): 1\n",
      "2020-06-23 12:02:32,270 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:02:32,318 epoch 9 - iter 0/4 - loss 0.52139026 - samples/sec: 696.93\n",
      "2020-06-23 12:02:32,365 epoch 9 - iter 1/4 - loss 0.61395931 - samples/sec: 827.79\n",
      "2020-06-23 12:02:32,412 epoch 9 - iter 2/4 - loss 0.62475803 - samples/sec: 800.75\n",
      "2020-06-23 12:02:32,430 epoch 9 - iter 3/4 - loss 0.77945296 - samples/sec: 2855.27\n",
      "2020-06-23 12:02:32,438 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:02:32,439 EPOCH 9 done: loss 0.7795 - lr 0.0500\n",
      "2020-06-23 12:02:34,450 DEV : loss 0.6630086302757263 - score 0.7667\n",
      "2020-06-23 12:02:34,586 BAD EPOCHS (no improvement): 2\n",
      "2020-06-23 12:02:45,401 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:02:45,450 epoch 10 - iter 0/4 - loss 0.56094503 - samples/sec: 684.21\n",
      "2020-06-23 12:02:45,492 epoch 10 - iter 1/4 - loss 0.73919922 - samples/sec: 985.95\n",
      "2020-06-23 12:02:45,546 epoch 10 - iter 2/4 - loss 0.66119275 - samples/sec: 667.51\n",
      "2020-06-23 12:02:45,562 epoch 10 - iter 3/4 - loss 0.63359751 - samples/sec: 3758.76\n",
      "2020-06-23 12:02:45,569 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:02:45,570 EPOCH 10 done: loss 0.6336 - lr 0.0500\n",
      "2020-06-23 12:02:47,748 DEV : loss 0.660490095615387 - score 0.7667\n",
      "2020-06-23 12:02:47,890 BAD EPOCHS (no improvement): 3\n",
      "2020-06-23 12:02:58,677 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:02:58,722 epoch 11 - iter 0/4 - loss 0.37756011 - samples/sec: 749.46\n",
      "2020-06-23 12:02:58,764 epoch 11 - iter 1/4 - loss 0.52425693 - samples/sec: 893.61\n",
      "2020-06-23 12:02:58,815 epoch 11 - iter 2/4 - loss 0.65349304 - samples/sec: 781.93\n",
      "2020-06-23 12:02:58,832 epoch 11 - iter 3/4 - loss 0.63975336 - samples/sec: 2982.48\n",
      "2020-06-23 12:02:58,842 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:02:58,843 EPOCH 11 done: loss 0.6398 - lr 0.0500\n",
      "2020-06-23 12:03:00,987 DEV : loss 0.6598548889160156 - score 0.7667\n",
      "2020-06-23 12:03:01,122 BAD EPOCHS (no improvement): 4\n",
      "2020-06-23 12:03:12,016 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:03:12,064 epoch 12 - iter 0/4 - loss 0.57707846 - samples/sec: 695.88\n",
      "2020-06-23 12:03:12,113 epoch 12 - iter 1/4 - loss 0.61690307 - samples/sec: 764.13\n",
      "2020-06-23 12:03:12,161 epoch 12 - iter 2/4 - loss 0.65459690 - samples/sec: 825.86\n",
      "2020-06-23 12:03:12,184 epoch 12 - iter 3/4 - loss 0.61083186 - samples/sec: 2587.08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-23 12:03:12,194 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:03:12,195 EPOCH 12 done: loss 0.6108 - lr 0.0500\n",
      "2020-06-23 12:03:14,097 DEV : loss 0.6591921448707581 - score 0.7667\n",
      "2020-06-23 12:03:14,236 BAD EPOCHS (no improvement): 5\n",
      "2020-06-23 12:03:25,160 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:03:25,201 epoch 13 - iter 0/4 - loss 0.63435638 - samples/sec: 811.54\n",
      "2020-06-23 12:03:25,262 epoch 13 - iter 1/4 - loss 0.61320490 - samples/sec: 650.55\n",
      "2020-06-23 12:03:25,320 epoch 13 - iter 2/4 - loss 0.63303649 - samples/sec: 692.31\n",
      "2020-06-23 12:03:25,335 epoch 13 - iter 3/4 - loss 0.70154411 - samples/sec: 3585.07\n",
      "2020-06-23 12:03:25,341 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:03:25,342 EPOCH 13 done: loss 0.7015 - lr 0.0500\n",
      "2020-06-23 12:03:27,537 DEV : loss 0.6621150970458984 - score 0.7667\n",
      "Epoch    12: reducing learning rate of group 0 to 2.5000e-02.\n",
      "2020-06-23 12:03:27,679 BAD EPOCHS (no improvement): 6\n",
      "2020-06-23 12:03:38,640 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:03:38,685 epoch 14 - iter 0/4 - loss 0.57648921 - samples/sec: 732.93\n",
      "2020-06-23 12:03:38,732 epoch 14 - iter 1/4 - loss 0.63270241 - samples/sec: 882.38\n",
      "2020-06-23 12:03:38,777 epoch 14 - iter 2/4 - loss 0.65185757 - samples/sec: 814.82\n",
      "2020-06-23 12:03:38,795 epoch 14 - iter 3/4 - loss 0.62301373 - samples/sec: 2867.78\n",
      "2020-06-23 12:03:38,806 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:03:38,807 EPOCH 14 done: loss 0.6230 - lr 0.0250\n",
      "2020-06-23 12:03:40,732 DEV : loss 0.6615248918533325 - score 0.7667\n",
      "2020-06-23 12:03:40,870 BAD EPOCHS (no improvement): 1\n",
      "2020-06-23 12:03:51,672 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:03:51,723 epoch 15 - iter 0/4 - loss 0.64331490 - samples/sec: 659.88\n",
      "2020-06-23 12:03:51,772 epoch 15 - iter 1/4 - loss 0.63445842 - samples/sec: 770.33\n",
      "2020-06-23 12:03:51,821 epoch 15 - iter 2/4 - loss 0.63381070 - samples/sec: 812.51\n",
      "2020-06-23 12:03:51,843 epoch 15 - iter 3/4 - loss 0.69398025 - samples/sec: 2696.49\n",
      "2020-06-23 12:03:51,849 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:03:51,850 EPOCH 15 done: loss 0.6940 - lr 0.0250\n",
      "2020-06-23 12:03:53,772 DEV : loss 0.6621159911155701 - score 0.7667\n",
      "2020-06-23 12:03:53,908 BAD EPOCHS (no improvement): 2\n",
      "2020-06-23 12:04:14,889 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:04:14,890 Testing using best model ...\n",
      "2020-06-23 12:04:14,891 loading file best-model.pt\n",
      "2020-06-23 12:04:19,500 0.7597\t0.7597\t0.7597\n",
      "2020-06-23 12:04:19,501 \n",
      "MICRO_AVG: acc 0.6125 - f1-score 0.7597\n",
      "MACRO_AVG: acc 0.2532 - f1-score 0.2878\n",
      "0          tp: 0 - fp: 0 - fn: 173 - tn: 2827 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000\n",
      "1          tp: 2279 - fp: 721 - fn: 0 - tn: 0 - precision: 0.7597 - recall: 1.0000 - accuracy: 0.7597 - f1-score: 0.8634\n",
      "2          tp: 0 - fp: 0 - fn: 548 - tn: 2452 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000\n",
      "2020-06-23 12:04:19,503 ----------------------------------------------------------------------------------------------------\n",
      "fasttext news/wiki\n",
      "2020-06-23 12:04:29,550 Reading data from /home/max/git/modelcompare/dataset_hatespeech/input\n",
      "2020-06-23 12:04:29,550 Train: /home/max/git/modelcompare/dataset_hatespeech/input/flair_train.csv\n",
      "2020-06-23 12:04:29,551 Dev: /home/max/git/modelcompare/dataset_hatespeech/input/flair_dev.csv\n",
      "2020-06-23 12:04:29,552 Test: /home/max/git/modelcompare/dataset_hatespeech/input/flair_test.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/ipykernel_launcher.py:24: DeprecationWarning: Call to deprecated function (or staticmethod) load_classification_corpus. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/flair/data_fetcher.py:452: DeprecationWarning: Call to deprecated function (or staticmethod) read_text_classification_file. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  train_file, tokenizer=tokenizer, max_tokens_per_doc=max_tokens_per_doc\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/flair/data_fetcher.py:457: DeprecationWarning: Call to deprecated function (or staticmethod) read_text_classification_file. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  test_file, tokenizer=tokenizer, max_tokens_per_doc=max_tokens_per_doc\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/flair/data_fetcher.py:464: DeprecationWarning: Call to deprecated function (or staticmethod) read_text_classification_file. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  dev_file, tokenizer=tokenizer, max_tokens_per_doc=max_tokens_per_doc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-23 12:04:31,926 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 89910.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-23 12:04:31,931 [b'1', b'2', b'0']\n",
      "2020-06-23 12:04:31,932 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 100/100 [00:00<00:00, 78855.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-23 12:04:31,937 [b'1', b'2', b'0']\n",
      "2020-06-23 12:04:31,939 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:04:31,940 Model: \"TextClassifier(\n",
      "  (document_embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): WordEmbeddings('en')\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=300, out_features=256, bias=True)\n",
      "    (rnn): LSTM(256, 512, batch_first=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=512, out_features=3, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\"\n",
      "2020-06-23 12:04:31,941 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:04:31,941 Corpus: \"Corpus: 100 train + 3000 dev + 3000 test sentences\"\n",
      "2020-06-23 12:04:31,942 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:04:31,942 Parameters:\n",
      "2020-06-23 12:04:31,943  - learning_rate: \"0.1\"\n",
      "2020-06-23 12:04:31,943  - mini_batch_size: \"32\"\n",
      "2020-06-23 12:04:31,944  - patience: \"5\"\n",
      "2020-06-23 12:04:31,944  - anneal_factor: \"0.5\"\n",
      "2020-06-23 12:04:31,945  - max_epochs: \"15\"\n",
      "2020-06-23 12:04:31,945  - shuffle: \"True\"\n",
      "2020-06-23 12:04:31,946  - train_with_dev: \"False\"\n",
      "2020-06-23 12:04:31,946  - batch_growth_annealing: \"False\"\n",
      "2020-06-23 12:04:31,946 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:04:31,947 Model training base path: \".\"\n",
      "2020-06-23 12:04:31,947 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:04:31,948 Device: cuda:0\n",
      "2020-06-23 12:04:31,948 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:04:31,950 Embeddings storage mode: cpu\n",
      "2020-06-23 12:04:31,951 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:04:32,007 epoch 1 - iter 0/4 - loss 1.12258577 - samples/sec: 577.16\n",
      "2020-06-23 12:04:32,079 epoch 1 - iter 1/4 - loss 1.06091189 - samples/sec: 490.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-23 12:04:32,150 epoch 1 - iter 2/4 - loss 1.03748167 - samples/sec: 484.59\n",
      "2020-06-23 12:04:32,166 epoch 1 - iter 3/4 - loss 1.00551321 - samples/sec: 3251.08\n",
      "2020-06-23 12:04:32,172 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:04:32,172 EPOCH 1 done: loss 1.0055 - lr 0.1000\n",
      "2020-06-23 12:04:35,456 DEV : loss 0.8411567211151123 - score 0.7667\n",
      "2020-06-23 12:04:35,601 BAD EPOCHS (no improvement): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DocumentRNNEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type StackedEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type WordEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-23 12:04:46,609 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:04:46,651 epoch 2 - iter 0/4 - loss 0.82936764 - samples/sec: 790.17\n",
      "2020-06-23 12:04:46,697 epoch 2 - iter 1/4 - loss 0.84591985 - samples/sec: 907.07\n",
      "2020-06-23 12:04:46,737 epoch 2 - iter 2/4 - loss 0.83511021 - samples/sec: 978.25\n",
      "2020-06-23 12:04:46,755 epoch 2 - iter 3/4 - loss 0.75467433 - samples/sec: 3250.22\n",
      "2020-06-23 12:04:46,761 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:04:46,762 EPOCH 2 done: loss 0.7547 - lr 0.1000\n",
      "2020-06-23 12:04:48,973 DEV : loss 0.7208600044250488 - score 0.7667\n",
      "2020-06-23 12:04:49,117 BAD EPOCHS (no improvement): 1\n",
      "2020-06-23 12:05:00,112 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:05:00,164 epoch 3 - iter 0/4 - loss 0.81361187 - samples/sec: 638.82\n",
      "2020-06-23 12:05:00,222 epoch 3 - iter 1/4 - loss 0.75564730 - samples/sec: 727.82\n",
      "2020-06-23 12:05:00,274 epoch 3 - iter 2/4 - loss 0.71617647 - samples/sec: 730.43\n",
      "2020-06-23 12:05:00,293 epoch 3 - iter 3/4 - loss 0.79934302 - samples/sec: 3152.28\n",
      "2020-06-23 12:05:00,300 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:05:00,301 EPOCH 3 done: loss 0.7993 - lr 0.1000\n",
      "2020-06-23 12:05:02,478 DEV : loss 0.7067557573318481 - score 0.7667\n",
      "2020-06-23 12:05:02,623 BAD EPOCHS (no improvement): 2\n",
      "2020-06-23 12:05:13,569 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:05:13,610 epoch 4 - iter 0/4 - loss 0.79547936 - samples/sec: 821.62\n",
      "2020-06-23 12:05:13,663 epoch 4 - iter 1/4 - loss 0.80290174 - samples/sec: 741.75\n",
      "2020-06-23 12:05:13,706 epoch 4 - iter 2/4 - loss 0.72024441 - samples/sec: 877.62\n",
      "2020-06-23 12:05:13,724 epoch 4 - iter 3/4 - loss 0.70956665 - samples/sec: 3586.70\n",
      "2020-06-23 12:05:13,730 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:05:13,731 EPOCH 4 done: loss 0.7096 - lr 0.1000\n",
      "2020-06-23 12:05:15,766 DEV : loss 0.6840473413467407 - score 0.7667\n",
      "2020-06-23 12:05:15,913 BAD EPOCHS (no improvement): 3\n",
      "2020-06-23 12:05:26,821 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:05:26,862 epoch 5 - iter 0/4 - loss 0.84870452 - samples/sec: 828.41\n",
      "2020-06-23 12:05:26,909 epoch 5 - iter 1/4 - loss 0.81398869 - samples/sec: 829.03\n",
      "2020-06-23 12:05:26,957 epoch 5 - iter 2/4 - loss 0.72040357 - samples/sec: 817.42\n",
      "2020-06-23 12:05:26,980 epoch 5 - iter 3/4 - loss 0.60954522 - samples/sec: 2459.24\n",
      "2020-06-23 12:05:26,989 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:05:26,990 EPOCH 5 done: loss 0.6095 - lr 0.1000\n",
      "2020-06-23 12:05:29,013 DEV : loss 0.6758835315704346 - score 0.7667\n",
      "2020-06-23 12:05:29,160 BAD EPOCHS (no improvement): 4\n",
      "2020-06-23 12:05:40,078 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:05:40,126 epoch 6 - iter 0/4 - loss 0.57050139 - samples/sec: 702.77\n",
      "2020-06-23 12:05:40,178 epoch 6 - iter 1/4 - loss 0.59016100 - samples/sec: 817.27\n",
      "2020-06-23 12:05:40,231 epoch 6 - iter 2/4 - loss 0.68815031 - samples/sec: 743.99\n",
      "2020-06-23 12:05:40,251 epoch 6 - iter 3/4 - loss 0.67530750 - samples/sec: 2854.72\n",
      "2020-06-23 12:05:40,262 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:05:40,262 EPOCH 6 done: loss 0.6753 - lr 0.1000\n",
      "2020-06-23 12:05:42,227 DEV : loss 0.6721866726875305 - score 0.7667\n",
      "2020-06-23 12:05:42,374 BAD EPOCHS (no improvement): 5\n",
      "2020-06-23 12:05:53,258 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:05:53,298 epoch 7 - iter 0/4 - loss 0.69311374 - samples/sec: 842.05\n",
      "2020-06-23 12:05:53,341 epoch 7 - iter 1/4 - loss 0.76753110 - samples/sec: 895.47\n",
      "2020-06-23 12:05:53,394 epoch 7 - iter 2/4 - loss 0.70203833 - samples/sec: 744.05\n",
      "2020-06-23 12:05:53,414 epoch 7 - iter 3/4 - loss 0.71946663 - samples/sec: 2531.45\n",
      "2020-06-23 12:05:53,425 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:05:53,426 EPOCH 7 done: loss 0.7195 - lr 0.1000\n",
      "2020-06-23 12:05:55,446 DEV : loss 0.6714533567428589 - score 0.7667\n",
      "Epoch     6: reducing learning rate of group 0 to 5.0000e-02.\n",
      "2020-06-23 12:05:55,595 BAD EPOCHS (no improvement): 6\n",
      "2020-06-23 12:06:06,517 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:06:06,570 epoch 8 - iter 0/4 - loss 0.68803734 - samples/sec: 633.17\n",
      "2020-06-23 12:06:06,623 epoch 8 - iter 1/4 - loss 0.78462690 - samples/sec: 828.02\n",
      "2020-06-23 12:06:06,673 epoch 8 - iter 2/4 - loss 0.69619135 - samples/sec: 851.65\n",
      "2020-06-23 12:06:06,692 epoch 8 - iter 3/4 - loss 0.68553941 - samples/sec: 2792.83\n",
      "2020-06-23 12:06:06,703 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:06:06,704 EPOCH 8 done: loss 0.6855 - lr 0.0500\n",
      "2020-06-23 12:06:08,687 DEV : loss 0.6707772612571716 - score 0.7667\n",
      "2020-06-23 12:06:08,832 BAD EPOCHS (no improvement): 1\n",
      "2020-06-23 12:06:19,622 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:06:19,670 epoch 9 - iter 0/4 - loss 0.56597775 - samples/sec: 712.95\n",
      "2020-06-23 12:06:19,710 epoch 9 - iter 1/4 - loss 0.66481885 - samples/sec: 936.88\n",
      "2020-06-23 12:06:19,752 epoch 9 - iter 2/4 - loss 0.66760204 - samples/sec: 905.05\n",
      "2020-06-23 12:06:19,769 epoch 9 - iter 3/4 - loss 0.80157636 - samples/sec: 2928.02\n",
      "2020-06-23 12:06:19,778 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:06:19,779 EPOCH 9 done: loss 0.8016 - lr 0.0500\n",
      "2020-06-23 12:06:21,687 DEV : loss 0.672739565372467 - score 0.7667\n",
      "2020-06-23 12:06:21,832 BAD EPOCHS (no improvement): 2\n",
      "2020-06-23 12:06:32,741 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:06:32,785 epoch 10 - iter 0/4 - loss 0.59626138 - samples/sec: 762.59\n",
      "2020-06-23 12:06:32,823 epoch 10 - iter 1/4 - loss 0.76314065 - samples/sec: 1046.60\n",
      "2020-06-23 12:06:32,869 epoch 10 - iter 2/4 - loss 0.69382668 - samples/sec: 803.25\n",
      "2020-06-23 12:06:32,885 epoch 10 - iter 3/4 - loss 0.66991901 - samples/sec: 3005.59\n",
      "2020-06-23 12:06:32,896 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:06:32,897 EPOCH 10 done: loss 0.6699 - lr 0.0500\n",
      "2020-06-23 12:06:34,866 DEV : loss 0.6707697510719299 - score 0.7667\n",
      "2020-06-23 12:06:35,012 BAD EPOCHS (no improvement): 3\n",
      "2020-06-23 12:06:46,022 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:06:46,070 epoch 11 - iter 0/4 - loss 0.42526668 - samples/sec: 700.84\n",
      "2020-06-23 12:06:46,116 epoch 11 - iter 1/4 - loss 0.56501114 - samples/sec: 944.24\n",
      "2020-06-23 12:06:46,165 epoch 11 - iter 2/4 - loss 0.68764063 - samples/sec: 754.56\n",
      "2020-06-23 12:06:46,183 epoch 11 - iter 3/4 - loss 0.67686972 - samples/sec: 2967.78\n",
      "2020-06-23 12:06:46,190 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:06:46,191 EPOCH 11 done: loss 0.6769 - lr 0.0500\n",
      "2020-06-23 12:06:48,168 DEV : loss 0.6707692742347717 - score 0.7667\n",
      "2020-06-23 12:06:48,316 BAD EPOCHS (no improvement): 4\n",
      "2020-06-23 12:06:59,227 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:06:59,272 epoch 12 - iter 0/4 - loss 0.57058090 - samples/sec: 742.18\n",
      "2020-06-23 12:06:59,319 epoch 12 - iter 1/4 - loss 0.63390341 - samples/sec: 805.43\n",
      "2020-06-23 12:06:59,366 epoch 12 - iter 2/4 - loss 0.68486416 - samples/sec: 890.79\n",
      "2020-06-23 12:06:59,386 epoch 12 - iter 3/4 - loss 0.67568044 - samples/sec: 2753.98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-23 12:06:59,392 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:06:59,393 EPOCH 12 done: loss 0.6757 - lr 0.0500\n",
      "2020-06-23 12:07:01,329 DEV : loss 0.6699227094650269 - score 0.7667\n",
      "2020-06-23 12:07:01,473 BAD EPOCHS (no improvement): 5\n",
      "2020-06-23 12:07:12,329 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:07:12,369 epoch 13 - iter 0/4 - loss 0.67127281 - samples/sec: 829.26\n",
      "2020-06-23 12:07:12,414 epoch 13 - iter 1/4 - loss 0.65598273 - samples/sec: 845.23\n",
      "2020-06-23 12:07:12,463 epoch 13 - iter 2/4 - loss 0.66497407 - samples/sec: 778.49\n",
      "2020-06-23 12:07:12,483 epoch 13 - iter 3/4 - loss 0.74663119 - samples/sec: 2883.61\n",
      "2020-06-23 12:07:12,491 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:07:12,492 EPOCH 13 done: loss 0.7466 - lr 0.0500\n",
      "2020-06-23 12:07:14,701 DEV : loss 0.6734588742256165 - score 0.7667\n",
      "Epoch    12: reducing learning rate of group 0 to 2.5000e-02.\n",
      "2020-06-23 12:07:14,847 BAD EPOCHS (no improvement): 6\n",
      "2020-06-23 12:07:25,911 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:07:25,951 epoch 14 - iter 0/4 - loss 0.57837307 - samples/sec: 844.70\n",
      "2020-06-23 12:07:25,999 epoch 14 - iter 1/4 - loss 0.65576398 - samples/sec: 857.90\n",
      "2020-06-23 12:07:26,047 epoch 14 - iter 2/4 - loss 0.68828020 - samples/sec: 780.75\n",
      "2020-06-23 12:07:26,064 epoch 14 - iter 3/4 - loss 0.67759734 - samples/sec: 2863.86\n",
      "2020-06-23 12:07:26,076 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:07:26,077 EPOCH 14 done: loss 0.6776 - lr 0.0250\n",
      "2020-06-23 12:07:28,288 DEV : loss 0.6726621985435486 - score 0.7667\n",
      "2020-06-23 12:07:28,437 BAD EPOCHS (no improvement): 1\n",
      "2020-06-23 12:07:39,437 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:07:39,482 epoch 15 - iter 0/4 - loss 0.69070071 - samples/sec: 754.10\n",
      "2020-06-23 12:07:39,531 epoch 15 - iter 1/4 - loss 0.68209082 - samples/sec: 841.42\n",
      "2020-06-23 12:07:39,578 epoch 15 - iter 2/4 - loss 0.66882350 - samples/sec: 839.82\n",
      "2020-06-23 12:07:39,599 epoch 15 - iter 3/4 - loss 0.72551656 - samples/sec: 2888.58\n",
      "2020-06-23 12:07:39,605 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:07:39,607 EPOCH 15 done: loss 0.7255 - lr 0.0250\n",
      "2020-06-23 12:07:41,607 DEV : loss 0.6736631989479065 - score 0.7667\n",
      "2020-06-23 12:07:41,752 BAD EPOCHS (no improvement): 2\n",
      "2020-06-23 12:08:03,644 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:08:03,646 Testing using best model ...\n",
      "2020-06-23 12:08:03,648 loading file best-model.pt\n",
      "2020-06-23 12:08:08,021 0.7597\t0.7597\t0.7597\n",
      "2020-06-23 12:08:08,022 \n",
      "MICRO_AVG: acc 0.6125 - f1-score 0.7597\n",
      "MACRO_AVG: acc 0.2532 - f1-score 0.2878\n",
      "0          tp: 0 - fp: 0 - fn: 173 - tn: 2827 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000\n",
      "1          tp: 2279 - fp: 721 - fn: 0 - tn: 0 - precision: 0.7597 - recall: 1.0000 - accuracy: 0.7597 - f1-score: 0.8634\n",
      "2          tp: 0 - fp: 0 - fn: 548 - tn: 2452 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000\n",
      "2020-06-23 12:08:08,023 ----------------------------------------------------------------------------------------------------\n",
      "en-twitter\n",
      "2020-06-23 12:08:17,398 Reading data from /home/max/git/modelcompare/dataset_hatespeech/input\n",
      "2020-06-23 12:08:17,399 Train: /home/max/git/modelcompare/dataset_hatespeech/input/flair_train.csv\n",
      "2020-06-23 12:08:17,400 Dev: /home/max/git/modelcompare/dataset_hatespeech/input/flair_dev.csv\n",
      "2020-06-23 12:08:17,400 Test: /home/max/git/modelcompare/dataset_hatespeech/input/flair_test.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/ipykernel_launcher.py:24: DeprecationWarning: Call to deprecated function (or staticmethod) load_classification_corpus. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/flair/data_fetcher.py:452: DeprecationWarning: Call to deprecated function (or staticmethod) read_text_classification_file. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  train_file, tokenizer=tokenizer, max_tokens_per_doc=max_tokens_per_doc\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/flair/data_fetcher.py:457: DeprecationWarning: Call to deprecated function (or staticmethod) read_text_classification_file. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  test_file, tokenizer=tokenizer, max_tokens_per_doc=max_tokens_per_doc\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/flair/data_fetcher.py:464: DeprecationWarning: Call to deprecated function (or staticmethod) read_text_classification_file. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  dev_file, tokenizer=tokenizer, max_tokens_per_doc=max_tokens_per_doc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-23 12:08:19,878 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 166111.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-23 12:08:19,881 [b'1', b'2', b'0']\n",
      "2020-06-23 12:08:19,882 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 100/100 [00:00<00:00, 140795.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-23 12:08:19,885 [b'1', b'2', b'0']\n",
      "2020-06-23 12:08:19,888 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:08:19,889 Model: \"TextClassifier(\n",
      "  (document_embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): WordEmbeddings('en-twitter')\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=100, out_features=256, bias=True)\n",
      "    (rnn): LSTM(256, 512, batch_first=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=512, out_features=3, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\"\n",
      "2020-06-23 12:08:19,889 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:08:19,889 Corpus: \"Corpus: 100 train + 3000 dev + 3000 test sentences\"\n",
      "2020-06-23 12:08:19,891 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:08:19,891 Parameters:\n",
      "2020-06-23 12:08:19,892  - learning_rate: \"0.1\"\n",
      "2020-06-23 12:08:19,893  - mini_batch_size: \"32\"\n",
      "2020-06-23 12:08:19,893  - patience: \"5\"\n",
      "2020-06-23 12:08:19,894  - anneal_factor: \"0.5\"\n",
      "2020-06-23 12:08:19,894  - max_epochs: \"15\"\n",
      "2020-06-23 12:08:19,895  - shuffle: \"True\"\n",
      "2020-06-23 12:08:19,896  - train_with_dev: \"False\"\n",
      "2020-06-23 12:08:19,896  - batch_growth_annealing: \"False\"\n",
      "2020-06-23 12:08:19,896 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:08:19,897 Model training base path: \".\"\n",
      "2020-06-23 12:08:19,898 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:08:19,898 Device: cuda:0\n",
      "2020-06-23 12:08:19,899 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:08:19,899 Embeddings storage mode: cpu\n",
      "2020-06-23 12:08:19,901 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:08:19,972 epoch 1 - iter 0/4 - loss 1.06775522 - samples/sec: 454.92\n",
      "2020-06-23 12:08:20,043 epoch 1 - iter 1/4 - loss 0.86591914 - samples/sec: 507.58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-23 12:08:20,106 epoch 1 - iter 2/4 - loss 0.85201408 - samples/sec: 563.93\n",
      "2020-06-23 12:08:20,123 epoch 1 - iter 3/4 - loss 0.84898101 - samples/sec: 3002.57\n",
      "2020-06-23 12:08:20,130 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:08:20,131 EPOCH 1 done: loss 0.8490 - lr 0.1000\n",
      "2020-06-23 12:08:23,423 DEV : loss 0.7212163805961609 - score 0.7667\n",
      "2020-06-23 12:08:23,570 BAD EPOCHS (no improvement): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DocumentRNNEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type StackedEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type WordEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-23 12:08:33,701 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:08:33,761 epoch 2 - iter 0/4 - loss 0.67330611 - samples/sec: 554.64\n",
      "2020-06-23 12:08:33,806 epoch 2 - iter 1/4 - loss 0.71372572 - samples/sec: 954.63\n",
      "2020-06-23 12:08:33,854 epoch 2 - iter 2/4 - loss 0.71139894 - samples/sec: 770.93\n",
      "2020-06-23 12:08:33,877 epoch 2 - iter 3/4 - loss 0.62098975 - samples/sec: 2951.79\n",
      "2020-06-23 12:08:33,883 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:08:33,884 EPOCH 2 done: loss 0.6210 - lr 0.1000\n",
      "2020-06-23 12:08:35,968 DEV : loss 0.6901137232780457 - score 0.7667\n",
      "2020-06-23 12:08:36,116 BAD EPOCHS (no improvement): 1\n",
      "2020-06-23 12:08:46,092 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:08:46,150 epoch 3 - iter 0/4 - loss 0.89927065 - samples/sec: 565.48\n",
      "2020-06-23 12:08:46,205 epoch 3 - iter 1/4 - loss 0.78002459 - samples/sec: 677.27\n",
      "2020-06-23 12:08:46,260 epoch 3 - iter 2/4 - loss 0.72838257 - samples/sec: 675.49\n",
      "2020-06-23 12:08:46,278 epoch 3 - iter 3/4 - loss 0.76711808 - samples/sec: 3018.03\n",
      "2020-06-23 12:08:46,285 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:08:46,285 EPOCH 3 done: loss 0.7671 - lr 0.1000\n",
      "2020-06-23 12:08:48,231 DEV : loss 0.7792022228240967 - score 0.74\n",
      "2020-06-23 12:08:48,381 BAD EPOCHS (no improvement): 2\n",
      "2020-06-23 12:08:48,383 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:08:48,422 epoch 4 - iter 0/4 - loss 0.74755722 - samples/sec: 843.47\n",
      "2020-06-23 12:08:48,470 epoch 4 - iter 1/4 - loss 0.76136157 - samples/sec: 828.31\n",
      "2020-06-23 12:08:48,523 epoch 4 - iter 2/4 - loss 0.67531617 - samples/sec: 715.02\n",
      "2020-06-23 12:08:48,538 epoch 4 - iter 3/4 - loss 0.65439348 - samples/sec: 4092.63\n",
      "2020-06-23 12:08:48,547 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:08:48,547 EPOCH 4 done: loss 0.6544 - lr 0.1000\n",
      "2020-06-23 12:08:50,682 DEV : loss 0.6418530941009521 - score 0.7667\n",
      "2020-06-23 12:08:50,831 BAD EPOCHS (no improvement): 3\n",
      "2020-06-23 12:09:00,863 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:09:00,903 epoch 5 - iter 0/4 - loss 0.71406084 - samples/sec: 832.11\n",
      "2020-06-23 12:09:00,949 epoch 5 - iter 1/4 - loss 0.71748683 - samples/sec: 851.48\n",
      "2020-06-23 12:09:00,996 epoch 5 - iter 2/4 - loss 0.64035076 - samples/sec: 847.97\n",
      "2020-06-23 12:09:01,019 epoch 5 - iter 3/4 - loss 0.52801316 - samples/sec: 2612.46\n",
      "2020-06-23 12:09:01,024 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:09:01,025 EPOCH 5 done: loss 0.5280 - lr 0.1000\n",
      "2020-06-23 12:09:03,253 DEV : loss 0.6872494220733643 - score 0.7667\n",
      "2020-06-23 12:09:03,401 BAD EPOCHS (no improvement): 4\n",
      "2020-06-23 12:09:13,353 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:09:13,389 epoch 6 - iter 0/4 - loss 0.50156403 - samples/sec: 938.87\n",
      "2020-06-23 12:09:13,434 epoch 6 - iter 1/4 - loss 0.56182101 - samples/sec: 825.92\n",
      "2020-06-23 12:09:13,480 epoch 6 - iter 2/4 - loss 0.65605062 - samples/sec: 885.96\n",
      "2020-06-23 12:09:13,499 epoch 6 - iter 3/4 - loss 0.62983863 - samples/sec: 2978.78\n",
      "2020-06-23 12:09:13,506 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:09:13,507 EPOCH 6 done: loss 0.6298 - lr 0.1000\n",
      "2020-06-23 12:09:15,434 DEV : loss 0.6532706022262573 - score 0.7667\n",
      "2020-06-23 12:09:15,583 BAD EPOCHS (no improvement): 5\n",
      "2020-06-23 12:09:25,926 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:09:25,972 epoch 7 - iter 0/4 - loss 0.64712214 - samples/sec: 736.29\n",
      "2020-06-23 12:09:26,014 epoch 7 - iter 1/4 - loss 0.70930022 - samples/sec: 946.02\n",
      "2020-06-23 12:09:26,061 epoch 7 - iter 2/4 - loss 0.61733388 - samples/sec: 774.13\n",
      "2020-06-23 12:09:26,078 epoch 7 - iter 3/4 - loss 0.64882765 - samples/sec: 3065.38\n",
      "2020-06-23 12:09:26,087 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:09:26,088 EPOCH 7 done: loss 0.6488 - lr 0.1000\n",
      "2020-06-23 12:09:28,095 DEV : loss 0.6558524966239929 - score 0.768\n",
      "2020-06-23 12:09:28,246 BAD EPOCHS (no improvement): 0\n",
      "2020-06-23 12:09:38,230 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:09:38,280 epoch 8 - iter 0/4 - loss 0.64705712 - samples/sec: 675.11\n",
      "2020-06-23 12:09:38,334 epoch 8 - iter 1/4 - loss 0.72658557 - samples/sec: 756.57\n",
      "2020-06-23 12:09:38,378 epoch 8 - iter 2/4 - loss 0.63930646 - samples/sec: 846.46\n",
      "2020-06-23 12:09:38,397 epoch 8 - iter 3/4 - loss 0.63240186 - samples/sec: 2885.10\n",
      "2020-06-23 12:09:38,408 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:09:38,409 EPOCH 8 done: loss 0.6324 - lr 0.1000\n",
      "2020-06-23 12:09:40,424 DEV : loss 0.6357089877128601 - score 0.7687\n",
      "2020-06-23 12:09:40,576 BAD EPOCHS (no improvement): 0\n",
      "2020-06-23 12:09:50,649 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:09:50,691 epoch 9 - iter 0/4 - loss 0.49564022 - samples/sec: 795.51\n",
      "2020-06-23 12:09:50,739 epoch 9 - iter 1/4 - loss 0.57368788 - samples/sec: 854.16\n",
      "2020-06-23 12:09:50,822 epoch 9 - iter 2/4 - loss 0.59400831 - samples/sec: 429.13\n",
      "2020-06-23 12:09:50,841 epoch 9 - iter 3/4 - loss 0.77219133 - samples/sec: 2565.76\n",
      "2020-06-23 12:09:50,849 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:09:50,850 EPOCH 9 done: loss 0.7722 - lr 0.1000\n",
      "2020-06-23 12:09:52,956 DEV : loss 0.6835161447525024 - score 0.763\n",
      "2020-06-23 12:09:53,106 BAD EPOCHS (no improvement): 1\n",
      "2020-06-23 12:09:53,108 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:09:53,151 epoch 10 - iter 0/4 - loss 0.59481478 - samples/sec: 766.25\n",
      "2020-06-23 12:09:53,202 epoch 10 - iter 1/4 - loss 0.70256841 - samples/sec: 743.25\n",
      "2020-06-23 12:09:53,253 epoch 10 - iter 2/4 - loss 0.62912608 - samples/sec: 809.15\n",
      "2020-06-23 12:09:53,267 epoch 10 - iter 3/4 - loss 0.59173117 - samples/sec: 4010.57\n",
      "2020-06-23 12:09:53,274 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:09:53,274 EPOCH 10 done: loss 0.5917 - lr 0.1000\n",
      "2020-06-23 12:09:55,429 DEV : loss 0.6264895796775818 - score 0.77\n",
      "2020-06-23 12:09:55,577 BAD EPOCHS (no improvement): 0\n",
      "2020-06-23 12:10:05,613 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:10:05,655 epoch 11 - iter 0/4 - loss 0.39668617 - samples/sec: 798.91\n",
      "2020-06-23 12:10:05,701 epoch 11 - iter 1/4 - loss 0.48491161 - samples/sec: 799.10\n",
      "2020-06-23 12:10:05,754 epoch 11 - iter 2/4 - loss 0.61633306 - samples/sec: 739.35\n",
      "2020-06-23 12:10:05,775 epoch 11 - iter 3/4 - loss 0.56831080 - samples/sec: 2199.03\n",
      "2020-06-23 12:10:05,786 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:10:05,787 EPOCH 11 done: loss 0.5683 - lr 0.1000\n",
      "2020-06-23 12:10:07,788 DEV : loss 0.6148684024810791 - score 0.768\n",
      "2020-06-23 12:10:07,938 BAD EPOCHS (no improvement): 1\n",
      "2020-06-23 12:10:07,939 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:10:07,977 epoch 12 - iter 0/4 - loss 0.51081753 - samples/sec: 895.85\n",
      "2020-06-23 12:10:08,018 epoch 12 - iter 1/4 - loss 0.54448935 - samples/sec: 939.85\n",
      "2020-06-23 12:10:08,064 epoch 12 - iter 2/4 - loss 0.59576134 - samples/sec: 865.06\n",
      "2020-06-23 12:10:08,082 epoch 12 - iter 3/4 - loss 0.54589739 - samples/sec: 2635.03\n",
      "2020-06-23 12:10:08,094 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-23 12:10:08,095 EPOCH 12 done: loss 0.5459 - lr 0.1000\n",
      "2020-06-23 12:10:10,034 DEV : loss 0.6181123852729797 - score 0.7693\n",
      "2020-06-23 12:10:10,180 BAD EPOCHS (no improvement): 2\n",
      "2020-06-23 12:10:10,181 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:10:10,217 epoch 13 - iter 0/4 - loss 0.59038860 - samples/sec: 930.22\n",
      "2020-06-23 12:10:10,265 epoch 13 - iter 1/4 - loss 0.56778693 - samples/sec: 767.01\n",
      "2020-06-23 12:10:10,315 epoch 13 - iter 2/4 - loss 0.58584915 - samples/sec: 836.27\n",
      "2020-06-23 12:10:10,336 epoch 13 - iter 3/4 - loss 0.73433064 - samples/sec: 3136.73\n",
      "2020-06-23 12:10:10,343 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:10:10,343 EPOCH 13 done: loss 0.7343 - lr 0.1000\n",
      "2020-06-23 12:10:12,357 DEV : loss 0.7417793869972229 - score 0.6743\n",
      "2020-06-23 12:10:12,505 BAD EPOCHS (no improvement): 3\n",
      "2020-06-23 12:10:12,507 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:10:12,546 epoch 14 - iter 0/4 - loss 0.70749444 - samples/sec: 851.38\n",
      "2020-06-23 12:10:12,594 epoch 14 - iter 1/4 - loss 0.66476089 - samples/sec: 758.95\n",
      "2020-06-23 12:10:12,640 epoch 14 - iter 2/4 - loss 0.63960948 - samples/sec: 808.49\n",
      "2020-06-23 12:10:12,659 epoch 14 - iter 3/4 - loss 0.55350428 - samples/sec: 2740.03\n",
      "2020-06-23 12:10:12,670 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:10:12,670 EPOCH 14 done: loss 0.5535 - lr 0.1000\n",
      "2020-06-23 12:10:14,626 DEV : loss 0.643886923789978 - score 0.758\n",
      "2020-06-23 12:10:14,775 BAD EPOCHS (no improvement): 4\n",
      "2020-06-23 12:10:14,777 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:10:14,818 epoch 15 - iter 0/4 - loss 0.50877845 - samples/sec: 803.95\n",
      "2020-06-23 12:10:14,864 epoch 15 - iter 1/4 - loss 0.53367573 - samples/sec: 874.92\n",
      "2020-06-23 12:10:14,911 epoch 15 - iter 2/4 - loss 0.52240543 - samples/sec: 873.49\n",
      "2020-06-23 12:10:14,930 epoch 15 - iter 3/4 - loss 0.63266152 - samples/sec: 3061.82\n",
      "2020-06-23 12:10:14,937 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:10:14,938 EPOCH 15 done: loss 0.6327 - lr 0.1000\n",
      "2020-06-23 12:10:16,924 DEV : loss 0.7815992832183838 - score 0.6257\n",
      "2020-06-23 12:10:17,073 BAD EPOCHS (no improvement): 5\n",
      "2020-06-23 12:10:27,199 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:10:27,201 Testing using best model ...\n",
      "2020-06-23 12:10:27,202 loading file best-model.pt\n",
      "2020-06-23 12:10:31,472 0.7623\t0.7623\t0.7623\n",
      "2020-06-23 12:10:31,473 \n",
      "MICRO_AVG: acc 0.6159 - f1-score 0.7623\n",
      "MACRO_AVG: acc 0.2638 - f1-score 0.30783333333333335\n",
      "0          tp: 0 - fp: 0 - fn: 173 - tn: 2827 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000\n",
      "1          tp: 2270 - fp: 704 - fn: 9 - tn: 17 - precision: 0.7633 - recall: 0.9961 - accuracy: 0.7610 - f1-score: 0.8643\n",
      "2          tp: 17 - fp: 9 - fn: 531 - tn: 2443 - precision: 0.6538 - recall: 0.0310 - accuracy: 0.0305 - f1-score: 0.0592\n",
      "2020-06-23 12:10:31,474 ----------------------------------------------------------------------------------------------------\n",
      "elmo\n",
      "2020-06-23 12:11:08,723 Reading data from /home/max/git/modelcompare/dataset_hatespeech/input\n",
      "2020-06-23 12:11:08,723 Train: /home/max/git/modelcompare/dataset_hatespeech/input/flair_train.csv\n",
      "2020-06-23 12:11:08,724 Dev: /home/max/git/modelcompare/dataset_hatespeech/input/flair_dev.csv\n",
      "2020-06-23 12:11:08,724 Test: /home/max/git/modelcompare/dataset_hatespeech/input/flair_test.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/ipykernel_launcher.py:24: DeprecationWarning: Call to deprecated function (or staticmethod) load_classification_corpus. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/flair/data_fetcher.py:452: DeprecationWarning: Call to deprecated function (or staticmethod) read_text_classification_file. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  train_file, tokenizer=tokenizer, max_tokens_per_doc=max_tokens_per_doc\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/flair/data_fetcher.py:457: DeprecationWarning: Call to deprecated function (or staticmethod) read_text_classification_file. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  test_file, tokenizer=tokenizer, max_tokens_per_doc=max_tokens_per_doc\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/flair/data_fetcher.py:464: DeprecationWarning: Call to deprecated function (or staticmethod) read_text_classification_file. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  dev_file, tokenizer=tokenizer, max_tokens_per_doc=max_tokens_per_doc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-23 12:11:11,670 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 219138.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-23 12:11:11,673 [b'1', b'2', b'0']\n",
      "2020-06-23 12:11:11,673 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 100/100 [00:00<00:00, 255750.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-23 12:11:11,675 [b'1', b'2', b'0']\n",
      "2020-06-23 12:11:11,678 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:11:11,679 Model: \"TextClassifier(\n",
      "  (document_embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): ELMoEmbeddings(model=0-elmo-original)\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=3072, out_features=256, bias=True)\n",
      "    (rnn): LSTM(256, 512, batch_first=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=512, out_features=3, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\"\n",
      "2020-06-23 12:11:11,680 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:11:11,682 Corpus: \"Corpus: 100 train + 3000 dev + 3000 test sentences\"\n",
      "2020-06-23 12:11:11,682 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:11:11,683 Parameters:\n",
      "2020-06-23 12:11:11,684  - learning_rate: \"0.1\"\n",
      "2020-06-23 12:11:11,684  - mini_batch_size: \"32\"\n",
      "2020-06-23 12:11:11,685  - patience: \"5\"\n",
      "2020-06-23 12:11:11,686  - anneal_factor: \"0.5\"\n",
      "2020-06-23 12:11:11,687  - max_epochs: \"15\"\n",
      "2020-06-23 12:11:11,687  - shuffle: \"True\"\n",
      "2020-06-23 12:11:11,688  - train_with_dev: \"False\"\n",
      "2020-06-23 12:11:11,688  - batch_growth_annealing: \"False\"\n",
      "2020-06-23 12:11:11,689 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:11:11,689 Model training base path: \".\"\n",
      "2020-06-23 12:11:11,690 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:11:11,691 Device: cuda:0\n",
      "2020-06-23 12:11:11,692 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:11:11,693 Embeddings storage mode: cpu\n",
      "2020-06-23 12:11:11,696 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-23 12:11:11,945 epoch 1 - iter 0/4 - loss 1.20414293 - samples/sec: 129.35\n",
      "2020-06-23 12:11:12,243 epoch 1 - iter 1/4 - loss 0.90223247 - samples/sec: 112.48\n",
      "2020-06-23 12:11:12,467 epoch 1 - iter 2/4 - loss 0.88235418 - samples/sec: 146.83\n",
      "2020-06-23 12:11:12,535 epoch 1 - iter 3/4 - loss 0.85789797 - samples/sec: 520.09\n",
      "2020-06-23 12:11:12,546 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:11:12,547 EPOCH 1 done: loss 0.8579 - lr 0.1000\n",
      "2020-06-23 12:11:31,969 DEV : loss 0.6743201017379761 - score 0.7667\n",
      "2020-06-23 12:11:32,118 BAD EPOCHS (no improvement): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DocumentRNNEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type StackedEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ELMoEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type _ElmoBiLm. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type _ElmoCharacterEncoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Conv1d. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Highway. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ModuleList. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ElmoLstm. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LstmCellWithProjection. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-23 12:11:33,207 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:11:33,268 epoch 2 - iter 0/4 - loss 0.66289943 - samples/sec: 545.88\n",
      "2020-06-23 12:11:33,315 epoch 2 - iter 1/4 - loss 0.71299613 - samples/sec: 795.10\n",
      "2020-06-23 12:11:33,371 epoch 2 - iter 2/4 - loss 0.71684919 - samples/sec: 684.60\n",
      "2020-06-23 12:11:33,392 epoch 2 - iter 3/4 - loss 0.59646435 - samples/sec: 3026.13\n",
      "2020-06-23 12:11:33,398 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:11:33,399 EPOCH 2 done: loss 0.5965 - lr 0.1000\n",
      "2020-06-23 12:11:35,585 DEV : loss 0.9560748934745789 - score 0.7667\n",
      "2020-06-23 12:11:35,735 BAD EPOCHS (no improvement): 1\n",
      "2020-06-23 12:11:36,845 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:11:36,892 epoch 3 - iter 0/4 - loss 1.06638706 - samples/sec: 708.91\n",
      "2020-06-23 12:11:36,936 epoch 3 - iter 1/4 - loss 0.81120336 - samples/sec: 849.45\n",
      "2020-06-23 12:11:36,986 epoch 3 - iter 2/4 - loss 0.72470025 - samples/sec: 785.00\n",
      "2020-06-23 12:11:37,004 epoch 3 - iter 3/4 - loss 0.71649539 - samples/sec: 2996.47\n",
      "2020-06-23 12:11:37,012 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:11:37,013 EPOCH 3 done: loss 0.7165 - lr 0.1000\n",
      "2020-06-23 12:11:39,149 DEV : loss 0.8111155033111572 - score 0.631\n",
      "2020-06-23 12:11:39,297 BAD EPOCHS (no improvement): 2\n",
      "2020-06-23 12:11:39,299 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:11:39,340 epoch 4 - iter 0/4 - loss 0.76813394 - samples/sec: 817.70\n",
      "2020-06-23 12:11:39,382 epoch 4 - iter 1/4 - loss 0.81273243 - samples/sec: 933.07\n",
      "2020-06-23 12:11:39,425 epoch 4 - iter 2/4 - loss 0.70051227 - samples/sec: 931.25\n",
      "2020-06-23 12:11:39,441 epoch 4 - iter 3/4 - loss 0.69557489 - samples/sec: 3500.08\n",
      "2020-06-23 12:11:39,447 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:11:39,448 EPOCH 4 done: loss 0.6956 - lr 0.1000\n",
      "2020-06-23 12:11:41,565 DEV : loss 0.6557310819625854 - score 0.7683\n",
      "2020-06-23 12:11:41,731 BAD EPOCHS (no improvement): 0\n",
      "2020-06-23 12:11:42,833 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:11:42,881 epoch 5 - iter 0/4 - loss 0.66722411 - samples/sec: 696.31\n",
      "2020-06-23 12:11:42,930 epoch 5 - iter 1/4 - loss 0.62760797 - samples/sec: 766.66\n",
      "2020-06-23 12:11:42,979 epoch 5 - iter 2/4 - loss 0.53885423 - samples/sec: 766.43\n",
      "2020-06-23 12:11:42,999 epoch 5 - iter 3/4 - loss 0.41703378 - samples/sec: 2378.14\n",
      "2020-06-23 12:11:43,011 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:11:43,012 EPOCH 5 done: loss 0.4170 - lr 0.1000\n",
      "2020-06-23 12:11:45,185 DEV : loss 0.8837898373603821 - score 0.7667\n",
      "2020-06-23 12:11:45,336 BAD EPOCHS (no improvement): 1\n",
      "2020-06-23 12:11:45,338 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:11:45,379 epoch 6 - iter 0/4 - loss 0.42848402 - samples/sec: 803.78\n",
      "2020-06-23 12:11:45,432 epoch 6 - iter 1/4 - loss 0.43746033 - samples/sec: 755.74\n",
      "2020-06-23 12:11:45,486 epoch 6 - iter 2/4 - loss 0.51768221 - samples/sec: 724.50\n",
      "2020-06-23 12:11:45,506 epoch 6 - iter 3/4 - loss 0.51532897 - samples/sec: 2558.53\n",
      "2020-06-23 12:11:45,518 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:11:45,518 EPOCH 6 done: loss 0.5153 - lr 0.1000\n",
      "2020-06-23 12:11:47,666 DEV : loss 0.7422454953193665 - score 0.762\n",
      "2020-06-23 12:11:47,818 BAD EPOCHS (no improvement): 2\n",
      "2020-06-23 12:11:47,820 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:11:47,863 epoch 7 - iter 0/4 - loss 0.43892893 - samples/sec: 753.98\n",
      "2020-06-23 12:11:47,906 epoch 7 - iter 1/4 - loss 0.48589431 - samples/sec: 881.18\n",
      "2020-06-23 12:11:47,962 epoch 7 - iter 2/4 - loss 0.47941092 - samples/sec: 698.42\n",
      "2020-06-23 12:11:47,980 epoch 7 - iter 3/4 - loss 0.51877470 - samples/sec: 2604.20\n",
      "2020-06-23 12:11:47,990 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:11:47,991 EPOCH 7 done: loss 0.5188 - lr 0.1000\n",
      "2020-06-23 12:11:50,321 DEV : loss 0.7190904021263123 - score 0.6867\n",
      "2020-06-23 12:11:50,473 BAD EPOCHS (no improvement): 3\n",
      "2020-06-23 12:11:50,475 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:11:50,519 epoch 8 - iter 0/4 - loss 0.46482453 - samples/sec: 760.36\n",
      "2020-06-23 12:11:50,565 epoch 8 - iter 1/4 - loss 0.50770257 - samples/sec: 797.52\n",
      "2020-06-23 12:11:50,611 epoch 8 - iter 2/4 - loss 0.51658764 - samples/sec: 816.38\n",
      "2020-06-23 12:11:50,630 epoch 8 - iter 3/4 - loss 0.59829160 - samples/sec: 2770.12\n",
      "2020-06-23 12:11:50,640 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:11:50,642 EPOCH 8 done: loss 0.5983 - lr 0.1000\n",
      "2020-06-23 12:11:52,840 DEV : loss 0.6248732209205627 - score 0.7617\n",
      "2020-06-23 12:11:52,992 BAD EPOCHS (no improvement): 4\n",
      "2020-06-23 12:11:52,993 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:11:53,038 epoch 9 - iter 0/4 - loss 0.36454827 - samples/sec: 733.75\n",
      "2020-06-23 12:11:53,079 epoch 9 - iter 1/4 - loss 0.46936584 - samples/sec: 922.08\n",
      "2020-06-23 12:11:53,125 epoch 9 - iter 2/4 - loss 0.48682449 - samples/sec: 825.96\n",
      "2020-06-23 12:11:53,143 epoch 9 - iter 3/4 - loss 0.61242469 - samples/sec: 2963.52\n",
      "2020-06-23 12:11:53,152 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:11:53,153 EPOCH 9 done: loss 0.6124 - lr 0.1000\n",
      "2020-06-23 12:11:55,438 DEV : loss 0.6891040205955505 - score 0.7567\n",
      "2020-06-23 12:11:55,596 BAD EPOCHS (no improvement): 5\n",
      "2020-06-23 12:11:55,598 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:11:55,651 epoch 10 - iter 0/4 - loss 0.38801077 - samples/sec: 622.29\n",
      "2020-06-23 12:11:55,698 epoch 10 - iter 1/4 - loss 0.60337068 - samples/sec: 823.19\n",
      "2020-06-23 12:11:55,750 epoch 10 - iter 2/4 - loss 0.56467705 - samples/sec: 699.53\n",
      "2020-06-23 12:11:55,768 epoch 10 - iter 3/4 - loss 0.50210713 - samples/sec: 3839.84\n",
      "2020-06-23 12:11:55,773 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:11:55,775 EPOCH 10 done: loss 0.5021 - lr 0.1000\n",
      "2020-06-23 12:11:58,102 DEV : loss 0.7401565313339233 - score 0.7427\n",
      "Epoch     9: reducing learning rate of group 0 to 5.0000e-02.\n",
      "2020-06-23 12:11:58,254 BAD EPOCHS (no improvement): 6\n",
      "2020-06-23 12:11:58,256 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:11:58,301 epoch 11 - iter 0/4 - loss 0.14935362 - samples/sec: 734.05\n",
      "2020-06-23 12:11:58,353 epoch 11 - iter 1/4 - loss 0.24851379 - samples/sec: 806.61\n",
      "2020-06-23 12:11:58,404 epoch 11 - iter 2/4 - loss 0.31810519 - samples/sec: 715.36\n",
      "2020-06-23 12:11:58,423 epoch 11 - iter 3/4 - loss 0.25857040 - samples/sec: 2666.54\n",
      "2020-06-23 12:11:58,436 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:11:58,437 EPOCH 11 done: loss 0.2586 - lr 0.0500\n",
      "2020-06-23 12:12:00,666 DEV : loss 0.7216285467147827 - score 0.7317\n",
      "2020-06-23 12:12:00,817 BAD EPOCHS (no improvement): 1\n",
      "2020-06-23 12:12:00,818 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:12:00,866 epoch 12 - iter 0/4 - loss 0.35808074 - samples/sec: 696.08\n",
      "2020-06-23 12:12:00,919 epoch 12 - iter 1/4 - loss 0.26929829 - samples/sec: 717.05\n",
      "2020-06-23 12:12:00,973 epoch 12 - iter 2/4 - loss 0.26751319 - samples/sec: 708.77\n",
      "2020-06-23 12:12:00,999 epoch 12 - iter 3/4 - loss 0.31003175 - samples/sec: 2217.23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-23 12:12:01,006 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:12:01,007 EPOCH 12 done: loss 0.3100 - lr 0.0500\n",
      "2020-06-23 12:12:03,293 DEV : loss 0.7280484437942505 - score 0.728\n",
      "2020-06-23 12:12:03,440 BAD EPOCHS (no improvement): 2\n",
      "2020-06-23 12:12:03,442 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:12:03,480 epoch 13 - iter 0/4 - loss 0.25505537 - samples/sec: 868.18\n",
      "2020-06-23 12:12:03,534 epoch 13 - iter 1/4 - loss 0.19914441 - samples/sec: 764.66\n",
      "2020-06-23 12:12:03,584 epoch 13 - iter 2/4 - loss 0.24425759 - samples/sec: 774.43\n",
      "2020-06-23 12:12:03,602 epoch 13 - iter 3/4 - loss 0.29874956 - samples/sec: 2942.53\n",
      "2020-06-23 12:12:03,609 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:12:03,610 EPOCH 13 done: loss 0.2987 - lr 0.0500\n",
      "2020-06-23 12:12:05,907 DEV : loss 0.8158363699913025 - score 0.651\n",
      "2020-06-23 12:12:06,058 BAD EPOCHS (no improvement): 3\n",
      "2020-06-23 12:12:06,060 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:12:06,103 epoch 14 - iter 0/4 - loss 0.41772574 - samples/sec: 771.78\n",
      "2020-06-23 12:12:06,154 epoch 14 - iter 1/4 - loss 0.39245951 - samples/sec: 742.29\n",
      "2020-06-23 12:12:06,214 epoch 14 - iter 2/4 - loss 0.33568568 - samples/sec: 660.50\n",
      "2020-06-23 12:12:06,232 epoch 14 - iter 3/4 - loss 0.29322932 - samples/sec: 2817.75\n",
      "2020-06-23 12:12:06,239 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:12:06,240 EPOCH 14 done: loss 0.2932 - lr 0.0500\n",
      "2020-06-23 12:12:08,641 DEV : loss 0.7243504524230957 - score 0.73\n",
      "2020-06-23 12:12:08,791 BAD EPOCHS (no improvement): 4\n",
      "2020-06-23 12:12:08,793 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:12:08,838 epoch 15 - iter 0/4 - loss 0.21455526 - samples/sec: 727.92\n",
      "2020-06-23 12:12:08,885 epoch 15 - iter 1/4 - loss 0.24852189 - samples/sec: 800.11\n",
      "2020-06-23 12:12:08,935 epoch 15 - iter 2/4 - loss 0.22961415 - samples/sec: 802.77\n",
      "2020-06-23 12:12:08,953 epoch 15 - iter 3/4 - loss 0.27842950 - samples/sec: 2984.01\n",
      "2020-06-23 12:12:08,965 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:12:08,966 EPOCH 15 done: loss 0.2784 - lr 0.0500\n",
      "2020-06-23 12:12:11,194 DEV : loss 0.6957404017448425 - score 0.7187\n",
      "2020-06-23 12:12:11,344 BAD EPOCHS (no improvement): 5\n",
      "2020-06-23 12:12:12,514 ----------------------------------------------------------------------------------------------------\n",
      "2020-06-23 12:12:12,515 Testing using best model ...\n",
      "2020-06-23 12:12:12,517 loading file best-model.pt\n",
      "2020-06-23 12:12:29,255 0.7667\t0.7667\t0.7667\n",
      "2020-06-23 12:12:29,256 \n",
      "MICRO_AVG: acc 0.6216 - f1-score 0.7667\n",
      "MACRO_AVG: acc 0.2717 - f1-score 0.3208666666666667\n",
      "0          tp: 0 - fp: 0 - fn: 173 - tn: 2827 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000\n",
      "1          tp: 2272 - fp: 693 - fn: 7 - tn: 28 - precision: 0.7663 - recall: 0.9969 - accuracy: 0.7645 - f1-score: 0.8665\n",
      "2          tp: 28 - fp: 7 - fn: 520 - tn: 2445 - precision: 0.8000 - recall: 0.0511 - accuracy: 0.0505 - f1-score: 0.0961\n",
      "2020-06-23 12:12:29,257 ----------------------------------------------------------------------------------------------------\n",
      "919.60973072052\n"
     ]
    }
   ],
   "source": [
    "total_time = time.time()\n",
    "\n",
    "word_embeddings = [ WordEmbeddings('glove'),              ]\n",
    "modelname = 'glove'\n",
    "train_and_predict(word_embeddings, modelname, modeldesc, savelist=savelist, epochs=EPOCHS)\n",
    "\n",
    "word_embeddings = [ WordEmbeddings('en-crawl'),                 ]\n",
    "modelname = 'fasttext web-crawl'\n",
    "train_and_predict(word_embeddings, modelname, modeldesc, savelist=savelist, epochs=EPOCHS)\n",
    "\n",
    "word_embeddings = [ WordEmbeddings('en'),                 ]\n",
    "modelname = 'fasttext news/wiki'\n",
    "train_and_predict(word_embeddings, modelname, modeldesc, savelist=savelist, epochs=EPOCHS)\n",
    "\n",
    "word_embeddings = [ WordEmbeddings('en-twitter'),                 ]\n",
    "modelname = 'en-twitter'\n",
    "train_and_predict(word_embeddings, modelname, modeldesc, savelist=savelist, epochs=EPOCHS)\n",
    "\n",
    "word_embeddings = [ ELMoEmbeddings('original')              ]\n",
    "modelname = 'elmo'\n",
    "train_and_predict(word_embeddings, modelname, modeldesc, savelist=savelist, epochs=EPOCHS)\n",
    "\n",
    "print(time.time() - total_time)\n",
    "\n",
    "# 1289 sec on 100 train (was 2x elmo)\n",
    "\n",
    "\n",
    "# 2403 sec on 18k\n",
    "# 2684 s   18 k"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 3 slowest \n",
    "# Flair\n",
    "total_time = time.time()\n",
    "word_embeddings = [ # FlairEmbeddings('multi-forward'), # this is 300 languge, gave very low score\n",
    "                  # FlairEmbeddings('multi-backward'), \n",
    "                   FlairEmbeddings('news-forward'),  #  \tEnglish \tTrained with 1 billion word corpus                   \n",
    "                   \n",
    "                  ]\n",
    "modelname = 'Flair'\n",
    "train_and_predict(word_embeddings, modelname, modeldesc, savelist=savelist, epochs=EPOCHS)\n",
    "\n",
    "print(time.time() - total_time)\n",
    "\n",
    "# 167s in 100 train\n",
    "# 620s for 18k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flair\n",
      "2020-05-15 16:16:53,990 Reading data from /home/max/git/newcombined/dataset_hatespeech/input\n",
      "2020-05-15 16:16:53,991 Train: /home/max/git/newcombined/dataset_hatespeech/input/flair_train.csv\n",
      "2020-05-15 16:16:53,992 Dev: /home/max/git/newcombined/dataset_hatespeech/input/flair_dev.csv\n",
      "2020-05-15 16:16:53,993 Test: /home/max/git/newcombined/dataset_hatespeech/input/flair_test.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/ipykernel_launcher.py:24: DeprecationWarning: Call to deprecated function (or staticmethod) load_classification_corpus. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/flair/data_fetcher.py:452: DeprecationWarning: Call to deprecated function (or staticmethod) read_text_classification_file. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  train_file, tokenizer=tokenizer, max_tokens_per_doc=max_tokens_per_doc\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/flair/data_fetcher.py:457: DeprecationWarning: Call to deprecated function (or staticmethod) read_text_classification_file. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  test_file, tokenizer=tokenizer, max_tokens_per_doc=max_tokens_per_doc\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/flair/data_fetcher.py:464: DeprecationWarning: Call to deprecated function (or staticmethod) read_text_classification_file. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  dev_file, tokenizer=tokenizer, max_tokens_per_doc=max_tokens_per_doc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-15 16:16:55,465 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 92040.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-15 16:16:55,468 [b'1', b'2', b'0']\n",
      "2020-05-15 16:16:55,469 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 100/100 [00:00<00:00, 182281.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-15 16:16:55,473 [b'1', b'2', b'0']\n",
      "2020-05-15 16:16:55,477 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:16:55,478 Model: \"TextClassifier(\n",
      "  (document_embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (rnn): LSTM(256, 512, batch_first=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=512, out_features=3, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\"\n",
      "2020-05-15 16:16:55,479 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:16:55,479 Corpus: \"Corpus: 100 train + 3000 dev + 3000 test sentences\"\n",
      "2020-05-15 16:16:55,479 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:16:55,480 Parameters:\n",
      "2020-05-15 16:16:55,481  - learning_rate: \"0.1\"\n",
      "2020-05-15 16:16:55,481  - mini_batch_size: \"32\"\n",
      "2020-05-15 16:16:55,482  - patience: \"5\"\n",
      "2020-05-15 16:16:55,482  - anneal_factor: \"0.5\"\n",
      "2020-05-15 16:16:55,483  - max_epochs: \"15\"\n",
      "2020-05-15 16:16:55,483  - shuffle: \"True\"\n",
      "2020-05-15 16:16:55,484  - train_with_dev: \"False\"\n",
      "2020-05-15 16:16:55,484  - batch_growth_annealing: \"False\"\n",
      "2020-05-15 16:16:55,485 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:16:55,485 Model training base path: \".\"\n",
      "2020-05-15 16:16:55,486 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:16:55,487 Device: cuda:0\n",
      "2020-05-15 16:16:55,488 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:16:55,490 Embeddings storage mode: cpu\n",
      "2020-05-15 16:16:55,492 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:16:55,616 epoch 1 - iter 0/4 - loss 1.09649563 - samples/sec: 260.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-15 16:16:55,788 epoch 1 - iter 1/4 - loss 1.03974864 - samples/sec: 199.91\n",
      "2020-05-15 16:16:55,934 epoch 1 - iter 2/4 - loss 1.02089870 - samples/sec: 234.98\n",
      "2020-05-15 16:16:55,987 epoch 1 - iter 3/4 - loss 0.99495506 - samples/sec: 766.80\n",
      "2020-05-15 16:16:55,997 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:16:55,998 EPOCH 1 done: loss 0.9950 - lr 0.1000\n",
      "2020-05-15 16:17:07,322 DEV : loss 0.8543857336044312 - score 0.7667\n",
      "2020-05-15 16:17:07,459 BAD EPOCHS (no improvement): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DocumentRNNEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type StackedEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type FlairEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LanguageModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-15 16:17:07,736 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:17:07,793 epoch 2 - iter 0/4 - loss 0.83632404 - samples/sec: 591.07\n",
      "2020-05-15 16:17:07,840 epoch 2 - iter 1/4 - loss 0.85096899 - samples/sec: 889.78\n",
      "2020-05-15 16:17:07,890 epoch 2 - iter 2/4 - loss 0.83957122 - samples/sec: 824.34\n",
      "2020-05-15 16:17:07,915 epoch 2 - iter 3/4 - loss 0.76601796 - samples/sec: 3255.42\n",
      "2020-05-15 16:17:07,930 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:17:07,931 EPOCH 2 done: loss 0.7660 - lr 0.1000\n",
      "2020-05-15 16:17:09,805 DEV : loss 0.7338841557502747 - score 0.7667\n",
      "2020-05-15 16:17:09,942 BAD EPOCHS (no improvement): 1\n",
      "2020-05-15 16:17:10,213 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:17:10,261 epoch 3 - iter 0/4 - loss 0.81052619 - samples/sec: 709.31\n",
      "2020-05-15 16:17:10,310 epoch 3 - iter 1/4 - loss 0.76153603 - samples/sec: 861.60\n",
      "2020-05-15 16:17:10,360 epoch 3 - iter 2/4 - loss 0.72351885 - samples/sec: 869.93\n",
      "2020-05-15 16:17:10,385 epoch 3 - iter 3/4 - loss 0.81176755 - samples/sec: 3168.20\n",
      "2020-05-15 16:17:10,400 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:17:10,401 EPOCH 3 done: loss 0.8118 - lr 0.1000\n",
      "2020-05-15 16:17:12,327 DEV : loss 0.71510249376297 - score 0.7667\n",
      "2020-05-15 16:17:12,459 BAD EPOCHS (no improvement): 2\n",
      "2020-05-15 16:17:12,700 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:17:12,746 epoch 4 - iter 0/4 - loss 0.80517328 - samples/sec: 720.15\n",
      "2020-05-15 16:17:12,797 epoch 4 - iter 1/4 - loss 0.80672246 - samples/sec: 805.89\n",
      "2020-05-15 16:17:12,849 epoch 4 - iter 2/4 - loss 0.72506454 - samples/sec: 833.88\n",
      "2020-05-15 16:17:12,872 epoch 4 - iter 3/4 - loss 0.71928975 - samples/sec: 3688.72\n",
      "2020-05-15 16:17:12,887 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:17:12,888 EPOCH 4 done: loss 0.7193 - lr 0.1000\n",
      "2020-05-15 16:17:14,822 DEV : loss 0.690628170967102 - score 0.7667\n",
      "2020-05-15 16:17:14,955 BAD EPOCHS (no improvement): 3\n",
      "2020-05-15 16:17:15,211 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:17:15,254 epoch 5 - iter 0/4 - loss 0.82527518 - samples/sec: 777.49\n",
      "2020-05-15 16:17:15,305 epoch 5 - iter 1/4 - loss 0.81014368 - samples/sec: 841.09\n",
      "2020-05-15 16:17:15,353 epoch 5 - iter 2/4 - loss 0.72054712 - samples/sec: 877.96\n",
      "2020-05-15 16:17:15,379 epoch 5 - iter 3/4 - loss 0.61780223 - samples/sec: 2618.01\n",
      "2020-05-15 16:17:15,394 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:17:15,395 EPOCH 5 done: loss 0.6178 - lr 0.1000\n",
      "2020-05-15 16:17:17,330 DEV : loss 0.6764510869979858 - score 0.7667\n",
      "2020-05-15 16:17:17,465 BAD EPOCHS (no improvement): 4\n",
      "2020-05-15 16:17:17,726 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:17:17,769 epoch 6 - iter 0/4 - loss 0.57810831 - samples/sec: 773.17\n",
      "2020-05-15 16:17:17,822 epoch 6 - iter 1/4 - loss 0.60562891 - samples/sec: 803.59\n",
      "2020-05-15 16:17:17,875 epoch 6 - iter 2/4 - loss 0.69767253 - samples/sec: 795.77\n",
      "2020-05-15 16:17:17,898 epoch 6 - iter 3/4 - loss 0.68896359 - samples/sec: 2860.20\n",
      "2020-05-15 16:17:17,913 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:17:17,914 EPOCH 6 done: loss 0.6890 - lr 0.1000\n",
      "2020-05-15 16:17:19,851 DEV : loss 0.6734094023704529 - score 0.7667\n",
      "2020-05-15 16:17:19,986 BAD EPOCHS (no improvement): 5\n",
      "2020-05-15 16:17:20,249 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:17:20,297 epoch 7 - iter 0/4 - loss 0.67926550 - samples/sec: 696.21\n",
      "2020-05-15 16:17:20,342 epoch 7 - iter 1/4 - loss 0.76004568 - samples/sec: 915.45\n",
      "2020-05-15 16:17:20,397 epoch 7 - iter 2/4 - loss 0.69503315 - samples/sec: 763.93\n",
      "2020-05-15 16:17:20,420 epoch 7 - iter 3/4 - loss 0.68005759 - samples/sec: 2649.91\n",
      "2020-05-15 16:17:20,430 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:17:20,431 EPOCH 7 done: loss 0.6801 - lr 0.1000\n",
      "2020-05-15 16:17:22,364 DEV : loss 0.6717487573623657 - score 0.7667\n",
      "Epoch     6: reducing learning rate of group 0 to 5.0000e-02.\n",
      "2020-05-15 16:17:22,496 BAD EPOCHS (no improvement): 6\n",
      "2020-05-15 16:17:22,757 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:17:22,807 epoch 8 - iter 0/4 - loss 0.69243789 - samples/sec: 667.13\n",
      "2020-05-15 16:17:22,855 epoch 8 - iter 1/4 - loss 0.77589303 - samples/sec: 844.05\n",
      "2020-05-15 16:17:22,902 epoch 8 - iter 2/4 - loss 0.68292017 - samples/sec: 875.61\n",
      "2020-05-15 16:17:22,927 epoch 8 - iter 3/4 - loss 0.68921495 - samples/sec: 2779.12\n",
      "2020-05-15 16:17:22,941 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:17:22,942 EPOCH 8 done: loss 0.6892 - lr 0.0500\n",
      "2020-05-15 16:17:24,887 DEV : loss 0.6714375019073486 - score 0.7667\n",
      "2020-05-15 16:17:25,020 BAD EPOCHS (no improvement): 1\n",
      "2020-05-15 16:17:25,257 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:17:25,306 epoch 9 - iter 0/4 - loss 0.55280852 - samples/sec: 690.59\n",
      "2020-05-15 16:17:25,353 epoch 9 - iter 1/4 - loss 0.66486898 - samples/sec: 855.69\n",
      "2020-05-15 16:17:25,410 epoch 9 - iter 2/4 - loss 0.66008906 - samples/sec: 785.65\n",
      "2020-05-15 16:17:25,434 epoch 9 - iter 3/4 - loss 0.79708040 - samples/sec: 2995.06\n",
      "2020-05-15 16:17:25,448 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:17:25,449 EPOCH 9 done: loss 0.7971 - lr 0.0500\n",
      "2020-05-15 16:17:27,409 DEV : loss 0.6739473938941956 - score 0.7667\n",
      "2020-05-15 16:17:27,544 BAD EPOCHS (no improvement): 2\n",
      "2020-05-15 16:17:27,802 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:17:27,851 epoch 10 - iter 0/4 - loss 0.57512146 - samples/sec: 685.22\n",
      "2020-05-15 16:17:27,899 epoch 10 - iter 1/4 - loss 0.76447445 - samples/sec: 858.47\n",
      "2020-05-15 16:17:27,952 epoch 10 - iter 2/4 - loss 0.68891766 - samples/sec: 762.10\n",
      "2020-05-15 16:17:27,973 epoch 10 - iter 3/4 - loss 0.67637756 - samples/sec: 3186.71\n",
      "2020-05-15 16:17:27,986 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:17:27,987 EPOCH 10 done: loss 0.6764 - lr 0.0500\n",
      "2020-05-15 16:17:29,938 DEV : loss 0.6724002957344055 - score 0.7667\n",
      "2020-05-15 16:17:30,072 BAD EPOCHS (no improvement): 3\n",
      "2020-05-15 16:17:30,325 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:17:30,369 epoch 11 - iter 0/4 - loss 0.43365762 - samples/sec: 748.49\n",
      "2020-05-15 16:17:30,418 epoch 11 - iter 1/4 - loss 0.56076528 - samples/sec: 863.36\n",
      "2020-05-15 16:17:30,475 epoch 11 - iter 2/4 - loss 0.67320928 - samples/sec: 764.75\n",
      "2020-05-15 16:17:30,496 epoch 11 - iter 3/4 - loss 0.65650056 - samples/sec: 2952.76\n",
      "2020-05-15 16:17:30,506 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:17:30,507 EPOCH 11 done: loss 0.6565 - lr 0.0500\n",
      "2020-05-15 16:17:32,461 DEV : loss 0.6715922355651855 - score 0.7667\n",
      "2020-05-15 16:17:32,595 BAD EPOCHS (no improvement): 4\n",
      "2020-05-15 16:17:32,856 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:17:32,903 epoch 12 - iter 0/4 - loss 0.57769018 - samples/sec: 705.24\n",
      "2020-05-15 16:17:32,955 epoch 12 - iter 1/4 - loss 0.62697819 - samples/sec: 784.15\n",
      "2020-05-15 16:17:33,003 epoch 12 - iter 2/4 - loss 0.68136980 - samples/sec: 856.79\n",
      "2020-05-15 16:17:33,027 epoch 12 - iter 3/4 - loss 0.66167802 - samples/sec: 2655.41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-15 16:17:33,044 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:17:33,045 EPOCH 12 done: loss 0.6617 - lr 0.0500\n",
      "2020-05-15 16:17:34,992 DEV : loss 0.6712132692337036 - score 0.7667\n",
      "2020-05-15 16:17:35,126 BAD EPOCHS (no improvement): 5\n",
      "2020-05-15 16:17:35,379 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:17:35,419 epoch 13 - iter 0/4 - loss 0.65164173 - samples/sec: 849.37\n",
      "2020-05-15 16:17:35,474 epoch 13 - iter 1/4 - loss 0.63884535 - samples/sec: 759.30\n",
      "2020-05-15 16:17:35,526 epoch 13 - iter 2/4 - loss 0.66198879 - samples/sec: 766.89\n",
      "2020-05-15 16:17:35,552 epoch 13 - iter 3/4 - loss 0.74194731 - samples/sec: 2931.03\n",
      "2020-05-15 16:17:35,567 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:17:35,568 EPOCH 13 done: loss 0.7419 - lr 0.0500\n",
      "2020-05-15 16:17:37,529 DEV : loss 0.6754419803619385 - score 0.7667\n",
      "Epoch    12: reducing learning rate of group 0 to 2.5000e-02.\n",
      "2020-05-15 16:17:37,664 BAD EPOCHS (no improvement): 6\n",
      "2020-05-15 16:17:37,920 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:17:37,965 epoch 14 - iter 0/4 - loss 0.58597404 - samples/sec: 745.19\n",
      "2020-05-15 16:17:38,019 epoch 14 - iter 1/4 - loss 0.66350773 - samples/sec: 782.35\n",
      "2020-05-15 16:17:38,071 epoch 14 - iter 2/4 - loss 0.68257020 - samples/sec: 781.83\n",
      "2020-05-15 16:17:38,093 epoch 14 - iter 3/4 - loss 0.66130246 - samples/sec: 2890.44\n",
      "2020-05-15 16:17:38,103 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:17:38,104 EPOCH 14 done: loss 0.6613 - lr 0.0250\n",
      "2020-05-15 16:17:40,064 DEV : loss 0.674003005027771 - score 0.7667\n",
      "2020-05-15 16:17:40,198 BAD EPOCHS (no improvement): 1\n",
      "2020-05-15 16:17:40,463 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:17:40,512 epoch 15 - iter 0/4 - loss 0.66512150 - samples/sec: 683.31\n",
      "2020-05-15 16:17:40,562 epoch 15 - iter 1/4 - loss 0.66010430 - samples/sec: 825.85\n",
      "2020-05-15 16:17:40,612 epoch 15 - iter 2/4 - loss 0.65323466 - samples/sec: 828.98\n",
      "2020-05-15 16:17:40,635 epoch 15 - iter 3/4 - loss 0.74545227 - samples/sec: 2689.30\n",
      "2020-05-15 16:17:40,649 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:17:40,650 EPOCH 15 done: loss 0.7455 - lr 0.0250\n",
      "2020-05-15 16:17:42,604 DEV : loss 0.6755557656288147 - score 0.7667\n",
      "2020-05-15 16:17:42,736 BAD EPOCHS (no improvement): 2\n",
      "2020-05-15 16:17:43,292 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:17:43,294 Testing using best model ...\n",
      "2020-05-15 16:17:43,295 loading file best-model.pt\n",
      "2020-05-15 16:17:50,889 0.7597\t0.7597\t0.7597\n",
      "2020-05-15 16:17:50,890 \n",
      "MICRO_AVG: acc 0.6125 - f1-score 0.7597\n",
      "MACRO_AVG: acc 0.2532 - f1-score 0.2878\n",
      "0          tp: 0 - fp: 0 - fn: 173 - tn: 2827 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000\n",
      "1          tp: 2279 - fp: 721 - fn: 0 - tn: 0 - precision: 0.7597 - recall: 1.0000 - accuracy: 0.7597 - f1-score: 0.8634\n",
      "2          tp: 0 - fp: 0 - fn: 548 - tn: 2452 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000\n",
      "2020-05-15 16:17:50,891 ----------------------------------------------------------------------------------------------------\n",
      "150.27684903144836\n"
     ]
    }
   ],
   "source": [
    "# Flair\n",
    "# https://github.com/flairNLP/flair/blob/master/resources/docs/embeddings/FLAIR_EMBEDDINGS.md\n",
    "total_time = time.time()\n",
    "word_embeddings = [ # FlairEmbeddings('multi-forward'), # this is 300 languge, gave very low score\n",
    "                  # FlairEmbeddings('multi-backward'), \n",
    "                   FlairEmbeddings('news-forward'),  #  \tEnglish \tTrained with 1 billion word corpus                   \n",
    "                   \n",
    "                  ]\n",
    "#modelname = 'Flair-news-fwd'\n",
    "modelname = 'Flair'\n",
    "train_and_predict(word_embeddings, modelname, modeldesc, savelist=savelist, epochs=EPOCHS)\n",
    "\n",
    "print(time.time() - total_time)\n",
    "\n",
    "# 167s in 100 train\n",
    "# 620s for 18k"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BERT OOMs on 32 batch, use 8\n",
    "total_time = time.time()\n",
    "\n",
    "#word_embeddings = [ BertEmbeddings('bert-base-cased'),                ]\n",
    "#modelname = 'bert-base-cased'\n",
    "#train_and_predict(word_embeddings, modelname, modeldesc, savelist=savelist, epochs=EPOCHS, batch_size=8)\n",
    "\n",
    "word_embeddings = [ BertEmbeddings('bert-base-uncased'),                ]\n",
    "modelname = 'bert-base-uncased'\n",
    "train_and_predict(word_embeddings, modelname, modeldesc, savelist=savelist, epochs=EPOCHS, batch_size=8)\n",
    "          \n",
    "\n",
    "print(time.time() - total_time)\n",
    "\n",
    "# 450 s 18k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-cased\n",
      "2020-05-15 16:19:28,381 Reading data from /home/max/git/newcombined/dataset_hatespeech/input\n",
      "2020-05-15 16:19:28,382 Train: /home/max/git/newcombined/dataset_hatespeech/input/flair_train.csv\n",
      "2020-05-15 16:19:28,383 Dev: /home/max/git/newcombined/dataset_hatespeech/input/flair_dev.csv\n",
      "2020-05-15 16:19:28,383 Test: /home/max/git/newcombined/dataset_hatespeech/input/flair_test.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/ipykernel_launcher.py:24: DeprecationWarning: Call to deprecated function (or staticmethod) load_classification_corpus. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/flair/data_fetcher.py:452: DeprecationWarning: Call to deprecated function (or staticmethod) read_text_classification_file. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  train_file, tokenizer=tokenizer, max_tokens_per_doc=max_tokens_per_doc\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/flair/data_fetcher.py:457: DeprecationWarning: Call to deprecated function (or staticmethod) read_text_classification_file. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  test_file, tokenizer=tokenizer, max_tokens_per_doc=max_tokens_per_doc\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/flair/data_fetcher.py:464: DeprecationWarning: Call to deprecated function (or staticmethod) read_text_classification_file. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  dev_file, tokenizer=tokenizer, max_tokens_per_doc=max_tokens_per_doc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-15 16:19:29,825 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 214542.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-15 16:19:29,828 [b'1', b'2', b'0']\n",
      "2020-05-15 16:19:29,829 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 100/100 [00:00<00:00, 262965.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-15 16:19:29,831 [b'1', b'2', b'0']\n",
      "2020-05-15 16:19:29,837 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:19:29,841 Model: \"TextClassifier(\n",
      "  (document_embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): BertEmbeddings(\n",
      "        (model): BertModel(\n",
      "          (embeddings): BertEmbeddings(\n",
      "            (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
      "            (position_embeddings): Embedding(512, 768)\n",
      "            (token_type_embeddings): Embedding(2, 768)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (encoder): BertEncoder(\n",
      "            (layer): ModuleList(\n",
      "              (0): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (1): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (2): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (3): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (4): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (5): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (6): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (7): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (8): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (9): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (10): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (11): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (pooler): BertPooler(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (activation): Tanh()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=3072, out_features=256, bias=True)\n",
      "    (rnn): LSTM(256, 512, batch_first=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=512, out_features=3, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-15 16:19:29,842 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:19:29,843 Corpus: \"Corpus: 100 train + 3000 dev + 3000 test sentences\"\n",
      "2020-05-15 16:19:29,843 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:19:29,844 Parameters:\n",
      "2020-05-15 16:19:29,844  - learning_rate: \"0.1\"\n",
      "2020-05-15 16:19:29,845  - mini_batch_size: \"8\"\n",
      "2020-05-15 16:19:29,846  - patience: \"5\"\n",
      "2020-05-15 16:19:29,846  - anneal_factor: \"0.5\"\n",
      "2020-05-15 16:19:29,847  - max_epochs: \"15\"\n",
      "2020-05-15 16:19:29,848  - shuffle: \"True\"\n",
      "2020-05-15 16:19:29,848  - train_with_dev: \"False\"\n",
      "2020-05-15 16:19:29,849  - batch_growth_annealing: \"False\"\n",
      "2020-05-15 16:19:29,849 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:19:29,850 Model training base path: \".\"\n",
      "2020-05-15 16:19:29,851 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:19:29,851 Device: cuda:0\n",
      "2020-05-15 16:19:29,852 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:19:29,852 Embeddings storage mode: cpu\n",
      "2020-05-15 16:19:29,855 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-15 16:19:30,090 epoch 1 - iter 0/13 - loss 1.41461110 - samples/sec: 34.40\n",
      "2020-05-15 16:19:30,558 epoch 1 - iter 1/13 - loss 1.20395559 - samples/sec: 24.35\n",
      "2020-05-15 16:19:31,002 epoch 1 - iter 2/13 - loss 1.07781065 - samples/sec: 24.69\n",
      "2020-05-15 16:19:31,335 epoch 1 - iter 3/13 - loss 1.37533584 - samples/sec: 38.41\n",
      "2020-05-15 16:19:31,682 epoch 1 - iter 4/13 - loss 1.32350943 - samples/sec: 35.39\n",
      "2020-05-15 16:19:32,022 epoch 1 - iter 5/13 - loss 1.16319876 - samples/sec: 36.86\n",
      "2020-05-15 16:19:32,619 epoch 1 - iter 6/13 - loss 1.09168454 - samples/sec: 16.78\n",
      "2020-05-15 16:19:33,121 epoch 1 - iter 7/13 - loss 1.02606053 - samples/sec: 21.14\n",
      "2020-05-15 16:19:33,481 epoch 1 - iter 8/13 - loss 1.00356418 - samples/sec: 33.70\n",
      "2020-05-15 16:19:33,757 epoch 1 - iter 9/13 - loss 1.00424483 - samples/sec: 50.61\n",
      "2020-05-15 16:19:34,130 epoch 1 - iter 10/13 - loss 1.04102479 - samples/sec: 32.07\n",
      "2020-05-15 16:19:34,501 epoch 1 - iter 11/13 - loss 1.05993107 - samples/sec: 32.20\n",
      "2020-05-15 16:19:34,681 epoch 1 - iter 12/13 - loss 1.04970026 - samples/sec: 134.60\n",
      "2020-05-15 16:19:34,804 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:19:34,805 EPOCH 1 done: loss 1.0497 - lr 0.1000\n",
      "2020-05-15 16:21:05,199 DEV : loss 0.6972043514251709 - score 0.7667\n",
      "2020-05-15 16:21:05,338 BAD EPOCHS (no improvement): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DocumentRNNEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type StackedEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LayerNorm. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertEncoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ModuleList. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertLayer. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertAttention. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertSelfAttention. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertSelfOutput. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertIntermediate. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertOutput. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertPooler. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Tanh. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-15 16:21:06,661 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:21:06,686 epoch 2 - iter 0/13 - loss 0.52802789 - samples/sec: 375.81\n",
      "2020-05-15 16:21:06,832 epoch 2 - iter 1/13 - loss 0.43738170 - samples/sec: 451.46\n",
      "2020-05-15 16:21:06,979 epoch 2 - iter 2/13 - loss 0.67428387 - samples/sec: 417.62\n",
      "2020-05-15 16:21:07,126 epoch 2 - iter 3/13 - loss 0.74903423 - samples/sec: 455.77\n",
      "2020-05-15 16:21:07,264 epoch 2 - iter 4/13 - loss 0.90052343 - samples/sec: 521.10\n",
      "2020-05-15 16:21:07,406 epoch 2 - iter 5/13 - loss 0.91421305 - samples/sec: 475.56\n",
      "2020-05-15 16:21:07,547 epoch 2 - iter 6/13 - loss 0.88897076 - samples/sec: 426.80\n",
      "2020-05-15 16:21:07,690 epoch 2 - iter 7/13 - loss 1.02994783 - samples/sec: 442.89\n",
      "2020-05-15 16:21:07,833 epoch 2 - iter 8/13 - loss 1.01186102 - samples/sec: 450.61\n",
      "2020-05-15 16:21:07,970 epoch 2 - iter 9/13 - loss 0.97415586 - samples/sec: 534.95\n",
      "2020-05-15 16:21:08,110 epoch 2 - iter 10/13 - loss 0.97756032 - samples/sec: 441.17\n",
      "2020-05-15 16:21:08,271 epoch 2 - iter 11/13 - loss 0.99013559 - samples/sec: 464.21\n",
      "2020-05-15 16:21:08,422 epoch 2 - iter 12/13 - loss 0.96861987 - samples/sec: 646.80\n",
      "2020-05-15 16:21:08,561 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:21:08,561 EPOCH 2 done: loss 0.9686 - lr 0.1000\n",
      "2020-05-15 16:21:11,162 DEV : loss 0.867752730846405 - score 0.7667\n",
      "2020-05-15 16:21:11,301 BAD EPOCHS (no improvement): 1\n",
      "2020-05-15 16:21:12,612 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:21:12,638 epoch 3 - iter 0/13 - loss 0.83907753 - samples/sec: 361.24\n",
      "2020-05-15 16:21:12,784 epoch 3 - iter 1/13 - loss 0.90161243 - samples/sec: 515.37\n",
      "2020-05-15 16:21:12,941 epoch 3 - iter 2/13 - loss 0.82352942 - samples/sec: 428.56\n",
      "2020-05-15 16:21:13,079 epoch 3 - iter 3/13 - loss 0.83947036 - samples/sec: 502.59\n",
      "2020-05-15 16:21:13,235 epoch 3 - iter 4/13 - loss 0.97557638 - samples/sec: 404.28\n",
      "2020-05-15 16:21:13,390 epoch 3 - iter 5/13 - loss 1.07503118 - samples/sec: 409.50\n",
      "2020-05-15 16:21:13,547 epoch 3 - iter 6/13 - loss 0.96713125 - samples/sec: 382.15\n",
      "2020-05-15 16:21:13,707 epoch 3 - iter 7/13 - loss 0.86556985 - samples/sec: 461.38\n",
      "2020-05-15 16:21:13,851 epoch 3 - iter 8/13 - loss 0.82178197 - samples/sec: 501.78\n",
      "2020-05-15 16:21:13,995 epoch 3 - iter 9/13 - loss 0.74870971 - samples/sec: 477.42\n",
      "2020-05-15 16:21:14,139 epoch 3 - iter 10/13 - loss 0.76327458 - samples/sec: 400.44\n",
      "2020-05-15 16:21:14,283 epoch 3 - iter 11/13 - loss 0.77835942 - samples/sec: 456.97\n",
      "2020-05-15 16:21:14,424 epoch 3 - iter 12/13 - loss 0.78017148 - samples/sec: 558.54\n",
      "2020-05-15 16:21:14,552 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:21:14,553 EPOCH 3 done: loss 0.7802 - lr 0.1000\n",
      "2020-05-15 16:21:17,140 DEV : loss 0.9680686593055725 - score 0.3543\n",
      "2020-05-15 16:21:17,276 BAD EPOCHS (no improvement): 2\n",
      "2020-05-15 16:21:17,278 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:21:17,301 epoch 4 - iter 0/13 - loss 0.74168330 - samples/sec: 375.96\n",
      "2020-05-15 16:21:17,451 epoch 4 - iter 1/13 - loss 1.15150633 - samples/sec: 484.85\n",
      "2020-05-15 16:21:17,600 epoch 4 - iter 2/13 - loss 0.80351572 - samples/sec: 448.73\n",
      "2020-05-15 16:21:17,745 epoch 4 - iter 3/13 - loss 0.92682690 - samples/sec: 454.63\n",
      "2020-05-15 16:21:17,886 epoch 4 - iter 4/13 - loss 1.00813933 - samples/sec: 456.52\n",
      "2020-05-15 16:21:18,030 epoch 4 - iter 5/13 - loss 0.94352523 - samples/sec: 459.94\n",
      "2020-05-15 16:21:18,173 epoch 4 - iter 6/13 - loss 1.00129968 - samples/sec: 447.91\n",
      "2020-05-15 16:21:18,322 epoch 4 - iter 7/13 - loss 0.96943135 - samples/sec: 397.28\n",
      "2020-05-15 16:21:18,459 epoch 4 - iter 8/13 - loss 0.95244503 - samples/sec: 486.40\n",
      "2020-05-15 16:21:18,605 epoch 4 - iter 9/13 - loss 0.87263349 - samples/sec: 422.65\n",
      "2020-05-15 16:21:18,746 epoch 4 - iter 10/13 - loss 0.83443684 - samples/sec: 438.96\n",
      "2020-05-15 16:21:18,892 epoch 4 - iter 11/13 - loss 0.79652441 - samples/sec: 471.65\n",
      "2020-05-15 16:21:19,030 epoch 4 - iter 12/13 - loss 0.81284494 - samples/sec: 697.02\n",
      "2020-05-15 16:21:19,154 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:21:19,154 EPOCH 4 done: loss 0.8128 - lr 0.1000\n",
      "2020-05-15 16:21:21,705 DEV : loss 0.6972891092300415 - score 0.7667\n",
      "2020-05-15 16:21:21,843 BAD EPOCHS (no improvement): 3\n",
      "2020-05-15 16:21:23,205 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:21:23,229 epoch 5 - iter 0/13 - loss 0.93245786 - samples/sec: 405.62\n",
      "2020-05-15 16:21:23,378 epoch 5 - iter 1/13 - loss 0.96904889 - samples/sec: 482.53\n",
      "2020-05-15 16:21:23,520 epoch 5 - iter 2/13 - loss 0.91524472 - samples/sec: 499.14\n",
      "2020-05-15 16:21:23,664 epoch 5 - iter 3/13 - loss 0.85984574 - samples/sec: 463.08\n",
      "2020-05-15 16:21:23,815 epoch 5 - iter 4/13 - loss 0.79950110 - samples/sec: 459.16\n",
      "2020-05-15 16:21:23,963 epoch 5 - iter 5/13 - loss 0.78613672 - samples/sec: 489.43\n",
      "2020-05-15 16:21:24,111 epoch 5 - iter 6/13 - loss 0.77182141 - samples/sec: 479.99\n",
      "2020-05-15 16:21:24,258 epoch 5 - iter 7/13 - loss 0.88726795 - samples/sec: 500.07\n",
      "2020-05-15 16:21:24,399 epoch 5 - iter 8/13 - loss 0.86514374 - samples/sec: 476.21\n",
      "2020-05-15 16:21:24,547 epoch 5 - iter 9/13 - loss 0.82380880 - samples/sec: 455.65\n",
      "2020-05-15 16:21:24,698 epoch 5 - iter 10/13 - loss 0.75866263 - samples/sec: 431.46\n",
      "2020-05-15 16:21:24,839 epoch 5 - iter 11/13 - loss 0.76727427 - samples/sec: 545.30\n",
      "2020-05-15 16:21:24,989 epoch 5 - iter 12/13 - loss 0.72949171 - samples/sec: 544.97\n",
      "2020-05-15 16:21:25,146 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:21:25,147 EPOCH 5 done: loss 0.7295 - lr 0.1000\n",
      "2020-05-15 16:21:27,763 DEV : loss 0.8436142206192017 - score 0.7667\n",
      "2020-05-15 16:21:27,906 BAD EPOCHS (no improvement): 4\n",
      "2020-05-15 16:21:29,247 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:21:29,268 epoch 6 - iter 0/13 - loss 0.82738948 - samples/sec: 472.30\n",
      "2020-05-15 16:21:29,415 epoch 6 - iter 1/13 - loss 0.55718035 - samples/sec: 726.88\n",
      "2020-05-15 16:21:29,555 epoch 6 - iter 2/13 - loss 0.51390344 - samples/sec: 641.56\n",
      "2020-05-15 16:21:29,695 epoch 6 - iter 3/13 - loss 0.44307361 - samples/sec: 610.72\n",
      "2020-05-15 16:21:29,836 epoch 6 - iter 4/13 - loss 0.38045985 - samples/sec: 524.80\n",
      "2020-05-15 16:21:30,001 epoch 6 - iter 5/13 - loss 0.55664364 - samples/sec: 436.63\n",
      "2020-05-15 16:21:30,161 epoch 6 - iter 6/13 - loss 0.55340487 - samples/sec: 431.14\n",
      "2020-05-15 16:21:30,318 epoch 6 - iter 7/13 - loss 0.54382841 - samples/sec: 500.72\n",
      "2020-05-15 16:21:30,494 epoch 6 - iter 8/13 - loss 0.52797139 - samples/sec: 529.00\n",
      "2020-05-15 16:21:30,662 epoch 6 - iter 9/13 - loss 0.57905094 - samples/sec: 445.10\n",
      "2020-05-15 16:21:30,823 epoch 6 - iter 10/13 - loss 0.61505950 - samples/sec: 409.00\n",
      "2020-05-15 16:21:30,981 epoch 6 - iter 11/13 - loss 0.65590067 - samples/sec: 382.47\n",
      "2020-05-15 16:21:31,127 epoch 6 - iter 12/13 - loss 0.64062801 - samples/sec: 749.03\n",
      "2020-05-15 16:21:31,265 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:21:31,266 EPOCH 6 done: loss 0.6406 - lr 0.1000\n",
      "2020-05-15 16:21:33,826 DEV : loss 0.7081512212753296 - score 0.7183\n",
      "2020-05-15 16:21:33,968 BAD EPOCHS (no improvement): 5\n",
      "2020-05-15 16:21:33,970 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:21:33,987 epoch 7 - iter 0/13 - loss 0.59934705 - samples/sec: 550.68\n",
      "2020-05-15 16:21:34,143 epoch 7 - iter 1/13 - loss 0.75045791 - samples/sec: 472.09\n",
      "2020-05-15 16:21:34,298 epoch 7 - iter 2/13 - loss 0.57983072 - samples/sec: 445.62\n",
      "2020-05-15 16:21:34,445 epoch 7 - iter 3/13 - loss 0.65946662 - samples/sec: 454.43\n",
      "2020-05-15 16:21:34,602 epoch 7 - iter 4/13 - loss 0.61549513 - samples/sec: 430.50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-15 16:21:34,759 epoch 7 - iter 5/13 - loss 0.56057896 - samples/sec: 527.27\n",
      "2020-05-15 16:21:34,920 epoch 7 - iter 6/13 - loss 0.70415018 - samples/sec: 489.75\n",
      "2020-05-15 16:21:35,073 epoch 7 - iter 7/13 - loss 0.71458572 - samples/sec: 569.11\n",
      "2020-05-15 16:21:35,230 epoch 7 - iter 8/13 - loss 0.72167468 - samples/sec: 481.70\n",
      "2020-05-15 16:21:35,388 epoch 7 - iter 9/13 - loss 0.69868794 - samples/sec: 416.98\n",
      "2020-05-15 16:21:35,554 epoch 7 - iter 10/13 - loss 0.64447232 - samples/sec: 432.16\n",
      "2020-05-15 16:21:35,711 epoch 7 - iter 11/13 - loss 0.63086717 - samples/sec: 512.96\n",
      "2020-05-15 16:21:35,873 epoch 7 - iter 12/13 - loss 0.60111670 - samples/sec: 582.54\n",
      "2020-05-15 16:21:36,004 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:21:36,004 EPOCH 7 done: loss 0.6011 - lr 0.1000\n",
      "2020-05-15 16:21:38,544 DEV : loss 0.7002679705619812 - score 0.747\n",
      "Epoch     6: reducing learning rate of group 0 to 5.0000e-02.\n",
      "2020-05-15 16:21:38,686 BAD EPOCHS (no improvement): 6\n",
      "2020-05-15 16:21:38,687 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:21:38,709 epoch 8 - iter 0/13 - loss 0.52372456 - samples/sec: 443.05\n",
      "2020-05-15 16:21:38,853 epoch 8 - iter 1/13 - loss 0.57504001 - samples/sec: 448.29\n",
      "2020-05-15 16:21:38,999 epoch 8 - iter 2/13 - loss 0.50640058 - samples/sec: 487.02\n",
      "2020-05-15 16:21:39,150 epoch 8 - iter 3/13 - loss 0.54463746 - samples/sec: 421.66\n",
      "2020-05-15 16:21:39,302 epoch 8 - iter 4/13 - loss 0.49452806 - samples/sec: 394.62\n",
      "2020-05-15 16:21:39,441 epoch 8 - iter 5/13 - loss 0.51873602 - samples/sec: 511.54\n",
      "2020-05-15 16:21:39,585 epoch 8 - iter 6/13 - loss 0.56846040 - samples/sec: 443.34\n",
      "2020-05-15 16:21:39,736 epoch 8 - iter 7/13 - loss 0.57165617 - samples/sec: 510.77\n",
      "2020-05-15 16:21:39,879 epoch 8 - iter 8/13 - loss 0.52780940 - samples/sec: 500.95\n",
      "2020-05-15 16:21:40,022 epoch 8 - iter 9/13 - loss 0.49348440 - samples/sec: 531.35\n",
      "2020-05-15 16:21:40,161 epoch 8 - iter 10/13 - loss 0.49761175 - samples/sec: 455.33\n",
      "2020-05-15 16:21:40,305 epoch 8 - iter 11/13 - loss 0.51941920 - samples/sec: 474.75\n",
      "2020-05-15 16:21:40,443 epoch 8 - iter 12/13 - loss 0.53438110 - samples/sec: 598.72\n",
      "2020-05-15 16:21:40,567 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:21:40,568 EPOCH 8 done: loss 0.5344 - lr 0.0500\n",
      "2020-05-15 16:21:43,090 DEV : loss 0.6860483288764954 - score 0.7683\n",
      "2020-05-15 16:21:43,230 BAD EPOCHS (no improvement): 0\n",
      "2020-05-15 16:21:44,529 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:21:44,552 epoch 9 - iter 0/13 - loss 0.47362703 - samples/sec: 428.10\n",
      "2020-05-15 16:21:44,694 epoch 9 - iter 1/13 - loss 0.48704115 - samples/sec: 390.40\n",
      "2020-05-15 16:21:44,838 epoch 9 - iter 2/13 - loss 0.39121189 - samples/sec: 430.95\n",
      "2020-05-15 16:21:44,977 epoch 9 - iter 3/13 - loss 0.32483112 - samples/sec: 505.47\n",
      "2020-05-15 16:21:45,118 epoch 9 - iter 4/13 - loss 0.45870614 - samples/sec: 491.50\n",
      "2020-05-15 16:21:45,259 epoch 9 - iter 5/13 - loss 0.42449305 - samples/sec: 479.51\n",
      "2020-05-15 16:21:45,401 epoch 9 - iter 6/13 - loss 0.41256829 - samples/sec: 513.47\n",
      "2020-05-15 16:21:45,537 epoch 9 - iter 7/13 - loss 0.42663118 - samples/sec: 502.57\n",
      "2020-05-15 16:21:45,677 epoch 9 - iter 8/13 - loss 0.48607795 - samples/sec: 483.72\n",
      "2020-05-15 16:21:45,822 epoch 9 - iter 9/13 - loss 0.45278043 - samples/sec: 392.79\n",
      "2020-05-15 16:21:45,959 epoch 9 - iter 10/13 - loss 0.43954022 - samples/sec: 498.85\n",
      "2020-05-15 16:21:46,101 epoch 9 - iter 11/13 - loss 0.42615313 - samples/sec: 462.67\n",
      "2020-05-15 16:21:46,239 epoch 9 - iter 12/13 - loss 0.45856222 - samples/sec: 640.45\n",
      "2020-05-15 16:21:46,361 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:21:46,362 EPOCH 9 done: loss 0.4586 - lr 0.0500\n",
      "2020-05-15 16:21:48,871 DEV : loss 0.7211163640022278 - score 0.7087\n",
      "2020-05-15 16:21:49,011 BAD EPOCHS (no improvement): 1\n",
      "2020-05-15 16:21:49,012 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:21:49,032 epoch 10 - iter 0/13 - loss 0.44604373 - samples/sec: 464.55\n",
      "2020-05-15 16:21:49,169 epoch 10 - iter 1/13 - loss 0.25239803 - samples/sec: 502.61\n",
      "2020-05-15 16:21:49,314 epoch 10 - iter 2/13 - loss 0.39808592 - samples/sec: 423.14\n",
      "2020-05-15 16:21:49,454 epoch 10 - iter 3/13 - loss 0.36753809 - samples/sec: 459.40\n",
      "2020-05-15 16:21:49,597 epoch 10 - iter 4/13 - loss 0.34933925 - samples/sec: 437.82\n",
      "2020-05-15 16:21:49,737 epoch 10 - iter 5/13 - loss 0.40480191 - samples/sec: 481.47\n",
      "2020-05-15 16:21:49,884 epoch 10 - iter 6/13 - loss 0.46803591 - samples/sec: 443.95\n",
      "2020-05-15 16:21:50,023 epoch 10 - iter 7/13 - loss 0.53060096 - samples/sec: 575.78\n",
      "2020-05-15 16:21:50,169 epoch 10 - iter 8/13 - loss 0.49423948 - samples/sec: 383.07\n",
      "2020-05-15 16:21:50,312 epoch 10 - iter 9/13 - loss 0.47036668 - samples/sec: 412.88\n",
      "2020-05-15 16:21:50,452 epoch 10 - iter 10/13 - loss 0.44206503 - samples/sec: 505.62\n",
      "2020-05-15 16:21:50,594 epoch 10 - iter 11/13 - loss 0.42675748 - samples/sec: 438.02\n",
      "2020-05-15 16:21:50,734 epoch 10 - iter 12/13 - loss 0.40635273 - samples/sec: 756.98\n",
      "2020-05-15 16:21:50,858 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:21:50,859 EPOCH 10 done: loss 0.4064 - lr 0.0500\n",
      "2020-05-15 16:21:53,374 DEV : loss 0.6946401000022888 - score 0.7547\n",
      "2020-05-15 16:21:53,517 BAD EPOCHS (no improvement): 2\n",
      "2020-05-15 16:21:53,519 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:21:53,539 epoch 11 - iter 0/13 - loss 0.11433923 - samples/sec: 441.49\n",
      "2020-05-15 16:21:53,694 epoch 11 - iter 1/13 - loss 0.21840036 - samples/sec: 447.89\n",
      "2020-05-15 16:21:53,845 epoch 11 - iter 2/13 - loss 0.21100648 - samples/sec: 430.85\n",
      "2020-05-15 16:21:53,985 epoch 11 - iter 3/13 - loss 0.25324999 - samples/sec: 498.68\n",
      "2020-05-15 16:21:54,130 epoch 11 - iter 4/13 - loss 0.33716489 - samples/sec: 446.30\n",
      "2020-05-15 16:21:54,273 epoch 11 - iter 5/13 - loss 0.30282176 - samples/sec: 437.74\n",
      "2020-05-15 16:21:54,417 epoch 11 - iter 6/13 - loss 0.29529515 - samples/sec: 445.87\n",
      "2020-05-15 16:21:54,557 epoch 11 - iter 7/13 - loss 0.32590144 - samples/sec: 501.61\n",
      "2020-05-15 16:21:54,702 epoch 11 - iter 8/13 - loss 0.32544449 - samples/sec: 452.48\n",
      "2020-05-15 16:21:54,845 epoch 11 - iter 9/13 - loss 0.34781594 - samples/sec: 424.37\n",
      "2020-05-15 16:21:54,989 epoch 11 - iter 10/13 - loss 0.33600190 - samples/sec: 452.20\n",
      "2020-05-15 16:21:55,130 epoch 11 - iter 11/13 - loss 0.34257379 - samples/sec: 474.97\n",
      "2020-05-15 16:21:55,267 epoch 11 - iter 12/13 - loss 0.32202652 - samples/sec: 650.43\n",
      "2020-05-15 16:21:55,396 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:21:55,397 EPOCH 11 done: loss 0.3220 - lr 0.0500\n",
      "2020-05-15 16:21:57,916 DEV : loss 0.7200090885162354 - score 0.7607\n",
      "2020-05-15 16:21:58,060 BAD EPOCHS (no improvement): 3\n",
      "2020-05-15 16:21:58,061 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:21:58,081 epoch 12 - iter 0/13 - loss 0.54892755 - samples/sec: 442.93\n",
      "2020-05-15 16:21:58,226 epoch 12 - iter 1/13 - loss 0.67954352 - samples/sec: 483.21\n",
      "2020-05-15 16:21:58,367 epoch 12 - iter 2/13 - loss 0.51270005 - samples/sec: 506.57\n",
      "2020-05-15 16:21:58,510 epoch 12 - iter 3/13 - loss 0.46334003 - samples/sec: 481.55\n",
      "2020-05-15 16:21:58,653 epoch 12 - iter 4/13 - loss 0.45642542 - samples/sec: 537.68\n",
      "2020-05-15 16:21:58,793 epoch 12 - iter 5/13 - loss 0.43815381 - samples/sec: 441.91\n",
      "2020-05-15 16:21:58,942 epoch 12 - iter 6/13 - loss 0.42168625 - samples/sec: 474.35\n",
      "2020-05-15 16:21:59,083 epoch 12 - iter 7/13 - loss 0.40370838 - samples/sec: 492.60\n",
      "2020-05-15 16:21:59,230 epoch 12 - iter 8/13 - loss 0.42747877 - samples/sec: 485.33\n",
      "2020-05-15 16:21:59,378 epoch 12 - iter 9/13 - loss 0.41749628 - samples/sec: 428.88\n",
      "2020-05-15 16:21:59,518 epoch 12 - iter 10/13 - loss 0.45237238 - samples/sec: 470.82\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-15 16:21:59,663 epoch 12 - iter 11/13 - loss 0.45356869 - samples/sec: 506.10\n",
      "2020-05-15 16:21:59,811 epoch 12 - iter 12/13 - loss 0.43297456 - samples/sec: 549.87\n",
      "2020-05-15 16:21:59,938 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:21:59,939 EPOCH 12 done: loss 0.4330 - lr 0.0500\n",
      "2020-05-15 16:22:02,448 DEV : loss 0.6851819753646851 - score 0.7423\n",
      "2020-05-15 16:22:02,589 BAD EPOCHS (no improvement): 4\n",
      "2020-05-15 16:22:02,590 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:22:02,610 epoch 13 - iter 0/13 - loss 0.23217896 - samples/sec: 482.25\n",
      "2020-05-15 16:22:02,756 epoch 13 - iter 1/13 - loss 0.23361129 - samples/sec: 476.19\n",
      "2020-05-15 16:22:02,897 epoch 13 - iter 2/13 - loss 0.16635740 - samples/sec: 480.15\n",
      "2020-05-15 16:22:03,037 epoch 13 - iter 3/13 - loss 0.42903942 - samples/sec: 472.20\n",
      "2020-05-15 16:22:03,180 epoch 13 - iter 4/13 - loss 0.38119575 - samples/sec: 437.10\n",
      "2020-05-15 16:22:03,323 epoch 13 - iter 5/13 - loss 0.33480323 - samples/sec: 450.10\n",
      "2020-05-15 16:22:03,467 epoch 13 - iter 6/13 - loss 0.30139132 - samples/sec: 471.97\n",
      "2020-05-15 16:22:03,614 epoch 13 - iter 7/13 - loss 0.28947260 - samples/sec: 380.70\n",
      "2020-05-15 16:22:03,753 epoch 13 - iter 8/13 - loss 0.26810708 - samples/sec: 526.63\n",
      "2020-05-15 16:22:03,897 epoch 13 - iter 9/13 - loss 0.28902838 - samples/sec: 436.37\n",
      "2020-05-15 16:22:04,040 epoch 13 - iter 10/13 - loss 0.28689154 - samples/sec: 442.15\n",
      "2020-05-15 16:22:04,183 epoch 13 - iter 11/13 - loss 0.30537813 - samples/sec: 462.72\n",
      "2020-05-15 16:22:04,324 epoch 13 - iter 12/13 - loss 0.30573244 - samples/sec: 639.80\n",
      "2020-05-15 16:22:04,454 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:22:04,455 EPOCH 13 done: loss 0.3057 - lr 0.0500\n",
      "2020-05-15 16:22:06,981 DEV : loss 0.7574881315231323 - score 0.7183\n",
      "2020-05-15 16:22:07,124 BAD EPOCHS (no improvement): 5\n",
      "2020-05-15 16:22:07,126 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:22:07,144 epoch 14 - iter 0/13 - loss 0.72768486 - samples/sec: 488.75\n",
      "2020-05-15 16:22:07,285 epoch 14 - iter 1/13 - loss 0.65954506 - samples/sec: 593.08\n",
      "2020-05-15 16:22:07,424 epoch 14 - iter 2/13 - loss 0.49350544 - samples/sec: 487.31\n",
      "2020-05-15 16:22:07,567 epoch 14 - iter 3/13 - loss 0.38375711 - samples/sec: 469.88\n",
      "2020-05-15 16:22:07,714 epoch 14 - iter 4/13 - loss 0.40378359 - samples/sec: 477.25\n",
      "2020-05-15 16:22:07,851 epoch 14 - iter 5/13 - loss 0.41465099 - samples/sec: 478.17\n",
      "2020-05-15 16:22:08,000 epoch 14 - iter 6/13 - loss 0.38208852 - samples/sec: 438.21\n",
      "2020-05-15 16:22:08,143 epoch 14 - iter 7/13 - loss 0.36328176 - samples/sec: 455.34\n",
      "2020-05-15 16:22:08,289 epoch 14 - iter 8/13 - loss 0.34541458 - samples/sec: 477.68\n",
      "2020-05-15 16:22:08,434 epoch 14 - iter 9/13 - loss 0.34339646 - samples/sec: 468.70\n",
      "2020-05-15 16:22:08,574 epoch 14 - iter 10/13 - loss 0.32405361 - samples/sec: 471.82\n",
      "2020-05-15 16:22:08,718 epoch 14 - iter 11/13 - loss 0.30349286 - samples/sec: 439.13\n",
      "2020-05-15 16:22:08,863 epoch 14 - iter 12/13 - loss 0.31350072 - samples/sec: 585.51\n",
      "2020-05-15 16:22:08,990 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:22:08,991 EPOCH 14 done: loss 0.3135 - lr 0.0500\n",
      "2020-05-15 16:22:11,518 DEV : loss 1.5770717859268188 - score 0.4247\n",
      "Epoch    13: reducing learning rate of group 0 to 2.5000e-02.\n",
      "2020-05-15 16:22:11,658 BAD EPOCHS (no improvement): 6\n",
      "2020-05-15 16:22:11,660 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:22:11,680 epoch 15 - iter 0/13 - loss 0.82662904 - samples/sec: 458.83\n",
      "2020-05-15 16:22:11,824 epoch 15 - iter 1/13 - loss 0.64223446 - samples/sec: 447.71\n",
      "2020-05-15 16:22:11,967 epoch 15 - iter 2/13 - loss 0.57141023 - samples/sec: 470.60\n",
      "2020-05-15 16:22:12,113 epoch 15 - iter 3/13 - loss 0.52304003 - samples/sec: 438.48\n",
      "2020-05-15 16:22:12,255 epoch 15 - iter 4/13 - loss 0.42903100 - samples/sec: 434.70\n",
      "2020-05-15 16:22:12,392 epoch 15 - iter 5/13 - loss 0.37582903 - samples/sec: 512.15\n",
      "2020-05-15 16:22:12,542 epoch 15 - iter 6/13 - loss 0.42188842 - samples/sec: 401.46\n",
      "2020-05-15 16:22:12,686 epoch 15 - iter 7/13 - loss 0.39780648 - samples/sec: 423.80\n",
      "2020-05-15 16:22:12,824 epoch 15 - iter 8/13 - loss 0.36866156 - samples/sec: 508.90\n",
      "2020-05-15 16:22:12,965 epoch 15 - iter 9/13 - loss 0.34785928 - samples/sec: 511.06\n",
      "2020-05-15 16:22:13,104 epoch 15 - iter 10/13 - loss 0.34985412 - samples/sec: 431.50\n",
      "2020-05-15 16:22:13,248 epoch 15 - iter 11/13 - loss 0.33425205 - samples/sec: 434.56\n",
      "2020-05-15 16:22:13,387 epoch 15 - iter 12/13 - loss 0.34489027 - samples/sec: 640.28\n",
      "2020-05-15 16:22:13,515 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:22:13,516 EPOCH 15 done: loss 0.3449 - lr 0.0250\n",
      "2020-05-15 16:22:16,030 DEV : loss 0.7604377269744873 - score 0.7243\n",
      "2020-05-15 16:22:16,173 BAD EPOCHS (no improvement): 1\n",
      "2020-05-15 16:22:17,477 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:22:17,478 Testing using best model ...\n",
      "2020-05-15 16:22:17,481 loading file best-model.pt\n",
      "2020-05-15 16:23:48,249 0.7693\t0.7693\t0.7693\n",
      "2020-05-15 16:23:48,250 \n",
      "MICRO_AVG: acc 0.6251 - f1-score 0.7693\n",
      "MACRO_AVG: acc 0.2851 - f1-score 0.34369999999999995\n",
      "0          tp: 0 - fp: 0 - fn: 173 - tn: 2827 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000\n",
      "1          tp: 2257 - fp: 666 - fn: 22 - tn: 55 - precision: 0.7722 - recall: 0.9903 - accuracy: 0.7664 - f1-score: 0.8678\n",
      "2          tp: 51 - fp: 26 - fn: 497 - tn: 2426 - precision: 0.6623 - recall: 0.0931 - accuracy: 0.0889 - f1-score: 0.1633\n",
      "2020-05-15 16:23:48,251 ----------------------------------------------------------------------------------------------------\n",
      "BytePairEmbedding\n",
      "2020-05-15 16:24:59,711 Reading data from /home/max/git/newcombined/dataset_hatespeech/input\n",
      "2020-05-15 16:24:59,712 Train: /home/max/git/newcombined/dataset_hatespeech/input/flair_train.csv\n",
      "2020-05-15 16:24:59,712 Dev: /home/max/git/newcombined/dataset_hatespeech/input/flair_dev.csv\n",
      "2020-05-15 16:24:59,713 Test: /home/max/git/newcombined/dataset_hatespeech/input/flair_test.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/ipykernel_launcher.py:24: DeprecationWarning: Call to deprecated function (or staticmethod) load_classification_corpus. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/flair/data_fetcher.py:452: DeprecationWarning: Call to deprecated function (or staticmethod) read_text_classification_file. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  train_file, tokenizer=tokenizer, max_tokens_per_doc=max_tokens_per_doc\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/flair/data_fetcher.py:457: DeprecationWarning: Call to deprecated function (or staticmethod) read_text_classification_file. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  test_file, tokenizer=tokenizer, max_tokens_per_doc=max_tokens_per_doc\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/flair/data_fetcher.py:464: DeprecationWarning: Call to deprecated function (or staticmethod) read_text_classification_file. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  dev_file, tokenizer=tokenizer, max_tokens_per_doc=max_tokens_per_doc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-15 16:25:03,241 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 93020.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-15 16:25:03,246 [b'1', b'2', b'0']\n",
      "2020-05-15 16:25:03,247 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 100/100 [00:00<00:00, 169398.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-15 16:25:03,251 [b'1', b'2', b'0']\n",
      "2020-05-15 16:25:03,254 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:25:03,255 Model: \"TextClassifier(\n",
      "  (document_embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): BytePairEmbeddings(model=0-bpe-en-100000-50)\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=100, out_features=256, bias=True)\n",
      "    (rnn): LSTM(256, 512, batch_first=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=512, out_features=3, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\"\n",
      "2020-05-15 16:25:03,256 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:25:03,258 Corpus: \"Corpus: 100 train + 3000 dev + 3000 test sentences\"\n",
      "2020-05-15 16:25:03,258 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:25:03,258 Parameters:\n",
      "2020-05-15 16:25:03,259  - learning_rate: \"0.1\"\n",
      "2020-05-15 16:25:03,260  - mini_batch_size: \"8\"\n",
      "2020-05-15 16:25:03,260  - patience: \"5\"\n",
      "2020-05-15 16:25:03,260  - anneal_factor: \"0.5\"\n",
      "2020-05-15 16:25:03,261  - max_epochs: \"15\"\n",
      "2020-05-15 16:25:03,261  - shuffle: \"True\"\n",
      "2020-05-15 16:25:03,262  - train_with_dev: \"False\"\n",
      "2020-05-15 16:25:03,262  - batch_growth_annealing: \"False\"\n",
      "2020-05-15 16:25:03,263 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:25:03,263 Model training base path: \".\"\n",
      "2020-05-15 16:25:03,264 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:25:03,264 Device: cuda:0\n",
      "2020-05-15 16:25:03,265 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:25:03,265 Embeddings storage mode: cpu\n",
      "2020-05-15 16:25:03,271 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:25:03,302 epoch 1 - iter 0/13 - loss 1.16529024 - samples/sec: 273.81\n",
      "2020-05-15 16:25:03,346 epoch 1 - iter 1/13 - loss 1.06600815 - samples/sec: 221.62\n",
      "2020-05-15 16:25:03,390 epoch 1 - iter 2/13 - loss 0.96225617 - samples/sec: 209.13\n",
      "2020-05-15 16:25:03,424 epoch 1 - iter 3/13 - loss 0.93295111 - samples/sec: 280.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-15 16:25:03,465 epoch 1 - iter 4/13 - loss 0.92073661 - samples/sec: 276.75\n",
      "2020-05-15 16:25:03,500 epoch 1 - iter 5/13 - loss 0.87916565 - samples/sec: 267.92\n",
      "2020-05-15 16:25:03,541 epoch 1 - iter 6/13 - loss 0.82724278 - samples/sec: 226.60\n",
      "2020-05-15 16:25:03,586 epoch 1 - iter 7/13 - loss 0.79553075 - samples/sec: 203.32\n",
      "2020-05-15 16:25:03,624 epoch 1 - iter 8/13 - loss 0.81103737 - samples/sec: 306.21\n",
      "2020-05-15 16:25:03,658 epoch 1 - iter 9/13 - loss 0.83728761 - samples/sec: 276.29\n",
      "2020-05-15 16:25:03,697 epoch 1 - iter 10/13 - loss 0.85851285 - samples/sec: 248.32\n",
      "2020-05-15 16:25:03,731 epoch 1 - iter 11/13 - loss 0.82654532 - samples/sec: 303.97\n",
      "2020-05-15 16:25:03,755 epoch 1 - iter 12/13 - loss 0.83152409 - samples/sec: 446.98\n",
      "2020-05-15 16:25:03,764 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:25:03,765 EPOCH 1 done: loss 0.8315 - lr 0.1000\n",
      "2020-05-15 16:25:11,260 DEV : loss 0.6747785806655884 - score 0.7667\n",
      "2020-05-15 16:25:11,407 BAD EPOCHS (no improvement): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DocumentRNNEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type StackedEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BytePairEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-15 16:25:12,230 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:25:12,253 epoch 2 - iter 0/13 - loss 0.79609919 - samples/sec: 389.72\n",
      "2020-05-15 16:25:12,274 epoch 2 - iter 1/13 - loss 0.55464861 - samples/sec: 517.61\n",
      "2020-05-15 16:25:12,302 epoch 2 - iter 2/13 - loss 0.55214993 - samples/sec: 376.95\n",
      "2020-05-15 16:25:12,324 epoch 2 - iter 3/13 - loss 0.60946737 - samples/sec: 500.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-15 16:25:12,352 epoch 2 - iter 4/13 - loss 0.64863864 - samples/sec: 510.80\n",
      "2020-05-15 16:25:12,374 epoch 2 - iter 5/13 - loss 0.65376938 - samples/sec: 508.08\n",
      "2020-05-15 16:25:12,401 epoch 2 - iter 6/13 - loss 0.62946188 - samples/sec: 441.76\n",
      "2020-05-15 16:25:12,422 epoch 2 - iter 7/13 - loss 0.67950963 - samples/sec: 516.87\n",
      "2020-05-15 16:25:12,449 epoch 2 - iter 8/13 - loss 0.68462497 - samples/sec: 386.93\n",
      "2020-05-15 16:25:12,474 epoch 2 - iter 9/13 - loss 0.66254631 - samples/sec: 429.56\n",
      "2020-05-15 16:25:12,498 epoch 2 - iter 10/13 - loss 0.64845571 - samples/sec: 526.15\n",
      "2020-05-15 16:25:12,527 epoch 2 - iter 11/13 - loss 0.67873405 - samples/sec: 482.43\n",
      "2020-05-15 16:25:12,548 epoch 2 - iter 12/13 - loss 0.65932834 - samples/sec: 797.81\n",
      "2020-05-15 16:25:12,554 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:25:12,555 EPOCH 2 done: loss 0.6593 - lr 0.1000\n",
      "2020-05-15 16:25:14,878 DEV : loss 0.6721192002296448 - score 0.7667\n",
      "2020-05-15 16:25:15,027 BAD EPOCHS (no improvement): 1\n",
      "2020-05-15 16:25:15,784 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:25:15,806 epoch 3 - iter 0/13 - loss 0.53267169 - samples/sec: 400.74\n",
      "2020-05-15 16:25:15,825 epoch 3 - iter 1/13 - loss 0.85968816 - samples/sec: 587.22\n",
      "2020-05-15 16:25:15,846 epoch 3 - iter 2/13 - loss 0.77620775 - samples/sec: 558.27\n",
      "2020-05-15 16:25:15,866 epoch 3 - iter 3/13 - loss 0.76624022 - samples/sec: 555.22\n",
      "2020-05-15 16:25:15,892 epoch 3 - iter 4/13 - loss 0.86744002 - samples/sec: 412.85\n",
      "2020-05-15 16:25:15,911 epoch 3 - iter 5/13 - loss 0.82896679 - samples/sec: 623.33\n",
      "2020-05-15 16:25:15,928 epoch 3 - iter 6/13 - loss 0.77084722 - samples/sec: 730.62\n",
      "2020-05-15 16:25:15,949 epoch 3 - iter 7/13 - loss 0.70112445 - samples/sec: 568.38\n",
      "2020-05-15 16:25:15,973 epoch 3 - iter 8/13 - loss 0.67165440 - samples/sec: 603.51\n",
      "2020-05-15 16:25:15,998 epoch 3 - iter 9/13 - loss 0.62158978 - samples/sec: 535.73\n",
      "2020-05-15 16:25:16,023 epoch 3 - iter 10/13 - loss 0.61747811 - samples/sec: 441.73\n",
      "2020-05-15 16:25:16,043 epoch 3 - iter 11/13 - loss 0.63835296 - samples/sec: 571.26\n",
      "2020-05-15 16:25:16,061 epoch 3 - iter 12/13 - loss 0.68846065 - samples/sec: 746.92\n",
      "2020-05-15 16:25:16,071 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:25:16,072 EPOCH 3 done: loss 0.6885 - lr 0.1000\n",
      "2020-05-15 16:25:18,668 DEV : loss 0.6957124471664429 - score 0.743\n",
      "2020-05-15 16:25:18,821 BAD EPOCHS (no improvement): 2\n",
      "2020-05-15 16:25:18,822 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:25:18,839 epoch 4 - iter 0/13 - loss 0.97844750 - samples/sec: 494.37\n",
      "2020-05-15 16:25:18,864 epoch 4 - iter 1/13 - loss 0.86479515 - samples/sec: 428.10\n",
      "2020-05-15 16:25:18,888 epoch 4 - iter 2/13 - loss 0.73892136 - samples/sec: 512.48\n",
      "2020-05-15 16:25:18,913 epoch 4 - iter 3/13 - loss 0.69661239 - samples/sec: 537.95\n",
      "2020-05-15 16:25:18,940 epoch 4 - iter 4/13 - loss 0.78334522 - samples/sec: 544.68\n",
      "2020-05-15 16:25:18,966 epoch 4 - iter 5/13 - loss 0.70365687 - samples/sec: 585.04\n",
      "2020-05-15 16:25:18,991 epoch 4 - iter 6/13 - loss 0.75685783 - samples/sec: 441.02\n",
      "2020-05-15 16:25:19,019 epoch 4 - iter 7/13 - loss 0.73793157 - samples/sec: 481.47\n",
      "2020-05-15 16:25:19,044 epoch 4 - iter 8/13 - loss 0.72566659 - samples/sec: 446.98\n",
      "2020-05-15 16:25:19,068 epoch 4 - iter 9/13 - loss 0.68477542 - samples/sec: 572.34\n",
      "2020-05-15 16:25:19,091 epoch 4 - iter 10/13 - loss 0.64616946 - samples/sec: 470.87\n",
      "2020-05-15 16:25:19,116 epoch 4 - iter 11/13 - loss 0.64183346 - samples/sec: 600.39\n",
      "2020-05-15 16:25:19,128 epoch 4 - iter 12/13 - loss 0.66923944 - samples/sec: 1170.57\n",
      "2020-05-15 16:25:19,134 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:25:19,135 EPOCH 4 done: loss 0.6692 - lr 0.1000\n",
      "2020-05-15 16:25:21,515 DEV : loss 0.6700388789176941 - score 0.767\n",
      "2020-05-15 16:25:21,661 BAD EPOCHS (no improvement): 0\n",
      "2020-05-15 16:25:22,407 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:25:22,433 epoch 5 - iter 0/13 - loss 1.04983640 - samples/sec: 337.49\n",
      "2020-05-15 16:25:22,460 epoch 5 - iter 1/13 - loss 0.97417200 - samples/sec: 473.65\n",
      "2020-05-15 16:25:22,488 epoch 5 - iter 2/13 - loss 0.85692390 - samples/sec: 523.55\n",
      "2020-05-15 16:25:22,513 epoch 5 - iter 3/13 - loss 0.77685529 - samples/sec: 537.22\n",
      "2020-05-15 16:25:22,541 epoch 5 - iter 4/13 - loss 0.73530545 - samples/sec: 471.87\n",
      "2020-05-15 16:25:22,568 epoch 5 - iter 5/13 - loss 0.72673794 - samples/sec: 527.93\n",
      "2020-05-15 16:25:22,595 epoch 5 - iter 6/13 - loss 0.67653382 - samples/sec: 535.52\n",
      "2020-05-15 16:25:22,620 epoch 5 - iter 7/13 - loss 0.72172465 - samples/sec: 427.78\n",
      "2020-05-15 16:25:22,641 epoch 5 - iter 8/13 - loss 0.70504430 - samples/sec: 534.06\n",
      "2020-05-15 16:25:22,663 epoch 5 - iter 9/13 - loss 0.66767510 - samples/sec: 507.89\n",
      "2020-05-15 16:25:22,688 epoch 5 - iter 10/13 - loss 0.62889591 - samples/sec: 421.16\n",
      "2020-05-15 16:25:22,707 epoch 5 - iter 11/13 - loss 0.62474822 - samples/sec: 627.28\n",
      "2020-05-15 16:25:22,724 epoch 5 - iter 12/13 - loss 0.58817434 - samples/sec: 770.34\n",
      "2020-05-15 16:25:22,730 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:25:22,730 EPOCH 5 done: loss 0.5882 - lr 0.1000\n",
      "2020-05-15 16:25:25,142 DEV : loss 0.6914488673210144 - score 0.7667\n",
      "2020-05-15 16:25:25,285 BAD EPOCHS (no improvement): 1\n",
      "2020-05-15 16:25:25,287 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:25:25,304 epoch 6 - iter 0/13 - loss 0.54630762 - samples/sec: 501.43\n",
      "2020-05-15 16:25:25,319 epoch 6 - iter 1/13 - loss 0.48960845 - samples/sec: 844.88\n",
      "2020-05-15 16:25:25,339 epoch 6 - iter 2/13 - loss 0.47889788 - samples/sec: 598.94\n",
      "2020-05-15 16:25:25,359 epoch 6 - iter 3/13 - loss 0.47933736 - samples/sec: 563.80\n",
      "2020-05-15 16:25:25,381 epoch 6 - iter 4/13 - loss 0.41816590 - samples/sec: 505.27\n",
      "2020-05-15 16:25:25,401 epoch 6 - iter 5/13 - loss 0.50180438 - samples/sec: 568.81\n",
      "2020-05-15 16:25:25,421 epoch 6 - iter 6/13 - loss 0.48816207 - samples/sec: 545.57\n",
      "2020-05-15 16:25:25,441 epoch 6 - iter 7/13 - loss 0.49635298 - samples/sec: 571.96\n",
      "2020-05-15 16:25:25,460 epoch 6 - iter 8/13 - loss 0.53588451 - samples/sec: 646.80\n",
      "2020-05-15 16:25:25,484 epoch 6 - iter 9/13 - loss 0.57304469 - samples/sec: 551.29\n",
      "2020-05-15 16:25:25,506 epoch 6 - iter 10/13 - loss 0.57871493 - samples/sec: 509.61\n",
      "2020-05-15 16:25:25,526 epoch 6 - iter 11/13 - loss 0.60199748 - samples/sec: 547.32\n",
      "2020-05-15 16:25:25,548 epoch 6 - iter 12/13 - loss 0.58175755 - samples/sec: 751.45\n",
      "2020-05-15 16:25:25,554 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:25:25,555 EPOCH 6 done: loss 0.5818 - lr 0.1000\n",
      "2020-05-15 16:25:27,880 DEV : loss 0.6642581224441528 - score 0.7667\n",
      "2020-05-15 16:25:28,028 BAD EPOCHS (no improvement): 2\n",
      "2020-05-15 16:25:28,030 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:25:28,048 epoch 7 - iter 0/13 - loss 0.42867601 - samples/sec: 460.55\n",
      "2020-05-15 16:25:28,072 epoch 7 - iter 1/13 - loss 0.63039762 - samples/sec: 565.80\n",
      "2020-05-15 16:25:28,093 epoch 7 - iter 2/13 - loss 0.54576490 - samples/sec: 557.06\n",
      "2020-05-15 16:25:28,114 epoch 7 - iter 3/13 - loss 0.56066786 - samples/sec: 513.49\n",
      "2020-05-15 16:25:28,139 epoch 7 - iter 4/13 - loss 0.54702908 - samples/sec: 430.01\n",
      "2020-05-15 16:25:28,163 epoch 7 - iter 5/13 - loss 0.52201677 - samples/sec: 607.20\n",
      "2020-05-15 16:25:28,184 epoch 7 - iter 6/13 - loss 0.54926364 - samples/sec: 540.90\n",
      "2020-05-15 16:25:28,208 epoch 7 - iter 7/13 - loss 0.61530122 - samples/sec: 507.25\n",
      "2020-05-15 16:25:28,233 epoch 7 - iter 8/13 - loss 0.60399455 - samples/sec: 539.70\n",
      "2020-05-15 16:25:28,263 epoch 7 - iter 9/13 - loss 0.62566307 - samples/sec: 430.58\n",
      "2020-05-15 16:25:28,288 epoch 7 - iter 10/13 - loss 0.60327458 - samples/sec: 553.29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-15 16:25:28,307 epoch 7 - iter 11/13 - loss 0.58229414 - samples/sec: 608.88\n",
      "2020-05-15 16:25:28,324 epoch 7 - iter 12/13 - loss 0.57068111 - samples/sec: 758.17\n",
      "2020-05-15 16:25:28,334 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:25:28,335 EPOCH 7 done: loss 0.5707 - lr 0.1000\n",
      "2020-05-15 16:25:31,000 DEV : loss 0.6939222812652588 - score 0.7663\n",
      "2020-05-15 16:25:31,149 BAD EPOCHS (no improvement): 3\n",
      "2020-05-15 16:25:31,150 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:25:31,167 epoch 8 - iter 0/13 - loss 0.29959819 - samples/sec: 524.85\n",
      "2020-05-15 16:25:31,194 epoch 8 - iter 1/13 - loss 0.52598937 - samples/sec: 502.93\n",
      "2020-05-15 16:25:31,220 epoch 8 - iter 2/13 - loss 0.44922308 - samples/sec: 540.19\n",
      "2020-05-15 16:25:31,244 epoch 8 - iter 3/13 - loss 0.57538338 - samples/sec: 456.77\n",
      "2020-05-15 16:25:31,271 epoch 8 - iter 4/13 - loss 0.50351978 - samples/sec: 397.19\n",
      "2020-05-15 16:25:31,292 epoch 8 - iter 5/13 - loss 0.61078922 - samples/sec: 512.85\n",
      "2020-05-15 16:25:31,318 epoch 8 - iter 6/13 - loss 0.67330824 - samples/sec: 542.67\n",
      "2020-05-15 16:25:31,343 epoch 8 - iter 7/13 - loss 0.69003096 - samples/sec: 426.65\n",
      "2020-05-15 16:25:31,367 epoch 8 - iter 8/13 - loss 0.64609972 - samples/sec: 571.65\n",
      "2020-05-15 16:25:31,392 epoch 8 - iter 9/13 - loss 0.62579209 - samples/sec: 616.76\n",
      "2020-05-15 16:25:31,412 epoch 8 - iter 10/13 - loss 0.60537499 - samples/sec: 577.36\n",
      "2020-05-15 16:25:31,437 epoch 8 - iter 11/13 - loss 0.61519859 - samples/sec: 545.61\n",
      "2020-05-15 16:25:31,460 epoch 8 - iter 12/13 - loss 0.62888580 - samples/sec: 675.55\n",
      "2020-05-15 16:25:31,466 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:25:31,467 EPOCH 8 done: loss 0.6289 - lr 0.1000\n",
      "2020-05-15 16:25:33,910 DEV : loss 0.6706230044364929 - score 0.7627\n",
      "2020-05-15 16:25:34,053 BAD EPOCHS (no improvement): 4\n",
      "2020-05-15 16:25:34,055 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:25:34,077 epoch 9 - iter 0/13 - loss 0.62228608 - samples/sec: 390.39\n",
      "2020-05-15 16:25:34,103 epoch 9 - iter 1/13 - loss 0.63440883 - samples/sec: 488.41\n",
      "2020-05-15 16:25:34,129 epoch 9 - iter 2/13 - loss 0.54925482 - samples/sec: 515.36\n",
      "2020-05-15 16:25:34,155 epoch 9 - iter 3/13 - loss 0.45027413 - samples/sec: 553.55\n",
      "2020-05-15 16:25:34,180 epoch 9 - iter 4/13 - loss 0.56781911 - samples/sec: 493.45\n",
      "2020-05-15 16:25:34,200 epoch 9 - iter 5/13 - loss 0.52392155 - samples/sec: 540.94\n",
      "2020-05-15 16:25:34,220 epoch 9 - iter 6/13 - loss 0.51070244 - samples/sec: 554.12\n",
      "2020-05-15 16:25:34,244 epoch 9 - iter 7/13 - loss 0.53352879 - samples/sec: 575.45\n",
      "2020-05-15 16:25:34,264 epoch 9 - iter 8/13 - loss 0.57030665 - samples/sec: 551.76\n",
      "2020-05-15 16:25:34,291 epoch 9 - iter 9/13 - loss 0.55075806 - samples/sec: 471.73\n",
      "2020-05-15 16:25:34,315 epoch 9 - iter 10/13 - loss 0.55195709 - samples/sec: 605.76\n",
      "2020-05-15 16:25:34,340 epoch 9 - iter 11/13 - loss 0.52665104 - samples/sec: 415.15\n",
      "2020-05-15 16:25:34,356 epoch 9 - iter 12/13 - loss 0.55853579 - samples/sec: 788.88\n",
      "2020-05-15 16:25:34,366 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:25:34,367 EPOCH 9 done: loss 0.5585 - lr 0.1000\n",
      "2020-05-15 16:25:36,761 DEV : loss 0.6743444800376892 - score 0.759\n",
      "2020-05-15 16:25:36,906 BAD EPOCHS (no improvement): 5\n",
      "2020-05-15 16:25:36,908 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:25:36,923 epoch 10 - iter 0/13 - loss 0.39714202 - samples/sec: 569.07\n",
      "2020-05-15 16:25:36,949 epoch 10 - iter 1/13 - loss 0.28643578 - samples/sec: 526.76\n",
      "2020-05-15 16:25:36,970 epoch 10 - iter 2/13 - loss 0.47105511 - samples/sec: 522.41\n",
      "2020-05-15 16:25:36,997 epoch 10 - iter 3/13 - loss 0.42656551 - samples/sec: 394.44\n",
      "2020-05-15 16:25:37,018 epoch 10 - iter 4/13 - loss 0.40569743 - samples/sec: 531.61\n",
      "2020-05-15 16:25:37,045 epoch 10 - iter 5/13 - loss 0.49322067 - samples/sec: 391.89\n",
      "2020-05-15 16:25:37,069 epoch 10 - iter 6/13 - loss 0.53750716 - samples/sec: 565.57\n",
      "2020-05-15 16:25:37,093 epoch 10 - iter 7/13 - loss 0.59240439 - samples/sec: 670.24\n",
      "2020-05-15 16:25:37,115 epoch 10 - iter 8/13 - loss 0.56653056 - samples/sec: 502.73\n",
      "2020-05-15 16:25:37,142 epoch 10 - iter 9/13 - loss 0.54935085 - samples/sec: 490.07\n",
      "2020-05-15 16:25:37,165 epoch 10 - iter 10/13 - loss 0.53278283 - samples/sec: 680.80\n",
      "2020-05-15 16:25:37,185 epoch 10 - iter 11/13 - loss 0.51643222 - samples/sec: 620.84\n",
      "2020-05-15 16:25:37,199 epoch 10 - iter 12/13 - loss 0.51253547 - samples/sec: 1038.26\n",
      "2020-05-15 16:25:37,205 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:25:37,206 EPOCH 10 done: loss 0.5125 - lr 0.1000\n",
      "2020-05-15 16:25:39,904 DEV : loss 0.6931204199790955 - score 0.7587\n",
      "Epoch     9: reducing learning rate of group 0 to 5.0000e-02.\n",
      "2020-05-15 16:25:40,049 BAD EPOCHS (no improvement): 6\n",
      "2020-05-15 16:25:40,051 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:25:40,069 epoch 11 - iter 0/13 - loss 0.30557990 - samples/sec: 494.82\n",
      "2020-05-15 16:25:40,096 epoch 11 - iter 1/13 - loss 0.46057507 - samples/sec: 544.56\n",
      "2020-05-15 16:25:40,123 epoch 11 - iter 2/13 - loss 0.36862336 - samples/sec: 449.52\n",
      "2020-05-15 16:25:40,147 epoch 11 - iter 3/13 - loss 0.35489819 - samples/sec: 498.68\n",
      "2020-05-15 16:25:40,175 epoch 11 - iter 4/13 - loss 0.43819998 - samples/sec: 469.82\n",
      "2020-05-15 16:25:40,200 epoch 11 - iter 5/13 - loss 0.39534032 - samples/sec: 548.77\n",
      "2020-05-15 16:25:40,226 epoch 11 - iter 6/13 - loss 0.39377932 - samples/sec: 550.43\n",
      "2020-05-15 16:25:40,245 epoch 11 - iter 7/13 - loss 0.42010049 - samples/sec: 600.48\n",
      "2020-05-15 16:25:40,267 epoch 11 - iter 8/13 - loss 0.43081020 - samples/sec: 517.38\n",
      "2020-05-15 16:25:40,295 epoch 11 - iter 9/13 - loss 0.46073932 - samples/sec: 467.01\n",
      "2020-05-15 16:25:40,316 epoch 11 - iter 10/13 - loss 0.49542483 - samples/sec: 567.43\n",
      "2020-05-15 16:25:40,342 epoch 11 - iter 11/13 - loss 0.48564302 - samples/sec: 550.38\n",
      "2020-05-15 16:25:40,362 epoch 11 - iter 12/13 - loss 0.48013343 - samples/sec: 766.94\n",
      "2020-05-15 16:25:40,368 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:25:40,369 EPOCH 11 done: loss 0.4801 - lr 0.0500\n",
      "2020-05-15 16:25:42,869 DEV : loss 0.7056064605712891 - score 0.7367\n",
      "2020-05-15 16:25:43,013 BAD EPOCHS (no improvement): 1\n",
      "2020-05-15 16:25:43,015 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:25:43,034 epoch 12 - iter 0/13 - loss 0.57121897 - samples/sec: 470.83\n",
      "2020-05-15 16:25:43,061 epoch 12 - iter 1/13 - loss 0.75188124 - samples/sec: 451.46\n",
      "2020-05-15 16:25:43,081 epoch 12 - iter 2/13 - loss 0.68885485 - samples/sec: 567.38\n",
      "2020-05-15 16:25:43,103 epoch 12 - iter 3/13 - loss 0.61250363 - samples/sec: 507.90\n",
      "2020-05-15 16:25:43,131 epoch 12 - iter 4/13 - loss 0.64720511 - samples/sec: 506.39\n",
      "2020-05-15 16:25:43,163 epoch 12 - iter 5/13 - loss 0.58978158 - samples/sec: 453.10\n",
      "2020-05-15 16:25:43,193 epoch 12 - iter 6/13 - loss 0.54986126 - samples/sec: 424.16\n",
      "2020-05-15 16:25:43,215 epoch 12 - iter 7/13 - loss 0.51420515 - samples/sec: 662.06\n",
      "2020-05-15 16:25:43,233 epoch 12 - iter 8/13 - loss 0.49963350 - samples/sec: 675.29\n",
      "2020-05-15 16:25:43,255 epoch 12 - iter 9/13 - loss 0.48016958 - samples/sec: 520.12\n",
      "2020-05-15 16:25:43,280 epoch 12 - iter 10/13 - loss 0.51160855 - samples/sec: 531.72\n",
      "2020-05-15 16:25:43,307 epoch 12 - iter 11/13 - loss 0.54207642 - samples/sec: 560.99\n",
      "2020-05-15 16:25:43,330 epoch 12 - iter 12/13 - loss 0.54533083 - samples/sec: 570.43\n",
      "2020-05-15 16:25:43,338 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:25:43,339 EPOCH 12 done: loss 0.5453 - lr 0.0500\n",
      "2020-05-15 16:25:45,904 DEV : loss 0.7049536108970642 - score 0.734\n",
      "2020-05-15 16:25:46,050 BAD EPOCHS (no improvement): 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-15 16:25:46,051 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:25:46,067 epoch 13 - iter 0/13 - loss 0.71879673 - samples/sec: 563.31\n",
      "2020-05-15 16:25:46,088 epoch 13 - iter 1/13 - loss 0.49519822 - samples/sec: 558.13\n",
      "2020-05-15 16:25:46,107 epoch 13 - iter 2/13 - loss 0.44308680 - samples/sec: 692.49\n",
      "2020-05-15 16:25:46,127 epoch 13 - iter 3/13 - loss 0.49287412 - samples/sec: 601.19\n",
      "2020-05-15 16:25:46,160 epoch 13 - iter 4/13 - loss 0.49923037 - samples/sec: 434.40\n",
      "2020-05-15 16:25:46,183 epoch 13 - iter 5/13 - loss 0.45436286 - samples/sec: 489.58\n",
      "2020-05-15 16:25:46,212 epoch 13 - iter 6/13 - loss 0.44483879 - samples/sec: 372.55\n",
      "2020-05-15 16:25:46,243 epoch 13 - iter 7/13 - loss 0.46565047 - samples/sec: 409.26\n",
      "2020-05-15 16:25:46,268 epoch 13 - iter 8/13 - loss 0.47630741 - samples/sec: 586.07\n",
      "2020-05-15 16:25:46,297 epoch 13 - iter 9/13 - loss 0.46855751 - samples/sec: 358.03\n",
      "2020-05-15 16:25:46,328 epoch 13 - iter 10/13 - loss 0.49200712 - samples/sec: 341.99\n",
      "2020-05-15 16:25:46,352 epoch 13 - iter 11/13 - loss 0.47744591 - samples/sec: 572.91\n",
      "2020-05-15 16:25:46,367 epoch 13 - iter 12/13 - loss 0.50546392 - samples/sec: 937.46\n",
      "2020-05-15 16:25:46,372 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:25:46,373 EPOCH 13 done: loss 0.5055 - lr 0.0500\n",
      "2020-05-15 16:25:48,956 DEV : loss 0.7109909653663635 - score 0.7353\n",
      "2020-05-15 16:25:49,106 BAD EPOCHS (no improvement): 3\n",
      "2020-05-15 16:25:49,107 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:25:49,128 epoch 14 - iter 0/13 - loss 0.61157632 - samples/sec: 387.27\n",
      "2020-05-15 16:25:49,151 epoch 14 - iter 1/13 - loss 0.67353216 - samples/sec: 742.31\n",
      "2020-05-15 16:25:49,172 epoch 14 - iter 2/13 - loss 0.51756122 - samples/sec: 514.23\n",
      "2020-05-15 16:25:49,191 epoch 14 - iter 3/13 - loss 0.43876446 - samples/sec: 616.82\n",
      "2020-05-15 16:25:49,209 epoch 14 - iter 4/13 - loss 0.44821883 - samples/sec: 663.71\n",
      "2020-05-15 16:25:49,228 epoch 14 - iter 5/13 - loss 0.46031190 - samples/sec: 653.23\n",
      "2020-05-15 16:25:49,247 epoch 14 - iter 6/13 - loss 0.45704672 - samples/sec: 587.99\n",
      "2020-05-15 16:25:49,267 epoch 14 - iter 7/13 - loss 0.47652740 - samples/sec: 595.40\n",
      "2020-05-15 16:25:49,285 epoch 14 - iter 8/13 - loss 0.46688981 - samples/sec: 628.47\n",
      "2020-05-15 16:25:49,305 epoch 14 - iter 9/13 - loss 0.47934597 - samples/sec: 561.78\n",
      "2020-05-15 16:25:49,323 epoch 14 - iter 10/13 - loss 0.46352417 - samples/sec: 635.01\n",
      "2020-05-15 16:25:49,344 epoch 14 - iter 11/13 - loss 0.45268111 - samples/sec: 546.90\n",
      "2020-05-15 16:25:49,360 epoch 14 - iter 12/13 - loss 0.47171129 - samples/sec: 915.14\n",
      "2020-05-15 16:25:49,366 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:25:49,367 EPOCH 14 done: loss 0.4717 - lr 0.0500\n",
      "2020-05-15 16:25:51,769 DEV : loss 0.7124412655830383 - score 0.728\n",
      "2020-05-15 16:25:51,913 BAD EPOCHS (no improvement): 4\n",
      "2020-05-15 16:25:51,915 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:25:51,937 epoch 15 - iter 0/13 - loss 0.14475547 - samples/sec: 388.07\n",
      "2020-05-15 16:25:51,961 epoch 15 - iter 1/13 - loss 0.29792032 - samples/sec: 559.83\n",
      "2020-05-15 16:25:51,986 epoch 15 - iter 2/13 - loss 0.48724805 - samples/sec: 555.31\n",
      "2020-05-15 16:25:52,013 epoch 15 - iter 3/13 - loss 0.46888598 - samples/sec: 370.57\n",
      "2020-05-15 16:25:52,039 epoch 15 - iter 4/13 - loss 0.44508919 - samples/sec: 506.37\n",
      "2020-05-15 16:25:52,068 epoch 15 - iter 5/13 - loss 0.47355609 - samples/sec: 439.23\n",
      "2020-05-15 16:25:52,095 epoch 15 - iter 6/13 - loss 0.51674700 - samples/sec: 485.41\n",
      "2020-05-15 16:25:52,121 epoch 15 - iter 7/13 - loss 0.50200129 - samples/sec: 530.24\n",
      "2020-05-15 16:25:52,149 epoch 15 - iter 8/13 - loss 0.48819876 - samples/sec: 507.70\n",
      "2020-05-15 16:25:52,171 epoch 15 - iter 9/13 - loss 0.48392640 - samples/sec: 506.13\n",
      "2020-05-15 16:25:52,196 epoch 15 - iter 10/13 - loss 0.48824972 - samples/sec: 498.28\n",
      "2020-05-15 16:25:52,228 epoch 15 - iter 11/13 - loss 0.47301624 - samples/sec: 377.77\n",
      "2020-05-15 16:25:52,250 epoch 15 - iter 12/13 - loss 0.53545884 - samples/sec: 720.56\n",
      "2020-05-15 16:25:52,256 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:25:52,256 EPOCH 15 done: loss 0.5355 - lr 0.0500\n",
      "2020-05-15 16:25:54,877 DEV : loss 0.7070891857147217 - score 0.7293\n",
      "2020-05-15 16:25:55,020 BAD EPOCHS (no improvement): 5\n",
      "2020-05-15 16:25:55,804 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:25:55,805 Testing using best model ...\n",
      "2020-05-15 16:25:55,807 loading file best-model.pt\n",
      "2020-05-15 16:26:00,992 0.76\t0.76\t0.76\n",
      "2020-05-15 16:26:00,993 \n",
      "MICRO_AVG: acc 0.6129 - f1-score 0.76\n",
      "MACRO_AVG: acc 0.2539 - f1-score 0.2890666666666667\n",
      "0          tp: 0 - fp: 0 - fn: 173 - tn: 2827 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000\n",
      "1          tp: 2279 - fp: 720 - fn: 0 - tn: 1 - precision: 0.7599 - recall: 1.0000 - accuracy: 0.7599 - f1-score: 0.8636\n",
      "2          tp: 1 - fp: 0 - fn: 547 - tn: 2452 - precision: 1.0000 - recall: 0.0018 - accuracy: 0.0018 - f1-score: 0.0036\n",
      "2020-05-15 16:26:00,994 ----------------------------------------------------------------------------------------------------\n",
      "405.2626566886902\n"
     ]
    }
   ],
   "source": [
    "# BERT OOMs on 32 batch, use 8\n",
    "total_time = time.time()\n",
    "\n",
    "# Bert Cased - separating lower and upper case, uncased - ignoring case.\n",
    "# Use cased\n",
    "\n",
    "word_embeddings = [ BertEmbeddings('bert-base-cased'),                ]\n",
    "modelname = 'bert-base-cased'\n",
    "train_and_predict(word_embeddings, modelname, modeldesc, savelist=savelist, epochs=EPOCHS, batch_size=8)\n",
    "\n",
    "#word_embeddings = [ BertEmbeddings('bert-base-uncased'),                ]\n",
    "#modelname = 'bert-base-uncased'\n",
    "#train_and_predict(word_embeddings, modelname, modeldesc, savelist=savelist, epochs=EPOCHS, batch_size=8)\n",
    "\n",
    "#BPE - takes memory - reduce batch size radically!\n",
    "word_embeddings = [ BytePairEmbeddings('en'),   ]\n",
    "modelname = 'BytePairEmbedding'\n",
    "train_and_predict(word_embeddings, modelname, modeldesc, savelist=savelist, epochs=EPOCHS, batch_size=8)               \n",
    "\n",
    "print(time.time() - total_time)\n",
    "\n",
    "# 450 s 18k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-1\n",
      "2020-05-15 16:26:16,286 Reading data from /home/max/git/newcombined/dataset_hatespeech/input\n",
      "2020-05-15 16:26:16,286 Train: /home/max/git/newcombined/dataset_hatespeech/input/flair_train.csv\n",
      "2020-05-15 16:26:16,287 Dev: /home/max/git/newcombined/dataset_hatespeech/input/flair_dev.csv\n",
      "2020-05-15 16:26:16,287 Test: /home/max/git/newcombined/dataset_hatespeech/input/flair_test.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/ipykernel_launcher.py:24: DeprecationWarning: Call to deprecated function (or staticmethod) load_classification_corpus. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/flair/data_fetcher.py:452: DeprecationWarning: Call to deprecated function (or staticmethod) read_text_classification_file. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  train_file, tokenizer=tokenizer, max_tokens_per_doc=max_tokens_per_doc\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/flair/data_fetcher.py:457: DeprecationWarning: Call to deprecated function (or staticmethod) read_text_classification_file. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  test_file, tokenizer=tokenizer, max_tokens_per_doc=max_tokens_per_doc\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/flair/data_fetcher.py:464: DeprecationWarning: Call to deprecated function (or staticmethod) read_text_classification_file. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  dev_file, tokenizer=tokenizer, max_tokens_per_doc=max_tokens_per_doc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-15 16:26:17,748 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 205200.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-15 16:26:17,751 [b'1', b'2', b'0']\n",
      "2020-05-15 16:26:17,752 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 100/100 [00:00<00:00, 259870.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-15 16:26:17,754 [b'1', b'2', b'0']\n",
      "2020-05-15 16:26:17,762 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:26:17,764 Model: \"TextClassifier(\n",
      "  (document_embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): OpenAIGPTEmbeddings(\n",
      "        model=0-openai-gpt\n",
      "        (model): OpenAIGPTModel(\n",
      "          (tokens_embed): Embedding(40478, 768)\n",
      "          (positions_embed): Embedding(512, 768)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "          (h): ModuleList(\n",
      "            (0): Block(\n",
      "              (attn): Attention(\n",
      "                (c_attn): Conv1D()\n",
      "                (c_proj): Conv1D()\n",
      "                (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "                (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): MLP(\n",
      "                (c_fc): Conv1D()\n",
      "                (c_proj): Conv1D()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (1): Block(\n",
      "              (attn): Attention(\n",
      "                (c_attn): Conv1D()\n",
      "                (c_proj): Conv1D()\n",
      "                (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "                (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): MLP(\n",
      "                (c_fc): Conv1D()\n",
      "                (c_proj): Conv1D()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (2): Block(\n",
      "              (attn): Attention(\n",
      "                (c_attn): Conv1D()\n",
      "                (c_proj): Conv1D()\n",
      "                (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "                (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): MLP(\n",
      "                (c_fc): Conv1D()\n",
      "                (c_proj): Conv1D()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (3): Block(\n",
      "              (attn): Attention(\n",
      "                (c_attn): Conv1D()\n",
      "                (c_proj): Conv1D()\n",
      "                (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "                (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): MLP(\n",
      "                (c_fc): Conv1D()\n",
      "                (c_proj): Conv1D()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (4): Block(\n",
      "              (attn): Attention(\n",
      "                (c_attn): Conv1D()\n",
      "                (c_proj): Conv1D()\n",
      "                (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "                (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): MLP(\n",
      "                (c_fc): Conv1D()\n",
      "                (c_proj): Conv1D()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (5): Block(\n",
      "              (attn): Attention(\n",
      "                (c_attn): Conv1D()\n",
      "                (c_proj): Conv1D()\n",
      "                (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "                (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): MLP(\n",
      "                (c_fc): Conv1D()\n",
      "                (c_proj): Conv1D()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (6): Block(\n",
      "              (attn): Attention(\n",
      "                (c_attn): Conv1D()\n",
      "                (c_proj): Conv1D()\n",
      "                (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "                (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): MLP(\n",
      "                (c_fc): Conv1D()\n",
      "                (c_proj): Conv1D()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (7): Block(\n",
      "              (attn): Attention(\n",
      "                (c_attn): Conv1D()\n",
      "                (c_proj): Conv1D()\n",
      "                (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "                (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): MLP(\n",
      "                (c_fc): Conv1D()\n",
      "                (c_proj): Conv1D()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (8): Block(\n",
      "              (attn): Attention(\n",
      "                (c_attn): Conv1D()\n",
      "                (c_proj): Conv1D()\n",
      "                (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "                (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): MLP(\n",
      "                (c_fc): Conv1D()\n",
      "                (c_proj): Conv1D()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (9): Block(\n",
      "              (attn): Attention(\n",
      "                (c_attn): Conv1D()\n",
      "                (c_proj): Conv1D()\n",
      "                (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "                (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): MLP(\n",
      "                (c_fc): Conv1D()\n",
      "                (c_proj): Conv1D()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (10): Block(\n",
      "              (attn): Attention(\n",
      "                (c_attn): Conv1D()\n",
      "                (c_proj): Conv1D()\n",
      "                (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "                (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): MLP(\n",
      "                (c_fc): Conv1D()\n",
      "                (c_proj): Conv1D()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (11): Block(\n",
      "              (attn): Attention(\n",
      "                (c_attn): Conv1D()\n",
      "                (c_proj): Conv1D()\n",
      "                (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "                (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): MLP(\n",
      "                (c_fc): Conv1D()\n",
      "                (c_proj): Conv1D()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (rnn): LSTM(256, 512, batch_first=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=512, out_features=3, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\"\n",
      "2020-05-15 16:26:17,765 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-15 16:26:17,765 Corpus: \"Corpus: 100 train + 3000 dev + 3000 test sentences\"\n",
      "2020-05-15 16:26:17,766 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:26:17,766 Parameters:\n",
      "2020-05-15 16:26:17,766  - learning_rate: \"0.1\"\n",
      "2020-05-15 16:26:17,767  - mini_batch_size: \"8\"\n",
      "2020-05-15 16:26:17,767  - patience: \"5\"\n",
      "2020-05-15 16:26:17,767  - anneal_factor: \"0.5\"\n",
      "2020-05-15 16:26:17,768  - max_epochs: \"15\"\n",
      "2020-05-15 16:26:17,768  - shuffle: \"True\"\n",
      "2020-05-15 16:26:17,768  - train_with_dev: \"False\"\n",
      "2020-05-15 16:26:17,769  - batch_growth_annealing: \"False\"\n",
      "2020-05-15 16:26:17,769 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:26:17,769 Model training base path: \".\"\n",
      "2020-05-15 16:26:17,770 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:26:17,770 Device: cuda:0\n",
      "2020-05-15 16:26:17,771 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:26:17,772 Embeddings storage mode: cpu\n",
      "2020-05-15 16:26:17,778 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:26:17,939 epoch 1 - iter 0/13 - loss 1.12738824 - samples/sec: 50.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-15 16:26:18,228 epoch 1 - iter 1/13 - loss 1.02366441 - samples/sec: 46.87\n",
      "2020-05-15 16:26:18,507 epoch 1 - iter 2/13 - loss 0.89995372 - samples/sec: 44.56\n",
      "2020-05-15 16:26:18,777 epoch 1 - iter 3/13 - loss 0.89521576 - samples/sec: 49.18\n",
      "2020-05-15 16:26:19,048 epoch 1 - iter 4/13 - loss 0.90249537 - samples/sec: 48.36\n",
      "2020-05-15 16:26:19,305 epoch 1 - iter 5/13 - loss 0.86521447 - samples/sec: 50.51\n",
      "2020-05-15 16:26:19,589 epoch 1 - iter 6/13 - loss 0.80853843 - samples/sec: 43.72\n",
      "2020-05-15 16:26:19,881 epoch 1 - iter 7/13 - loss 0.76403137 - samples/sec: 44.48\n",
      "2020-05-15 16:26:20,148 epoch 1 - iter 8/13 - loss 0.76629166 - samples/sec: 48.27\n",
      "2020-05-15 16:26:20,410 epoch 1 - iter 9/13 - loss 0.77882037 - samples/sec: 49.42\n",
      "2020-05-15 16:26:20,675 epoch 1 - iter 10/13 - loss 0.79162548 - samples/sec: 49.06\n",
      "2020-05-15 16:26:20,932 epoch 1 - iter 11/13 - loss 0.75281586 - samples/sec: 50.92\n",
      "2020-05-15 16:26:21,111 epoch 1 - iter 12/13 - loss 0.76204223 - samples/sec: 106.16\n",
      "2020-05-15 16:26:21,217 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:26:21,218 EPOCH 1 done: loss 0.7620 - lr 0.1000\n",
      "2020-05-15 16:27:21,686 DEV : loss 0.6576547622680664 - score 0.7667\n",
      "2020-05-15 16:27:21,826 BAD EPOCHS (no improvement): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DocumentRNNEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type StackedEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type OpenAIGPTEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type OpenAIGPTModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ModuleList. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Block. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Attention. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Conv1D. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LayerNorm. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type MLP. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/max/anaconda3/envs/tf-hub/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-15 16:27:23,673 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:27:23,696 epoch 2 - iter 0/13 - loss 0.63897491 - samples/sec: 430.39\n",
      "2020-05-15 16:27:23,818 epoch 2 - iter 1/13 - loss 0.45528595 - samples/sec: 486.47\n",
      "2020-05-15 16:27:23,939 epoch 2 - iter 2/13 - loss 0.49192606 - samples/sec: 506.12\n",
      "2020-05-15 16:27:24,061 epoch 2 - iter 3/13 - loss 0.53844932 - samples/sec: 460.98\n",
      "2020-05-15 16:27:24,183 epoch 2 - iter 4/13 - loss 0.56347318 - samples/sec: 528.67\n",
      "2020-05-15 16:27:24,303 epoch 2 - iter 5/13 - loss 0.58326705 - samples/sec: 600.20\n",
      "2020-05-15 16:27:24,431 epoch 2 - iter 6/13 - loss 0.57723636 - samples/sec: 438.36\n",
      "2020-05-15 16:27:24,552 epoch 2 - iter 7/13 - loss 0.60624900 - samples/sec: 466.90\n",
      "2020-05-15 16:27:24,681 epoch 2 - iter 8/13 - loss 0.61534938 - samples/sec: 406.37\n",
      "2020-05-15 16:27:24,807 epoch 2 - iter 9/13 - loss 0.59594664 - samples/sec: 554.91\n",
      "2020-05-15 16:27:24,928 epoch 2 - iter 10/13 - loss 0.58707226 - samples/sec: 578.58\n",
      "2020-05-15 16:27:25,049 epoch 2 - iter 11/13 - loss 0.63294789 - samples/sec: 446.26\n",
      "2020-05-15 16:27:25,163 epoch 2 - iter 12/13 - loss 0.62733072 - samples/sec: 675.26\n",
      "2020-05-15 16:27:25,272 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:27:25,273 EPOCH 2 done: loss 0.6273 - lr 0.1000\n",
      "2020-05-15 16:27:27,830 DEV : loss 0.6597703099250793 - score 0.7667\n",
      "2020-05-15 16:27:27,966 BAD EPOCHS (no improvement): 1\n",
      "2020-05-15 16:27:29,721 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:27:29,745 epoch 3 - iter 0/13 - loss 0.40267268 - samples/sec: 389.85\n",
      "2020-05-15 16:27:29,873 epoch 3 - iter 1/13 - loss 0.66146819 - samples/sec: 620.77\n",
      "2020-05-15 16:27:30,016 epoch 3 - iter 2/13 - loss 0.60826742 - samples/sec: 517.06\n",
      "2020-05-15 16:27:30,143 epoch 3 - iter 3/13 - loss 0.58433741 - samples/sec: 407.47\n",
      "2020-05-15 16:27:30,269 epoch 3 - iter 4/13 - loss 0.67854652 - samples/sec: 445.74\n",
      "2020-05-15 16:27:30,391 epoch 3 - iter 5/13 - loss 0.66649247 - samples/sec: 543.02\n",
      "2020-05-15 16:27:30,517 epoch 3 - iter 6/13 - loss 0.63696065 - samples/sec: 411.59\n",
      "2020-05-15 16:27:30,637 epoch 3 - iter 7/13 - loss 0.56422279 - samples/sec: 471.03\n",
      "2020-05-15 16:27:30,762 epoch 3 - iter 8/13 - loss 0.53549693 - samples/sec: 494.80\n",
      "2020-05-15 16:27:30,879 epoch 3 - iter 9/13 - loss 0.49300598 - samples/sec: 458.73\n",
      "2020-05-15 16:27:31,006 epoch 3 - iter 10/13 - loss 0.48788696 - samples/sec: 407.16\n",
      "2020-05-15 16:27:31,130 epoch 3 - iter 11/13 - loss 0.52817674 - samples/sec: 455.65\n",
      "2020-05-15 16:27:31,254 epoch 3 - iter 12/13 - loss 0.55860584 - samples/sec: 670.14\n",
      "2020-05-15 16:27:31,363 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:27:31,364 EPOCH 3 done: loss 0.5586 - lr 0.1000\n",
      "2020-05-15 16:27:33,946 DEV : loss 0.7682072520256042 - score 0.6377\n",
      "2020-05-15 16:27:34,084 BAD EPOCHS (no improvement): 2\n",
      "2020-05-15 16:27:34,085 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:27:34,100 epoch 4 - iter 0/13 - loss 0.68470240 - samples/sec: 595.38\n",
      "2020-05-15 16:27:34,214 epoch 4 - iter 1/13 - loss 0.62028813 - samples/sec: 572.70\n",
      "2020-05-15 16:27:34,334 epoch 4 - iter 2/13 - loss 0.48098435 - samples/sec: 437.75\n",
      "2020-05-15 16:27:34,450 epoch 4 - iter 3/13 - loss 0.52272728 - samples/sec: 443.10\n",
      "2020-05-15 16:27:34,565 epoch 4 - iter 4/13 - loss 0.61715641 - samples/sec: 495.53\n",
      "2020-05-15 16:27:34,681 epoch 4 - iter 5/13 - loss 0.60014879 - samples/sec: 486.52\n",
      "2020-05-15 16:27:34,804 epoch 4 - iter 6/13 - loss 0.65474151 - samples/sec: 648.95\n",
      "2020-05-15 16:27:34,928 epoch 4 - iter 7/13 - loss 0.62661882 - samples/sec: 460.36\n",
      "2020-05-15 16:27:35,054 epoch 4 - iter 8/13 - loss 0.60881743 - samples/sec: 426.54\n",
      "2020-05-15 16:27:35,175 epoch 4 - iter 9/13 - loss 0.56408763 - samples/sec: 616.56\n",
      "2020-05-15 16:27:35,294 epoch 4 - iter 10/13 - loss 0.52743685 - samples/sec: 503.70\n",
      "2020-05-15 16:27:35,414 epoch 4 - iter 11/13 - loss 0.50871734 - samples/sec: 461.74\n",
      "2020-05-15 16:27:35,525 epoch 4 - iter 12/13 - loss 0.51757026 - samples/sec: 766.80\n",
      "2020-05-15 16:27:35,632 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:27:35,633 EPOCH 4 done: loss 0.5176 - lr 0.1000\n",
      "2020-05-15 16:27:38,679 DEV : loss 0.6313880085945129 - score 0.7637\n",
      "2020-05-15 16:27:38,830 BAD EPOCHS (no improvement): 3\n",
      "2020-05-15 16:27:38,831 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:27:38,856 epoch 5 - iter 0/13 - loss 0.41587391 - samples/sec: 357.16\n",
      "2020-05-15 16:27:38,996 epoch 5 - iter 1/13 - loss 0.48955004 - samples/sec: 552.79\n",
      "2020-05-15 16:27:39,127 epoch 5 - iter 2/13 - loss 0.52096497 - samples/sec: 424.43\n",
      "2020-05-15 16:27:39,252 epoch 5 - iter 3/13 - loss 0.50459477 - samples/sec: 471.02\n",
      "2020-05-15 16:27:39,379 epoch 5 - iter 4/13 - loss 0.47166455 - samples/sec: 363.70\n",
      "2020-05-15 16:27:39,512 epoch 5 - iter 5/13 - loss 0.44438889 - samples/sec: 444.62\n",
      "2020-05-15 16:27:39,642 epoch 5 - iter 6/13 - loss 0.40466242 - samples/sec: 420.22\n",
      "2020-05-15 16:27:39,762 epoch 5 - iter 7/13 - loss 0.46185189 - samples/sec: 473.91\n",
      "2020-05-15 16:27:39,884 epoch 5 - iter 8/13 - loss 0.44167561 - samples/sec: 573.72\n",
      "2020-05-15 16:27:40,012 epoch 5 - iter 9/13 - loss 0.41767750 - samples/sec: 490.04\n",
      "2020-05-15 16:27:40,133 epoch 5 - iter 10/13 - loss 0.39029374 - samples/sec: 595.15\n",
      "2020-05-15 16:27:40,245 epoch 5 - iter 11/13 - loss 0.39835873 - samples/sec: 698.08\n",
      "2020-05-15 16:27:40,356 epoch 5 - iter 12/13 - loss 0.37505284 - samples/sec: 714.08\n",
      "2020-05-15 16:27:40,460 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:27:40,461 EPOCH 5 done: loss 0.3751 - lr 0.1000\n",
      "2020-05-15 16:27:43,327 DEV : loss 0.7168483734130859 - score 0.7697\n",
      "2020-05-15 16:27:43,466 BAD EPOCHS (no improvement): 0\n",
      "2020-05-15 16:27:45,277 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:27:45,304 epoch 6 - iter 0/13 - loss 0.17771783 - samples/sec: 354.92\n",
      "2020-05-15 16:27:45,427 epoch 6 - iter 1/13 - loss 0.15160960 - samples/sec: 516.21\n",
      "2020-05-15 16:27:45,557 epoch 6 - iter 2/13 - loss 0.17728381 - samples/sec: 465.76\n",
      "2020-05-15 16:27:45,677 epoch 6 - iter 3/13 - loss 0.19110517 - samples/sec: 511.73\n",
      "2020-05-15 16:27:45,794 epoch 6 - iter 4/13 - loss 0.18596330 - samples/sec: 422.18\n",
      "2020-05-15 16:27:45,914 epoch 6 - iter 5/13 - loss 0.32966944 - samples/sec: 481.05\n",
      "2020-05-15 16:27:46,036 epoch 6 - iter 6/13 - loss 0.32233788 - samples/sec: 550.23\n",
      "2020-05-15 16:27:46,150 epoch 6 - iter 7/13 - loss 0.32769039 - samples/sec: 505.36\n",
      "2020-05-15 16:27:46,265 epoch 6 - iter 8/13 - loss 0.32364667 - samples/sec: 497.07\n",
      "2020-05-15 16:27:46,388 epoch 6 - iter 9/13 - loss 0.37022237 - samples/sec: 385.59\n",
      "2020-05-15 16:27:46,512 epoch 6 - iter 10/13 - loss 0.36075131 - samples/sec: 372.25\n",
      "2020-05-15 16:27:46,634 epoch 6 - iter 11/13 - loss 0.37859253 - samples/sec: 399.11\n",
      "2020-05-15 16:27:46,760 epoch 6 - iter 12/13 - loss 0.35910875 - samples/sec: 837.75\n",
      "2020-05-15 16:27:46,873 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:27:46,874 EPOCH 6 done: loss 0.3591 - lr 0.1000\n",
      "2020-05-15 16:27:49,428 DEV : loss 0.6333287954330444 - score 0.7627\n",
      "2020-05-15 16:27:49,564 BAD EPOCHS (no improvement): 1\n",
      "2020-05-15 16:27:49,566 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:27:49,585 epoch 7 - iter 0/13 - loss 0.13758412 - samples/sec: 459.30\n",
      "2020-05-15 16:27:49,706 epoch 7 - iter 1/13 - loss 0.27715811 - samples/sec: 502.83\n",
      "2020-05-15 16:27:49,835 epoch 7 - iter 2/13 - loss 0.23519403 - samples/sec: 513.62\n",
      "2020-05-15 16:27:49,966 epoch 7 - iter 3/13 - loss 0.24764390 - samples/sec: 442.53\n",
      "2020-05-15 16:27:50,109 epoch 7 - iter 4/13 - loss 0.24027649 - samples/sec: 418.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-15 16:27:50,242 epoch 7 - iter 5/13 - loss 0.21901545 - samples/sec: 552.09\n",
      "2020-05-15 16:27:50,364 epoch 7 - iter 6/13 - loss 0.23229106 - samples/sec: 477.70\n",
      "2020-05-15 16:27:50,484 epoch 7 - iter 7/13 - loss 0.26107138 - samples/sec: 574.04\n",
      "2020-05-15 16:27:50,604 epoch 7 - iter 8/13 - loss 0.26079476 - samples/sec: 486.37\n",
      "2020-05-15 16:27:50,728 epoch 7 - iter 9/13 - loss 0.26099357 - samples/sec: 429.84\n",
      "2020-05-15 16:27:50,849 epoch 7 - iter 10/13 - loss 0.24235101 - samples/sec: 491.15\n",
      "2020-05-15 16:27:50,967 epoch 7 - iter 11/13 - loss 0.27669199 - samples/sec: 563.88\n",
      "2020-05-15 16:27:51,107 epoch 7 - iter 12/13 - loss 0.27300339 - samples/sec: 554.26\n",
      "2020-05-15 16:27:51,266 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:27:51,267 EPOCH 7 done: loss 0.2730 - lr 0.1000\n",
      "2020-05-15 16:27:54,252 DEV : loss 0.688732922077179 - score 0.7683\n",
      "2020-05-15 16:27:54,388 BAD EPOCHS (no improvement): 2\n",
      "2020-05-15 16:27:54,390 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:27:54,406 epoch 8 - iter 0/13 - loss 0.13300958 - samples/sec: 543.54\n",
      "2020-05-15 16:27:54,534 epoch 8 - iter 1/13 - loss 0.15040120 - samples/sec: 461.92\n",
      "2020-05-15 16:27:54,660 epoch 8 - iter 2/13 - loss 0.13173132 - samples/sec: 469.27\n",
      "2020-05-15 16:27:54,787 epoch 8 - iter 3/13 - loss 0.23788043 - samples/sec: 390.27\n",
      "2020-05-15 16:27:54,948 epoch 8 - iter 4/13 - loss 0.22495487 - samples/sec: 339.16\n",
      "2020-05-15 16:27:55,078 epoch 8 - iter 5/13 - loss 0.25488383 - samples/sec: 573.15\n",
      "2020-05-15 16:27:55,195 epoch 8 - iter 6/13 - loss 0.34697693 - samples/sec: 504.35\n",
      "2020-05-15 16:27:55,313 epoch 8 - iter 7/13 - loss 0.31471035 - samples/sec: 508.33\n",
      "2020-05-15 16:27:55,426 epoch 8 - iter 8/13 - loss 0.28950931 - samples/sec: 444.95\n",
      "2020-05-15 16:27:55,539 epoch 8 - iter 9/13 - loss 0.28001442 - samples/sec: 607.00\n",
      "2020-05-15 16:27:55,657 epoch 8 - iter 10/13 - loss 0.26386733 - samples/sec: 500.42\n",
      "2020-05-15 16:27:55,770 epoch 8 - iter 11/13 - loss 0.29616912 - samples/sec: 445.70\n",
      "2020-05-15 16:27:55,880 epoch 8 - iter 12/13 - loss 0.30999578 - samples/sec: 620.62\n",
      "2020-05-15 16:27:55,978 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:27:55,979 EPOCH 8 done: loss 0.3100 - lr 0.1000\n",
      "2020-05-15 16:27:58,469 DEV : loss 0.8103445768356323 - score 0.6663\n",
      "2020-05-15 16:27:58,608 BAD EPOCHS (no improvement): 3\n",
      "2020-05-15 16:27:58,609 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:27:58,630 epoch 9 - iter 0/13 - loss 0.07160085 - samples/sec: 428.09\n",
      "2020-05-15 16:27:58,748 epoch 9 - iter 1/13 - loss 0.24609715 - samples/sec: 425.26\n",
      "2020-05-15 16:27:58,869 epoch 9 - iter 2/13 - loss 0.18518945 - samples/sec: 458.37\n",
      "2020-05-15 16:27:58,989 epoch 9 - iter 3/13 - loss 0.15778314 - samples/sec: 384.63\n",
      "2020-05-15 16:27:59,108 epoch 9 - iter 4/13 - loss 0.26842290 - samples/sec: 558.45\n",
      "2020-05-15 16:27:59,226 epoch 9 - iter 5/13 - loss 0.24386748 - samples/sec: 586.25\n",
      "2020-05-15 16:27:59,346 epoch 9 - iter 6/13 - loss 0.24578442 - samples/sec: 503.50\n",
      "2020-05-15 16:27:59,465 epoch 9 - iter 7/13 - loss 0.27698942 - samples/sec: 479.08\n",
      "2020-05-15 16:27:59,594 epoch 9 - iter 8/13 - loss 0.27071964 - samples/sec: 504.68\n",
      "2020-05-15 16:27:59,716 epoch 9 - iter 9/13 - loss 0.26205895 - samples/sec: 505.83\n",
      "2020-05-15 16:27:59,837 epoch 9 - iter 10/13 - loss 0.24285139 - samples/sec: 475.75\n",
      "2020-05-15 16:27:59,967 epoch 9 - iter 11/13 - loss 0.22772763 - samples/sec: 365.67\n",
      "2020-05-15 16:28:00,092 epoch 9 - iter 12/13 - loss 0.29948680 - samples/sec: 485.09\n",
      "2020-05-15 16:28:00,198 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:28:00,199 EPOCH 9 done: loss 0.2995 - lr 0.1000\n",
      "2020-05-15 16:28:02,882 DEV : loss 0.7601872682571411 - score 0.7\n",
      "2020-05-15 16:28:03,023 BAD EPOCHS (no improvement): 4\n",
      "2020-05-15 16:28:03,024 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:28:03,044 epoch 10 - iter 0/13 - loss 0.34861907 - samples/sec: 461.81\n",
      "2020-05-15 16:28:03,168 epoch 10 - iter 1/13 - loss 0.21582023 - samples/sec: 469.19\n",
      "2020-05-15 16:28:03,309 epoch 10 - iter 2/13 - loss 0.16962696 - samples/sec: 338.40\n",
      "2020-05-15 16:28:03,436 epoch 10 - iter 3/13 - loss 0.17716510 - samples/sec: 556.73\n",
      "2020-05-15 16:28:03,563 epoch 10 - iter 4/13 - loss 0.14720058 - samples/sec: 471.57\n",
      "2020-05-15 16:28:03,697 epoch 10 - iter 5/13 - loss 0.18650977 - samples/sec: 416.01\n",
      "2020-05-15 16:28:03,825 epoch 10 - iter 6/13 - loss 0.21863698 - samples/sec: 448.70\n",
      "2020-05-15 16:28:03,947 epoch 10 - iter 7/13 - loss 0.26610491 - samples/sec: 573.46\n",
      "2020-05-15 16:28:04,076 epoch 10 - iter 8/13 - loss 0.23990189 - samples/sec: 448.98\n",
      "2020-05-15 16:28:04,203 epoch 10 - iter 9/13 - loss 0.21871261 - samples/sec: 360.26\n",
      "2020-05-15 16:28:04,328 epoch 10 - iter 10/13 - loss 0.21065255 - samples/sec: 518.07\n",
      "2020-05-15 16:28:04,453 epoch 10 - iter 11/13 - loss 0.20123134 - samples/sec: 491.42\n",
      "2020-05-15 16:28:04,566 epoch 10 - iter 12/13 - loss 0.21174704 - samples/sec: 649.45\n",
      "2020-05-15 16:28:04,700 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:28:04,701 EPOCH 10 done: loss 0.2117 - lr 0.1000\n",
      "2020-05-15 16:28:07,133 DEV : loss 0.8217013478279114 - score 0.6757\n",
      "2020-05-15 16:28:07,267 BAD EPOCHS (no improvement): 5\n",
      "2020-05-15 16:28:07,269 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:28:07,289 epoch 11 - iter 0/13 - loss 0.56811583 - samples/sec: 452.03\n",
      "2020-05-15 16:28:07,413 epoch 11 - iter 1/13 - loss 0.37282550 - samples/sec: 450.91\n",
      "2020-05-15 16:28:07,534 epoch 11 - iter 2/13 - loss 0.25113447 - samples/sec: 440.66\n",
      "2020-05-15 16:28:07,651 epoch 11 - iter 3/13 - loss 0.19852561 - samples/sec: 504.62\n",
      "2020-05-15 16:28:07,775 epoch 11 - iter 4/13 - loss 0.24777771 - samples/sec: 468.91\n",
      "2020-05-15 16:28:07,898 epoch 11 - iter 5/13 - loss 0.21161352 - samples/sec: 482.36\n",
      "2020-05-15 16:28:08,020 epoch 11 - iter 6/13 - loss 0.19874471 - samples/sec: 501.05\n",
      "2020-05-15 16:28:08,149 epoch 11 - iter 7/13 - loss 0.21482320 - samples/sec: 523.19\n",
      "2020-05-15 16:28:08,270 epoch 11 - iter 8/13 - loss 0.21019548 - samples/sec: 406.00\n",
      "2020-05-15 16:28:08,401 epoch 11 - iter 9/13 - loss 0.25085222 - samples/sec: 445.08\n",
      "2020-05-15 16:28:08,518 epoch 11 - iter 10/13 - loss 0.24760955 - samples/sec: 501.07\n",
      "2020-05-15 16:28:08,638 epoch 11 - iter 11/13 - loss 0.23702114 - samples/sec: 477.64\n",
      "2020-05-15 16:28:08,753 epoch 11 - iter 12/13 - loss 0.22797711 - samples/sec: 619.44\n",
      "2020-05-15 16:28:08,856 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:28:08,857 EPOCH 11 done: loss 0.2280 - lr 0.1000\n",
      "2020-05-15 16:28:11,302 DEV : loss 0.7017708420753479 - score 0.7717\n",
      "2020-05-15 16:28:11,439 BAD EPOCHS (no improvement): 0\n",
      "2020-05-15 16:28:13,234 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:28:13,264 epoch 12 - iter 0/13 - loss 0.08263968 - samples/sec: 311.79\n",
      "2020-05-15 16:28:13,385 epoch 12 - iter 1/13 - loss 0.15547516 - samples/sec: 532.53\n",
      "2020-05-15 16:28:13,504 epoch 12 - iter 2/13 - loss 0.12387075 - samples/sec: 565.10\n",
      "2020-05-15 16:28:13,623 epoch 12 - iter 3/13 - loss 0.10436554 - samples/sec: 475.69\n",
      "2020-05-15 16:28:13,741 epoch 12 - iter 4/13 - loss 0.13009767 - samples/sec: 571.97\n",
      "2020-05-15 16:28:13,862 epoch 12 - iter 5/13 - loss 0.12572810 - samples/sec: 420.11\n",
      "2020-05-15 16:28:13,982 epoch 12 - iter 6/13 - loss 0.12036123 - samples/sec: 491.09\n",
      "2020-05-15 16:28:14,101 epoch 12 - iter 7/13 - loss 0.13878982 - samples/sec: 598.75\n",
      "2020-05-15 16:28:14,224 epoch 12 - iter 8/13 - loss 0.13452608 - samples/sec: 484.32\n",
      "2020-05-15 16:28:14,346 epoch 12 - iter 9/13 - loss 0.12555378 - samples/sec: 459.30\n",
      "2020-05-15 16:28:14,464 epoch 12 - iter 10/13 - loss 0.12193183 - samples/sec: 377.66\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-15 16:28:14,586 epoch 12 - iter 11/13 - loss 0.12967260 - samples/sec: 433.34\n",
      "2020-05-15 16:28:14,703 epoch 12 - iter 12/13 - loss 0.14627472 - samples/sec: 436.42\n",
      "2020-05-15 16:28:14,803 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:28:14,803 EPOCH 12 done: loss 0.1463 - lr 0.1000\n",
      "2020-05-15 16:28:17,459 DEV : loss 1.060694932937622 - score 0.7683\n",
      "2020-05-15 16:28:17,601 BAD EPOCHS (no improvement): 1\n",
      "2020-05-15 16:28:17,602 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:28:17,622 epoch 13 - iter 0/13 - loss 0.19827484 - samples/sec: 463.82\n",
      "2020-05-15 16:28:17,741 epoch 13 - iter 1/13 - loss 0.10357209 - samples/sec: 474.06\n",
      "2020-05-15 16:28:17,863 epoch 13 - iter 2/13 - loss 0.08482356 - samples/sec: 484.10\n",
      "2020-05-15 16:28:18,018 epoch 13 - iter 3/13 - loss 0.06928484 - samples/sec: 460.17\n",
      "2020-05-15 16:28:18,137 epoch 13 - iter 4/13 - loss 0.06128717 - samples/sec: 517.82\n",
      "2020-05-15 16:28:18,280 epoch 13 - iter 5/13 - loss 0.05561159 - samples/sec: 401.83\n",
      "2020-05-15 16:28:18,417 epoch 13 - iter 6/13 - loss 0.05689105 - samples/sec: 384.29\n",
      "2020-05-15 16:28:18,543 epoch 13 - iter 7/13 - loss 0.05246451 - samples/sec: 538.13\n",
      "2020-05-15 16:28:18,660 epoch 13 - iter 8/13 - loss 0.05460372 - samples/sec: 558.39\n",
      "2020-05-15 16:28:18,780 epoch 13 - iter 9/13 - loss 0.12343634 - samples/sec: 416.72\n",
      "2020-05-15 16:28:18,920 epoch 13 - iter 10/13 - loss 0.12060831 - samples/sec: 319.48\n",
      "2020-05-15 16:28:19,055 epoch 13 - iter 11/13 - loss 0.11550801 - samples/sec: 427.80\n",
      "2020-05-15 16:28:19,179 epoch 13 - iter 12/13 - loss 0.12054522 - samples/sec: 743.67\n",
      "2020-05-15 16:28:19,290 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:28:19,291 EPOCH 13 done: loss 0.1205 - lr 0.1000\n",
      "2020-05-15 16:28:22,473 DEV : loss 0.8441816568374634 - score 0.6967\n",
      "2020-05-15 16:28:22,619 BAD EPOCHS (no improvement): 2\n",
      "2020-05-15 16:28:22,621 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:28:22,644 epoch 14 - iter 0/13 - loss 0.23847663 - samples/sec: 380.93\n",
      "2020-05-15 16:28:22,761 epoch 14 - iter 1/13 - loss 0.14697278 - samples/sec: 695.95\n",
      "2020-05-15 16:28:22,889 epoch 14 - iter 2/13 - loss 0.11044278 - samples/sec: 442.84\n",
      "2020-05-15 16:28:23,022 epoch 14 - iter 3/13 - loss 0.09835175 - samples/sec: 392.56\n",
      "2020-05-15 16:28:23,144 epoch 14 - iter 4/13 - loss 0.16509377 - samples/sec: 458.51\n",
      "2020-05-15 16:28:23,266 epoch 14 - iter 5/13 - loss 0.15015010 - samples/sec: 480.54\n",
      "2020-05-15 16:28:23,381 epoch 14 - iter 6/13 - loss 0.14339786 - samples/sec: 459.28\n",
      "2020-05-15 16:28:23,497 epoch 14 - iter 7/13 - loss 0.13526323 - samples/sec: 566.18\n",
      "2020-05-15 16:28:23,622 epoch 14 - iter 8/13 - loss 0.12605695 - samples/sec: 473.51\n",
      "2020-05-15 16:28:23,749 epoch 14 - iter 9/13 - loss 0.12270035 - samples/sec: 427.10\n",
      "2020-05-15 16:28:23,879 epoch 14 - iter 10/13 - loss 0.11395477 - samples/sec: 462.82\n",
      "2020-05-15 16:28:24,004 epoch 14 - iter 11/13 - loss 0.11821322 - samples/sec: 447.31\n",
      "2020-05-15 16:28:24,122 epoch 14 - iter 12/13 - loss 0.11402295 - samples/sec: 753.80\n",
      "2020-05-15 16:28:24,238 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:28:24,239 EPOCH 14 done: loss 0.1140 - lr 0.1000\n",
      "2020-05-15 16:28:26,928 DEV : loss 0.8174882531166077 - score 0.7773\n",
      "2020-05-15 16:28:27,068 BAD EPOCHS (no improvement): 0\n",
      "2020-05-15 16:28:28,824 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:28:28,847 epoch 15 - iter 0/13 - loss 0.03409611 - samples/sec: 394.02\n",
      "2020-05-15 16:28:28,970 epoch 15 - iter 1/13 - loss 0.06366726 - samples/sec: 484.76\n",
      "2020-05-15 16:28:29,089 epoch 15 - iter 2/13 - loss 0.08943022 - samples/sec: 480.14\n",
      "2020-05-15 16:28:29,207 epoch 15 - iter 3/13 - loss 0.08180967 - samples/sec: 510.68\n",
      "2020-05-15 16:28:29,327 epoch 15 - iter 4/13 - loss 0.07238565 - samples/sec: 384.86\n",
      "2020-05-15 16:28:29,449 epoch 15 - iter 5/13 - loss 0.06905162 - samples/sec: 491.50\n",
      "2020-05-15 16:28:29,567 epoch 15 - iter 6/13 - loss 0.13553944 - samples/sec: 493.37\n",
      "2020-05-15 16:28:29,682 epoch 15 - iter 7/13 - loss 0.12852998 - samples/sec: 485.41\n",
      "2020-05-15 16:28:29,805 epoch 15 - iter 8/13 - loss 0.11613543 - samples/sec: 458.46\n",
      "2020-05-15 16:28:29,926 epoch 15 - iter 9/13 - loss 0.12182926 - samples/sec: 528.92\n",
      "2020-05-15 16:28:30,044 epoch 15 - iter 10/13 - loss 0.12293938 - samples/sec: 528.79\n",
      "2020-05-15 16:28:30,159 epoch 15 - iter 11/13 - loss 0.11889420 - samples/sec: 466.84\n",
      "2020-05-15 16:28:30,276 epoch 15 - iter 12/13 - loss 0.13349817 - samples/sec: 602.05\n",
      "2020-05-15 16:28:30,377 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:28:30,378 EPOCH 15 done: loss 0.1335 - lr 0.1000\n",
      "2020-05-15 16:28:33,003 DEV : loss 1.0531742572784424 - score 0.643\n",
      "2020-05-15 16:28:33,141 BAD EPOCHS (no improvement): 1\n",
      "2020-05-15 16:28:34,843 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-15 16:28:34,844 Testing using best model ...\n",
      "2020-05-15 16:28:34,848 loading file best-model.pt\n",
      "2020-05-15 16:29:34,349 0.7767\t0.7767\t0.7767\n",
      "2020-05-15 16:29:34,350 \n",
      "MICRO_AVG: acc 0.6349 - f1-score 0.7767\n",
      "MACRO_AVG: acc 0.338 - f1-score 0.4212666666666667\n",
      "0          tp: 1 - fp: 22 - fn: 172 - tn: 2805 - precision: 0.0435 - recall: 0.0058 - accuracy: 0.0051 - f1-score: 0.0102\n",
      "1          tp: 2172 - fp: 530 - fn: 107 - tn: 191 - precision: 0.8038 - recall: 0.9530 - accuracy: 0.7732 - f1-score: 0.8721\n",
      "2          tp: 157 - fp: 118 - fn: 391 - tn: 2334 - precision: 0.5709 - recall: 0.2865 - accuracy: 0.2357 - f1-score: 0.3815\n",
      "2020-05-15 16:29:34,351 ----------------------------------------------------------------------------------------------------\n",
      "270.35204458236694\n"
     ]
    }
   ],
   "source": [
    "# GPT-1\n",
    "\n",
    "total_time = time.time()\n",
    "#Do we need batchsize 8 here?\n",
    "\n",
    "word_embeddings = [ OpenAIGPTEmbeddings(),                ]\n",
    "modelname = 'gpt-1'\n",
    "train_and_predict(word_embeddings, modelname, modeldesc, savelist=savelist, epochs=EPOCHS, batch_size=8)\n",
    "\n",
    "print(time.time() - total_time)\n",
    "\n",
    "# 302 sec 18k"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# GPT-2 skipped due to bug\n",
    "# GPT-2 in flair gives error:  IndexError: index 0 is out of bounds for dimension 0 with size 0\n",
    "#\n",
    "# possibly similar to this https://github.com/flairNLP/flair/issues/1221\n",
    "#  \"issue was with some of the unicode characters. XLNetTokenizer returns an empty list for the unicode\n",
    "# character ˜ (U+02DC).This means that the length of subwords for the corresponding token in \n",
    "# token_subwords_mapping is zero\"\n",
    "\n",
    "total_time = time.time()\n",
    "\n",
    "word_embeddings = [ OpenAIGPT2Embeddings(),                ]\n",
    "modelname = 'gpt-2'\n",
    "train_and_predict(word_embeddings, modelname, modeldesc, savelist=savelist, epochs=EPOCHS, batch_size=8)\n",
    "\n",
    "print(time.time() - total_time) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "#name='all_flair_512LSTM_15ep_8model'\n",
    "name='FINAL_flair_all_trainsz_'+str(trainsize)\n",
    "print(len(savelist))\n",
    "saveResults(savelist, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(savelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainsize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for combining results from 2 training times into 1 list etc."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import shelve\n",
    "def saveResults(savelist, name='all_default'):\n",
    "    filename= name+'.shlf'\n",
    "    shelf = shelve.open(outfolder+filename)\n",
    "    # serializing\n",
    "    #shelf[\"all_flair\"] = all_flair\n",
    "    shelf[name] = savelist\n",
    "    shelf.close() # you must close the shelve file!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the 2 last\n",
    "name='all_flair_512LSTM_15ep_model'\n",
    "name = 'FINAL_flair_all_trainsz_'+str(trainsize)\n",
    "te = loadResults(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#newlist = te+savelist\n",
    "newlist = te\n",
    "len(newlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cut doubles away\n",
    "# newlist = newlist[5:]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "# save combined\n",
    "name='FINAL_flair_all_trainsz_'+str(trainsize)\n",
    "print(len(newlist))\n",
    "saveResults(newlist, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move list item 5 to end\n",
    "# te[5]\n",
    "# te.append(te.pop(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(savelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te = savelist[0:5] + [savelist[9]] + savelist[6:9]\n",
    "len(te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flair\n",
      "bert-base-uncased\n"
     ]
    }
   ],
   "source": [
    "for i in savelist:\n",
    "    print(i['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove\n",
      "fasttext web-crawl\n",
      "fasttext news/wiki\n",
      "en-twitter\n",
      "elmo\n",
      "Flair\n",
      "bert-base-uncased\n",
      "BytePairEmbedding\n",
      "gpt-1\n"
     ]
    }
   ],
   "source": [
    "for i in te:\n",
    "    print(i['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'Flair', 'labels': 0       1\n",
       " 1       1\n",
       " 2       1\n",
       " 3       1\n",
       " 4       1\n",
       "        ..\n",
       " 2995    1\n",
       " 2996    1\n",
       " 2997    1\n",
       " 2998    1\n",
       " 2999    1\n",
       " Name: label, Length: 3000, dtype: int64, 'confidence': 0       0.876461\n",
       " 1       0.711614\n",
       " 2       0.925566\n",
       " 3       0.895800\n",
       " 4       0.885994\n",
       "           ...   \n",
       " 2995    0.751575\n",
       " 2996    0.940553\n",
       " 2997    0.902379\n",
       " 2998    0.835532\n",
       " 2999    0.964116\n",
       " Name: confidence, Length: 3000, dtype: float64, 'traintime': 125.29585218429565, 'predtime3k': 1589372012.3927588, 'modeldesc': '512LSTM_15epoch_non-bi'}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "savelist[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "savelist[0]['model']='Flair'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'Flair', 'labels': 0       1\n",
       " 1       1\n",
       " 2       1\n",
       " 3       1\n",
       " 4       1\n",
       "        ..\n",
       " 2995    1\n",
       " 2996    1\n",
       " 2997    1\n",
       " 2998    1\n",
       " 2999    1\n",
       " Name: label, Length: 3000, dtype: int64, 'confidence': 0       0.805744\n",
       " 1       0.546344\n",
       " 2       0.802081\n",
       " 3       0.714745\n",
       " 4       0.753530\n",
       "           ...   \n",
       " 2995    0.759887\n",
       " 2996    0.893057\n",
       " 2997    0.859725\n",
       " 2998    0.813483\n",
       " 2999    0.864573\n",
       " Name: confidence, Length: 3000, dtype: float64, 'traintime': 81.63569831848145, 'predtime3k': 1589378399.4414687, 'modeldesc': '512LSTM_15epoch_non-bi'}"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "te[5] = savelist[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "te[6] = savelist[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove\n",
      "fasttext web-crawl\n",
      "fasttext news/wiki\n",
      "en-twitter\n",
      "elmo\n",
      "Flair\n",
      "bert-base-uncased\n",
      "BytePairEmbedding\n",
      "gpt-1\n"
     ]
    }
   ],
   "source": [
    "for i in newlist:\n",
    "    print(i['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'bert-base-uncased', 'labels': 0       1\n",
       " 1       1\n",
       " 2       1\n",
       " 3       1\n",
       " 4       1\n",
       "        ..\n",
       " 2995    1\n",
       " 2996    2\n",
       " 2997    2\n",
       " 2998    2\n",
       " 2999    1\n",
       " Name: label, Length: 3000, dtype: int64, 'confidence': 0       0.996391\n",
       " 1       0.995925\n",
       " 2       0.740776\n",
       " 3       0.996946\n",
       " 4       0.999737\n",
       "           ...   \n",
       " 2995    0.997924\n",
       " 2996    0.775749\n",
       " 2997    0.926272\n",
       " 2998    0.993099\n",
       " 2999    0.947747\n",
       " Name: confidence, Length: 3000, dtype: float64, 'traintime': 273.37105560302734, 'predtime3k': 1589378548.903775, 'modeldesc': '512LSTM_15epoch_non-bi'}"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "savelist[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
